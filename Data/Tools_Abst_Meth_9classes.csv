File_Name;Content;Category
Diamond;"The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20000 times faster than BLASTX on short reads and has a similar degree of sensitivity.
DIAMOND is a high-throughput alignment program that compares a file of DNA sequencing reads against a file of protein reference sequences. It is implemented in C++ and is designed to run on multicore servers.DIAMOND is faster than BLASTX4 at comparing short DNA reads against the NCBI-nr database and maintains a comparable level of sensitivity on alignments. The program is explicitly designed to make use of modern computer architectures that have large memory capacity and many cores. It follows the seed-and-extend approach. Additional algorithmic ingredients are the use of a reduced alphabet, spaced seeds and double indexing. Seed and extend. The program is based on the traditional seed-and-extend paradigm for sequence comparison, in which exact occurrences of seeds (that is, short words of a given fixed length) contained in query sequences are located in the reference sequences, and these seed matches are then extended, if possible, to full alignments between the queries and references. The seed length used by an alignment program has a substantial impact on performance; shorter seeds increase sensitivity, whereas longer seeds increase speed. Reduced alphabet. To increase speed without losing sensitivity, one approach is to use a reduced alphabet when comparing seeds. Using this, RAPSearch2  is 40 to 100 times faster than BLASTX with minimal loss of sensitivity. For DIAMOND, we investigated the use of published reductions to four, eight and ten letters. By analyzing a large number of BLASTX alignments, we developed a new reduction to an alphabet of size eleven that achieves slightly better sensitivity. Spaced seeds. A second improvement of the seed step is to use spaced seeds, that is, longer seeds in which only a subset of positions are used. The number and exact layout of those positions are called the weight and shape of the spaced seed, respectively. Theoretical analysis shows that a single spaced seed can perform better than a contiguous seed of the same weight, if its shape is suitably chosen. Moreover, sensitivity can be increased further by using additional seed shapes, each resulting only in a sublinear increase in running time. By default, DIAMOND uses a set of four shapes of length 15 to 24 and weight 12. Seed index. The main bottleneck in aligning a large number of reads against a large reference database is not CPU performance but rather memory latency and bandwidth. The limiting factor is the amount of time required to load seed locations from main memory for comparison. Moving data from main memory into the cache takes hundreds of CPU clock cycles. Thus, a fast algorithm should take the cache hierarchy of computers into account so as to maximize data locality and minimize the number of main memory accesses. This can be done by decomposing the problem into smaller subproblems that fit into the cache. In most seed-and-extend programs, an index structure is built on the reference sequences to facilitate the location of seeds in the references. Queries are processed in the order that they occur in the input file. For a given query, all seeds are determined and the index is then used to look up all matching locations in the reference sequences. The reference locations are loaded from main memory into the cache. When another read is later encountered that contains some of the same seeds, the corresponding index and sequence data will usually have been evicted from the cache, so this data will have to be loaded from main memory again. Hence, using a single index in this way does not make good use of the cache. The problem is compounded when using a full-text index, such as a suffix array or a compressed FM index, as these require multiple individual memory accesses per index lookup. Double indexing. DIAMOND uses a double-indexing approach in which both the queries and the references are indexed. A DIAMOND index is a list of seed-location pairs that is lexicographically ordered on a compressed representation of the seed. By traversing the two index lists linearly in parallel, all matching seeds between the queries and the references are identified, allowing the local alignment computation at the corresponding seed locations. The index memory access pattern of this approach is linear and can be efficiently handled by the hardware prefetcher, which will fill the cache with the indexing information before it is needed. The double-indexed approach also improves data locality with respect to accessing the sequences. To see this, let Sq and Sr denote the set of seeds contained in the set of queries and references, respectively. For a given seed s, let ms and ns be the number of occurrences of the seed in the queries and the references. Using the standard index approach, as for each occurrence of a reference seed in the queries, all corresponding reference locations will be loaded into memory. Using the double-indexing approach, the number of memory access operations, assuming that the combined size of query and reference locations for one seed is small enough to fit into the cache. This number is much smaller than K unless the sum is dominated by singleton seeds. To demonstrate this effect on real data, we plot the ratio of memory accesses for the two approaches depending on the length of the query sequence in letters as observed on our benchmark data. The double-indexing algorithm used by DIAMOND is based on the well-known database sort-merge join algorithm, applied to the two seed sets of the queries and references. The main computation, the compilation and sorting of the two lists of seeds, can be efficiently addressed in parallel using a radix clustering in combination with a fast sorting algorithm. The total amount of time required to sort all the seeds in the given set of queries is smaller than what is required to access all seeds in a hash-table approach. Further, sorting all seeds in the reference sequences on the fly takes much less time than loading a precomputed index. For example, the complete seed index for the current version of the NCBI-nr database is about 100 GB in size. This takes 100 s to generate in memory and about six times as long to read from disk at a typical read rate of 150 MB per second. The alignment program mrsFast uses sorted lists as an index structure. The authors of that tool spent a lot of effort on making their algorithm cache oblivious. Much of the challenge discussed there stems from the fact that short nonoverlapping seeds were used, which causes the set of all occurrences of a seed to exceed the cache capacity. Owing to the more elaborate seed strategy used by DIAMOND, in our program the amount of data associated with a given seed will always be small enough to fit into the cache. Memory efficiency. A drawback of using multiple spaced seeds is that this uses a lot of memory, which is a main reason why this approach has not been widely used, despite its proven advantages. The naïve implementation of a multiple spaced seed index builds a hash-table index for each of the seed shapes. With one hash table index for the NCBI-nr database being about 100 GB in size, four seed shapes would consume 400 GB, and 16 shapes would consume 1.6 TB of memory. DIAMOND constructs and processes its indexes for one shape at a time, freeing up the memory used by one shape before moving on to the next. Thus, DIAMOND can perform alignment tasks with its sensitive 16-shape configuration using only as much memory as one shape index requires, which is an additional advantage of our approach. Moreover, using the radix cluster technique, the seed space is decomposed into 1024 disjoint partitions. By building and processing indexes for only a subset of these partitions at the same time, the memory usage will be limited to the size of the subset index. Seed extension. For each seed match found, DIAMOND determines whether it can be extended to an ungapped alignment of ten or more amino acids. If this is the case, then the seed match triggers the extend phase of the algorithm, which involves computing a Smith-Waterman alignment. DIAMOND uses its own streaming SIMD extension (SSE)-accelerated Smith-Waterman implementation that extends previous algorithms to allow the computation of banded and anchored alignments. By default, the program uses the BLOSUM matrix, a gap score of 11 and an extension score of 1, however, other BLOSUM matrices and scoring parameters can be used. The program determines the bit score and expected value of the computed alignment as in BLASTX. By default, alignments with a bit score 50 are not reported. Because DIAMOND proceeds seed by seed rather than read by read, a key issue is how to avoid computing the same local alignment between a read and a reference more than once at different times during the search phase. To address this, DIAMOND allows a seed match to trigger an extension only if it is the left-most seed match in the corresponding ungapped alignment.";Alignment
Minimap2;"Recent advances in sequencing technologies promise ultra-long reads of in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms. Minimap2 is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of 100 bp in length, 1 kb genomic reads at error rate 15 percent, full-length noisy Direct RNA or cDNA reads and assembly contigs or closely related full chromosomes of hundreds of megabases in length. Minimap2 does split-read alignment, employs concave gap cost for long insertions and deletions and introduces new heuristics to reduce spurious alignments. It is 3 to 4 times as fast as mainstream short-read mappers at comparable accuracy, and is 30 times faster than longread genomic or cDNA mappers at higher accuracy, surpassing most aligners specialized in one type of alignment. Minimap2 follows a typical seed-chain-align procedure as is used by most full-genome aligners. It collects minimizers of the reference sequences and indexes them in a hash table, with the key being the hash of a minimizer and the value being a list of locations of the minimizer copies. Then for each query sequence, minimap2 takes query minimizers as seeds, finds exact matches to the reference, and identifies sets of colinear anchors as chains. If base-level alignment is requested, minimap2 applies dynamic programming to extend from the ends of chains and to close regions between adjacent anchors in chains. Minimap2 uses indexing and seeding algorithms similar to minimap , and furthers the predecessor with more accurate chaining, the ability to produce base-level alignment and the support of spliced alignment. In the absence of copy number changes, each query segment should not be mapped to two places in the reference. However, chains found at the previous step may have significant or complete overlaps due to repeats in the reference . Minimap2 used the following procedure to identify primary chains that do not greatly overlap on the query. For each chain from the best to the worst according to their chaining scores: if on the query, the chain overlaps with a chain in by 50 per cent or higher percentage of the shorter chain, mark the chain as secondary to the chain in; otherwise, add the chain to. In the end,  contains all the primary chains. We did not choose a more sophisticated data structure because this step is not the performance bottleneck. For each primary chain, minimap2 estimates its mapping quality with an empirical formula. In reality, sequencing errors are sometimes clustered and k-mers are not independent of each other, especially when we take minimizers as seeds. These violate the assumptions in the derivation above. As a result, ˆ is only approximate and can be biased. It also ignores long deletions from the reference sequence. In practice, fortunately, ˆ is often close to and strongly correlated with the sequence divergence estimated from base-level alignments. Minimap2 performs DP-based global alignment between adjacent anchors in a chain. It uses a 2-piece affine gap cost. The Suzuki-Kasahara formulation When we allow gaps longer than several hundred base pairs, nucleotide-level alignment is much slower than chaining. SSE acceleration is critical to the performance of minimap2. Traditional SSE implementations can achieve 16 way parallelization for short sequences, but only 4 way parallelization when the peak alignment score reaches 32767. Long sequence alignment may exceed this threshold. proposed a difference-based formulation that lifted this limitation. When performing global alignment, we do not need to compute Hrt in each cell. We use 16-way vectorization throughout the alignment process. When extending alignments from ends of chains, we need to find the cell where Hrt reaches the maximum. When performing global alignment between anchors, we expect the alignment to stay close to the diagonal of the DP matrix. Banding is applicable most of the time. With global alignment, minimap2 may force to align unrelated sequences between two adjacent anchors. To avoid such an artifact, we compute cumulative alignment score along the alignment path and break the alignment where the score drops too fast in the diagonal direction. When minimap2 breaks a global alignment between two anchors, it performs local alignment between the two subsequences involved in the global alignment, but this time with the one subsequence reverse complemented. This additional alignment step may identify short inversions that are missed during chaining. Due to sequencing errors and local homology, some anchors in a chain may be wrong. If we blindly align regions between two misplaced anchors, we will produce a suboptimal alignment. These heuristics greatly alleviate the issues with misplaced anchors, but they are unable to fix all such errors. Local misalignment is a limitation of minimap2 which we hope to address in future. If RNA-seq reads are not sequenced from stranded libraries, the read strand relative to the underlying transcript is unknown. By default, minimap2 aligns each chain twice. The alignment with a higher score is taken as the final alignment. This procedure also infers the relative strand of reads that span canonical splicing sites. In the spliced alignment mode, minimap2 further increases the density of minimizers and disables banded alignment. Together with the two-round DP-based alignment, spliced alignment is several times slower than genomic DNA alignment. Aligning short paired-end reads During chaining, minimap2 takes a pair of reads as one fragment with a gap of unknown length in the middle. It applies a normal gap cost between seeds on the same read but is a more permissive gap cost between seeds on different reads. Evaluation on aligning simulated reads. Read alignments are sorted by mapping quality in the descending order. For each mapping quality threshold, the fraction of alignments (out of the number of input reads) with mapping quality above the threshold and their error rate are plotted along the curve.";Alignment
Bowtie;"Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie indexes the reference genome using a scheme based on the Burrows-Wheeler transform and the FM index.The common method for searching in an FM index is the exact-matching algorithm of Ferragina and Manzini. Bowtie does not simply adopt this algorithm because exact matching does not allow for sequencing errors or genetic variations. We introduce two novel extensions that make the technique applicable to short read alignment: a quality-aware backtracking algorithm that allows mismatches and favors high-quality alignments; and double indexing, a strategy to avoid excessive backtracking. The Bowtie aligner follows a policy similar to Maq's, in that it allows a small number of mismatches within the high-quality end of each read, and it places an upper limit on the sum of the quality values at mismatched alignment positions. Burrows-Wheeler indexing The BWT is a reversible permutation of the characters in a text. Although originally developed within the context of data compression, BWT-based indexing allows large texts to be searched efficiently in a small memory footprint. It has been applied to bioinformatics applications, including oligomer counting , whole-genome alignment , tiling microarray probe design, and Smith-Waterman alignment to a human-sized reference. The LF mapping is also used in exact matching. Because the matrix is sorted lexicographically, rows beginning with a given sequence appear consecutively. In a series of steps, the EXACTMATCH algorithm calculates the range of matrix rows beginning with successively longer suffixes of the query. At each step, the size of the range either shrinks or remains the same. UNPERMUTE is attributable to Burrows and Wheeler and EXACTMATCH to Ferragina and Manzini. Searching for inexact alignments EXACTMATCH is insufficient for short read alignment because alignments may contain mismatches, which may be due to sequencing errors, genuine differences between reference and query organisms, or both. We introduce an alignment algorithm that conducts a backtracking search to quickly find alignments that satisfy a specified alignment policy. Each character in a read has a numeric quality value, with lower values indicating a higher likelihood of a sequencing error. Our alignment policy allows a limited number of mismatches and prefers alignments where the sum of the quality values at all mismatched positions is low. The search proceeds similarly to EXACTMATCH, calculating matrix ranges for successively longer query suffixes. If the range becomes empty (a suffix does not occur in the text), then the algorithm may select an already-matched query position and substitute a different base there, introducing a mismatch into the alignment. The EXACTMATCH search resumes from just after the substituted position. The algorithm selects only those substitutions that are consistent with the alignment policy and which yield a modified suffix that occurs at least once in the text. If there are multiple candidate substitution positions, then the algorithm greedily selects a position with a minimal quality value. Backtracking scenarios play out within the context of a stack structure that grows when a new substitution is introduced and shrinks when the aligner rejects all candidate alignments for the substitutions currently on the stack. In short, Bowtie conducts a quality-aware, greedy, randomized, depth-first search through the space of possible alignments. If a valid alignment exists, then Bowtie will find it. Because the search is greedy, the first valid alignment encountered by Bowtie will not necessarily be the best in terms of number of mismatches or in terms of quality. The user may instruct Bowtie to continue searching until it can prove that any alignment it reports is 'best' in terms of number of mismatches. In our experience, this mode is two to three times slower than the default mode. We expect that the faster default mode will be preferred for large re-sequencing projects. The user may also opt for Bowtie to report all alignments up to a specified number or all alignments with no limit on the number for a given read. Bowtie mitigates excessive backtracking with the novel technique of double indexing. Two indices of the genome are created one containing the BWT of the genome, called the forward index, and a second containing the BWT of the genome with its character sequence reversed not reverse complemented called the mirror index. To see how this helps, consider a matching policy that allows one mismatch in the alignment. A valid alignment with one mismatch falls into one of two cases according to which half of the read contains the mismatch. Bowtie proceeds in two phases corresponding to those two cases. loads the forward index into memory and invokes the aligner with the constraint that it may not substitute at positions in the query's right half. uses the mirror index and invokes the aligner on the reversed query, with the constraint that the aligner may not substitute at positions in the reversed query's right half. The constraints on backtracking into the right half prevent excessive backtracking, whereas the use of two phases and two indices maintains full sensitivity.";Alignment
HISAT;"The human reference genome represents only a small number of individuals, which limits its usefulness for genotyping. We present a method named HISAT2 (hierarchical indexing for spliced alignment of transcripts 2) that can align both DNA and RNA sequences using a graph Ferragina Manzini index. We use HISAT2 to represent and search an expanded model of the human reference genome in which genomic variants in combination with haplotypes are incorporated into the data structure used for searching and alignment. We benchmark HISAT2 using simulated and real datasets to demonstrate that our strategy of representing a population of genomes, together with a fast, memory-efficient search algorithm, provides more detailed and accurate variant analyses than other methods. We apply HISAT2 for HLA typing and DNA fingerprinting; both applications form part of the HISAT-genotype software that enables analysis of haplotype-resolved genes or genomic regions. HISAT-genotype outperforms other computational methods and matches or exceeds the performance of laboratory-based assays. GFM index and sequence search through the index To perform the LF mapping, the number of times that a last column label of a given row r occurs up to and including r needs to be identified, which involves counting occurrences from the top of the table down to row r. This counting would be prohibitively time-consuming for the human genome. To accelerate the process, the table is partitioned into small blocks of only a few hundred rows each. Additional numbers are stored within each block recording the number of occurrences of a specific base that appear up to that block. We also optimized the local counting process, where we counted the number of times a specific base appeared within that block. This overall indexing scheme is called a GFM index. HISAT2 also allows for building indexes of various read length and using only one, or a few of them on an actual run so that it requires only a small amount of additional memory. HISAT-genotype’s typing algorithm Because allele sequences may only be partially available (for example, exons only), HISAT-genotype first identifies two alleles on the basis of the sequences commonly available for all alleles, for example exons. HISAT-genotype first chooses representative alleles from groups of alleles that have the same exon sequences. Next it identifies alleles in the representative alleles that are highly likely to be present in a sequenced sample. Then the other alleles from the groups with the same exons as the representatives are selected for assessment during the next step. Second, HISAT-genotype further identifies candidate alleles on the basis of both exons and introns. HISAT-genotype applies the following statistical model in each of the two steps to find maximum likelihood estimates of abundance through an EM algorithm. We previously implemented an EM solution in our centrifuge system, and we used a similar algorithm in HISAT-genotype, with modifications to the variable definitions as follows. HISAT-genotype finds the abundances α that best reflect the given read alignments, that is, the abundances that maximize the likelihood function above by repeating the EM procedure or until the difference between the previous and current estimates of abundances.";Alignment
STAR;Accurate alignment of high-throughput RNA-seq data is a challenging and yet unsolved problem because of the non-contiguous transcript structure, relatively short read lengths and constantly increasing throughput of the sequencing technologies. Currently available RNA-seq aligners suffer from high mapping error rates, low mapping speed, read length limitation and mapping biases. Results: To align our large ENCODE Transcriptome RNA-seq dataset, we developed the Spliced Transcripts Alignment to a Reference (STAR) software based on a previously undescribed RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. STAR outperforms other aligners by a factor in mapping speed, aligning to the human genome paired-end reads per hour on a modest server, while at the same time improving alignment sensitivity and precision. In addition to unbiased de novo detection of canonical junctions, STAR can discover non-canonical splices and chimeric transcripts, and is also capable of mapping full-length RNA sequences. Using Roche 454 sequencing of reverse transcription polymerase chain reaction amplicons, we experimentally validated novel intergenic splice junctions with a success rate, corroborating the high precision of the STAR mapping strategy. Many previously described RNA-seq aligners were developed as extensions of contiguous short read mappers, which were used to either align short reads to a database of splice junctions or align split-read portions contiguously to a reference genome, or a combination thereof. In contrast to these approaches, STAR was designed to align the non-contiguous sequences directly to the reference genome. STAR algorithm consists of two major steps: seed searching step and clustering/stitching/scoring step. 2.1 Seed search The central idea of the STAR seed finding phase is the sequential search for a Maximal Mappable Prefix. In the first step, the algorithm finds the MMP starting from the first base of the read. Because the read in this example comprises a splice junction, it cannot be mapped contiguously to the genome, and thus the first seed will be mapped to a donor splice site. Next, the MMP search is repeated for the unmapped portion of the read, which, in this case, will be mapped to an acceptor splice site. Note that this sequential application of MMP search only to the unmapped portions of the read makes the STAR algorithm extremely fast and distinguishes it from Mummer and MAUVE, which find all possible Maximal Exact Matches. This approach represents a natural way of finding precise locations of splice junctions in a read sequence and is advantageous over an arbitrary splitting of read sequences used in the split-read methods. The splice junctions are detected in a single alignment pass without any a priori knowledge of splice junctions’ loci or properties, and without a preliminary contiguous alignment pass needed by the junction database approaches. The MMP in STAR search is implemented through uncompressed suffix arrays. Notably, finding MMP is an inherent outcome of the standard binary string search in uncompressed SAs, and does not require any additional computational effort compared with the full-length exact match searches. The binary nature of the SA search results in a favorable logarithmic scaling of the search time with the reference genome length, allowing fast searching even against large genomes. STAR builds alignments of the entire read sequence by stitching together all the seeds that were aligned to the genome in the first phase. First, the seeds are clustered together by proximity to a selected set of anchor seeds. We found that an optimal procedure for anchor selection is through limiting the number of genomic loci the anchors align to. All the seeds that map within user-defined genomic windows around the anchors are stitched together assuming a local linear transcription model. The size of the genomic windows determines the maximum intron size for the spliced alignments. A frugal dynamic programming algorithm is used to stitch each pair of seeds, allowing for any number of mismatches but only one insertion or deletion. Importantly, the seeds from the mates of paired-end RNA-seq reads are clustered and stitched concurrently, with each paired-end read represented as a single sequence, allowing for a possible genomic gap or overlap between the inner ends of the mates. This is a principled way to use the paired-end information, as it reflects better the nature of the paired-end reads, namely, the fact that the mates are pieces (ends) of the same sequence. This approach increases the sensitivity of the algorithm, as only one correct anchor from one of the mates is sufficient to accurately align the entire read. If an alignment within one genomic window does not cover the entire read sequence, STAR will try to find two or more windows that cover the entire read, resulting in a chimeric alignment, with different parts of the read mapping to distal genomic loci, or different chromosomes, or different strands. STAR can find chimeric alignments in which the mates are chimeric to each other, with a chimeric junction located in the unsequenced portion of the RNA molecule between two mates. STAR can also find chimeric alignments in which one or both mates are internally chimerically aligned, thus pinpointing the precise location of the chimeric junction in the genome. For multimapping reads, all alignments with scores within a certain user-defined range below the highest score are reported. Although the sequential MMP search only finds the seeds exactly matching the genome, the subsequent stitching procedure is capable of aligning reads with a large number of mismatches, indels and splice junctions, scalable with the read length.;Alignment
TopHat2;"TopHat is a popular spliced aligner for RNA-sequence experiments. In this paper, we describe TopHat2, which incorporates many significant enhancements to TopHat. TopHat2 can align reads of various lengths produced by the latest sequencing technologies, while allowing for variable-length indels with respect to the reference genome. In addition to de novo spliced alignment, TopHat2 can align reads across fusion breaks, which can occur after genomic translocations. TopHat2 combines the ability to identify novel splice sites with direct mapping to known transcripts, producing sensitive and accurate alignments, even for highly repetitive genomes or in the presence of pseudogenes. Given RNA-seq reads as input, TopHat2 begins by mapping reads against the known transcriptome, if an annotation file is provided. This transcriptome mapping improves the overall sensitivity and accuracy of the mapping. It also gives the whole pipeline a significant speed increase, owing to the much smaller size of the transcriptome compared with that of the genome. After the transcriptome-mapping step, some reads remain unmapped because they are derived from unknown transcripts not present in the annotation, or because they contain many miscalled bases. In addition, there may be poorly aligned reads that have been mapped to the wrong location. TopHat2 aligns these unmapped or potentially misaligned reads against the genome. Any reads contained entirely within exons will be mapped, whereas other spanning introns may not be. Using unmapped reads, TopHat2 tries to find novel splice sites that are based on known junction signals. TopHat2 also provides an option to allow users to remap some of the mapped reads, depending on the edit distance values of these reads; that is, those reads whose edit distance is greater than or equal to a user-provided threshold will be treated as unmapped reads. To accomplish this, the unmapped reads and previously mapped reads with low alignment scores are split into smaller non-overlapping segments which are then aligned against the genome . Tophat2 examines any cases in which the left and right segments of the same read are mapped within a user-defined maximum intron size. When this pattern is detected, TopHat2 re-aligns the entire read sequence to that genomic region in order to identify the most likely locations of the splice sites. Using a similar approach, indels and fusion breakpoints are also detected in this step. The genomic sequences flanking these splice sites are concatenated, and the resulting spliced sequences are collected as a set of potential transcript fragments. Any reads not mapped in the previous stages, or mapped very poorly, are then re-aligned with Bowtie2 against this novel transcriptome. After these steps, some of the reads may have been aligned incorrectly by extending an exonic alignment a few bases into the adjacent intron. TopHat2 checks if such alignments extend into the introns identified in the split-alignment phase; if so, it can realign these reads to the adjacent exons instead. In the final stage, TopHat2 divides the reads into those with unique alignments and those with multiple alignments. For the multi-mapped reads, TopHat2 gathers statistical information about the relevant splice junctions, insertions, and deletions, which it uses to recalculate the alignment score for each read. Based on these new alignment scores, TopHat2 reports the most likely alignment locations for such multi-mapped reads. For paired-end reads, TopHat2 processes the two reads separately through the same mapping stages described above. In the final stage, the independently aligned reads are analyzed together to produce paired alignments, taking into consideration additional factors including fragment length and orientation.";Alignment
LordFast;"Recent advances in genomics and precision medicine have been made possible through the application of high throughput sequencing (HTS) to large collections of human genomes. Although HTS technologies have proven their use in cataloging human genome variation, computational analysis of the data they generate is still far from being perfect. The main limitation of Illumina and other popular sequencing technologies is their short read length relative to the lengths of  genomic repeats. Newer technologies such as Pacific Biosciences and Oxford Nanopore are producing longer reads, making it theoretically possible to overcome the difficulties imposed by repeat regions. Unfortunately, because of their high sequencing error rate, reads generated by these technologies are very difficult to work with and cannot be used in many of the standard downstream analysis pipelines. Note that it is not only difficult to find the correct mapping locations of such reads in a reference genome, but also to establish their correct alignment so as to differentiate sequencing errors from real genomic variants. Furthermore, especially since newer SMS instruments provide higher throughput, mapping and alignment need to be performed much faster than before, maintaining high sensitivity. We introduce lordFAST, a novel long-read mapper that is specifically designed to align reads generated by PacBio and potentially other SMS technologies to a reference. lordFAST not only has higher sensitivity than the available alternatives, it is also among the fastest and has a very low memory footprint. lordFAST is a heuristic anchor-based aligner for long reads generated by third generation sequencing technologies. lordFAST aims to find a set of candidate locations per read before the costly step of base-to-base alignment to the reference genome. lordFAST works in two main stages. In stage one, it builds an index from the reference genome, which is used to find short exact matches. The index is a combination of a lookup table and an FM index. In stage two, it maps the long reads to the reference genome in four steps: on each read, it identifies a fixed number of evenly spaced k-mers, which are matched to the reference genome through the use of the index. For each such match, it obtains the longest exact matching extension. Among these extended matches of each k-mer identified in each read, it finally chooses the longest which acts as anchor matches; for each read, it then splits the reference genome into overlapping windows and identifies each such window as a candidate region if the number of anchor matches in that window is above a threshold value; for each candidate region, it identifies the longest chain of concordant anchor matches, chain of anchor matches which have equal respective spacing in the read and the reference genome; it obtains the base-to-base alignment by performing dynamic programing between consecutive anchor matches in the selected chain. Stage one: reference genome indexing In order to build a index for the reference genome, we use a combination of a simple lookup table for initial short matches, and an FM index for extending such initial matches. This combined index benefits from the speed of lookup table and the compactness of the Burrows–Wheeler transform (BWT) representation for the reference genome. As is well known, the FM index provides a compact representation of a suffix array which we use to find exact matching extensions of initial h-mer matches. Note that in order to be able to perform efficient search on both strands of the reference genome, we use an extension to the FM index implemented in fermi. For each anchoring position, it finds the longest prefix matches to the genome as follows. First, it extracts the first h-mer starting from the anchoring position and uses the lookup table of the genome index to obtain the interval that represents the initial set of matching locations on the FM index. It then uses the LF-mapping operation of the FM index to extend the initial set of matches and identify the longest matches. Note that using longest matches reduces the total number of anchor matches significantly. candidate region selection In order to select the candidate regions for alignment, lordFAST splits the reference genome into overlapping windows. For each window, it calculates two scores for the forward and reverse strands from anchor matches of the respective strands. lordFAST keeps those windows whose score is not significantly worse than the maximum window score. In cases where two overlapping windows both meet the minimum window score requirement, lordFAST will keep the one with higher window score in the final list, ties are broken by choosing the window with smaller reference coordinate. chaining and anchor selection Among all the anchor matches in a candidate region, lordFAST chooses a set of ‘concordant’ anchors using local chaining. The best local chain is a set of co-linear, non-overlapping anchors on the reference genome that has the highest score among all such sets. To calculate the best local chain, lordFAST assigns a weight to each anchor match equal to the length of the match. lordFAST supports two chaining algorithms. By default, it obtains the best chain using the dynamic programing based chaining algorithm. Note that the time complexity of this chaining algorithm is quadratic, but in practice, it is fast due to our small number of anchor matches per read. It is also possible for the user to select the alternative chaining algorithm based on clasp. The anchor matches in the best local chain form the basis of the alignment in that region. alignment, lordFAST prioritizes the candidate regions based on their best chaining score and performs the final alignment for the top regions . In order to generate the base-to-base alignment of a region, it uses anchor matches from the top scoring chain and performs banded global alignment for gaps between pairs of consecutive anchor matches. Furthermore, the alignment between the prefix of the read and the reference prior to the first anchor can be performed by the use of an anchored global-to-local alignment and the alignment between the suffix of the read and the reference following the last anchor can be computed in an identical fashion. This strategy is a widely used technique to avoid computing the full alignment between long sequences as that needs huge memory and computational time. lordFAST uses Edlib for computing the global alignments and ksw library for computing the global-to-local alignments. Edlib is a library implementing a fast bit-vector algorithm devised by Mye. ksw, on the other hand, provides alignment extension based on the affine gap-cost model. It is worth mentioning that lordFAST supports clipping as follows: if the prefix of the read before the first anchor has an alignment score/similarity which is lower than a threshold, lordFAST performs clipping of that prefix. This is done by using ksw library to extend the alignment as long as a significant drop in the alignment score/similarity is not observed (using a parameter similar to BLAST’s X-dropoff). In addition, lordFAST supports split alignment. One alignment corresponds to the substring before anchor and the other alignment corresponds to the substring after anchor. Furthermore, since the drop in alignment score/similarity could be due to the presence of an inversion, we check if the alignment between the reverse complement has a score higher than thsplit. In that case, such an alignment will be also reported as another supplementary alignment.";Alignment
BWAMEM;BWA-MEM is a new alignment algorithm for aligning sequence reads or assembly contigs against a large reference genome such as human genome. It automatically chooses between local and end-to-end alignments, supports paired-end reads and performs chimeric alignment. The algorithm is robust to sequencing errors. For mapping sequences, BWA-MEM shows better performance than several state-of-art read aligners to date. Aligning a single query sequenc. Seeding and re-seeding BWA-MEM follows the canonical seed-and-extend paradigm. It initially seeds an alignment with supermaximal exact matches, SMEMs, using an algorithm we found previously , which essentially finds at each query position the longest exact match covering the position. However, occasionally the true alignment may not contain any SMEMs. To reduce mismappings caused by missing seeds, we introduce re-seeding. Chaining and chain filtering. We call a group of seeds that are colinear and close to each other as a chain. We greedily chain the seeds while seeding and then filter out short chains that are largely contained in a long chain and are much worse than the long chain. Chain filtering aims to reduce unsuccessful seed extension at a later step. Each chain may not always correspond to a final hit. Chains detected here do not need to be accurate. Seed extension. We rank a seed by the length of the chain it belongs to and then by the seed length. For each seed in the ranked list, from the best to the worst, we drop the seed if it is already contained in an alignment found before, or extend the seed with a banded affine-gap-penalty dynamic programming if it potentially leads to a new alignment. BWA-MEM’s seed extension differs from the standard seed extension in two aspects. This heuristic avoids extension through a poorly aligned region with good flanking alignment. It is similar to the X-dropoff heuristic in BLAST, but does not penalize long gaps in one of the reference or the query sequences. Secondly, while extending a seed, BWA-MEM tries to keep track of the best extension score reaching the end of the query sequence. If the difference between the best score reaching the end and the best local alignment score is below a threshold, the local alignment will be rejected even if it has a higher score. BWA-MEM uses this strategy to automatically choose between local and end-to-end alignments. It may align through true variations towards the end of a read and thus reduces reference bias, while avoids introducing excessive mismatches and gaps which may happen when we force a chimeric read into an end-to-end alignment. Paired-end mapping Rescuing missing hits Like BWA , BWAMEM processes a batch of reads at a time. For each batch, it estimates the mean µ and the variance of the insert size distribution from reliable single-end hits. For the top 100 hits of either end, if the mate is unmapped in a window from each hit, BWA-MEM performs SSE2-based Smith-Waterman alignment for the mate within the window. The second best SW score is recorded to detect potential mismapping in a long tandem repeat. Hits found from both the single-sequence alignment and SW rescuing will be used for pairing. Pairing. Given the hit for the first read and j-th hit for the second, BWA-MEM computes their distance dij if the two hits are in the right orientation, or sets dij to infinity otherwise.;Alignment
Kart;Next-generation sequencing provides a great opportunity to investigate genome-wide variation at nucleotide resolution. Due to the huge amount of data, NGS applications require very fast and accurate alignment algorithms. Most existing algorithms for read mapping basically adopt seed-and-extend strategy, which is sequential in nature and takes much longer time on longer reads. We develop a divide-and-conquer algorithm, called Kart, which can process long reads as fast as short reads by dividing a read into small fragments that can be aligned independently. it can tolerate much higher error rates. The experiments show that Kart spends much less time on longer reads than other aligners and still produce reliable alignments even when the error rate is high. Overview of our algorithms Most suffix/BWT array based aligners, which follow the canonical seed-and-extend methodology, initiate an alignment with an MEM and extend the alignment with different dynamic programming strategies. Therefore, the performance of an aligner is greatly affected by the algorithms for seed exploration and the strategies for handling inexact matches. These aligners are sequential in nature. We adopted a divide-and-conquer strategy to reduce the time-consuming gapped alignment step using dynamic programming, which is suitable for mapping highly similar fragment sequences each read is essentially a copy of a specific genome fragment except for a small percentage of sequencing errors. Simple pairs and normal pairs Since un-gapped without indels alignment is much faster than gapped alignment, for each mapped candidate region in the reference genome, we separate the given read sequence and their candidate regions into two groups: simple region pairs and normal region pairs , where all simple pairs have perfect alignment , and normal pairs require un-gapped gapped alignment. Once the simple and normal pairs are identified, they can be processed and aligned independently and the final mapping result for a candidate region is simply the concatenation of the alignment of each simple and normal pair. If two simple pairs overlap in a candidate alignment, say, in the genome portion the same goes for the read portion, we chop off the overlapped portion in the genome and its corresponding read portion from the shorter pair to ensure that all simple pairs are non-overlapping. Kart removes the overlap by shrinking the shorter simple pair. In this way, any two simple pairs in the same candidate alignment will not share any nucleotide. We then create normal pairs filling the gaps between adjacent simple pairs to make a complete alignment as follows. On the other hand, if the first simple region in a candidate alignment does not cover the first nucleotide of the read sequence, a normal pair would also be created to fill the gap. They partition the read sequence into interlaced simple and normal region pairs, which can be independently aligned and the final alignment is simply the concatenation of the alignment of each simple and normal pair. A closer look at the normal pairs indicates that a substantial portion of normal pairs do not require gapped alignment either. Kart supports paired-end reads mapping as follows. To align paired-end reads, Kart finds the candidate alignments for each read separately and then compares the two groups of mapping results to see if there is a pair of alignments one from each group that satisfies the paired-end distance constraints. If there is no such pair, it implies that the paired-end reads contain more sequencing errors such that at least one of them is not mapped properly. In such cases, Kart will initiate a rescue procedure trying to find a proper pair of alignments based on the candidate alignments of the opposite read. After removing overlaps between adjacent simple pairs, Kart fills in the gaps between simple pairs with normal pairs to make each cluster a complete candidate alignment. When both the read block and the genome block of a normal pair are more than long, we perform a second round of sequence partitioning to further divide it and reduce the portion that needs gapped alignment. The final read alignment is the concatenation of simple/normal pairs in the same candidate alignment. Finally, Kart reports the alignment with the highest alignment score or paired alignments for paired-end reads.;Alignment
NGMLR;"Structural variations are the largest source of genetic variation, but remain poorly understood because of limited genomics technology. Single molecule long-read sequencing from Pacific Biosciences and Oxford Nanopore has the potential to dramatically advance the field, although their high error rates challenge existing methods. Addressing this need, we introduce open-source methods for long-read alignment and SV identification that enable unprecedented SV sensitivity and precision, including within repeat-rich regions and of complex nested events that can have significant impact on human disorders. Examining several datasets, including healthy and cancerous human genomes, we discover thousands of novel variants using long-reads and categorize systematic errors in short-read approaches. NGMLR and Sniffles are further able to automatically filter false events and operate on low amounts of coverage to address the cost factor that has hindered the application of long-reads in clinical and research settings. NGMLR: Fast, Accurate Mapping of Long Single Molecule Reads NGMLR is designed to accurately map long single molecule sequencing reads from either Pacific Biosciences or Oxford Nanopore to a reference genome with the goal of enabling precise structural variation calls. We follow the terminology used by the SAM specification47 where a read mapping consists either of one linear alignment covering the full read length or multiple linear alignments covering non-overlapping segments of the read . The main challenge when mapping high error long-reads is to evaluate whether a read should be mapped to the reference genome with one linear alignment, or must be split. For example, the correct mapping for a read that spans an inversion can only be found when splitting the read into three segments. Conversely, reads that do not span a structural variation should always be mapped with a single linear alignment. However, error rates are high, and are not always uniform. These segments can cause read mappers to falsely split a read. Furthermore, the high insertion and deletion sequencing error of long-read technologies cause current read aligners to falsely split up large SVs into several smaller ones and make it difficult to detect exact break points. To address these challenges, NGMLR implements the following workflow : NGMLR identifies sub-segments of the read and of the reference genome that show high similarity and can be aligned with a single linear alignment. These segments can contain small insertions and deletions, but must not span a larger structural variation breakpoint. In reference to BLAST’s High-scoring Segment Pairs, we call those segment linear mapping pairs. For each LMP, NGMLR extracts the read sequence and the reference sequence and uses the Smith-Waterman algorithm to compute a pairwise sequence alignment using a convex gap cost model that accounts for sequencing error and SVs at the same time. NGMLR scans the sequence alignments for regions of low sequence identity to identify small SVs that were missed . Finally, NGMLR selects the set of linear alignments with the highest joint score, computes a mapping quality for each alignment and reports them as the final read mapping in a SAM/BAM file. Convex scoring model When aligning high error long-reads it is crucial to choose an appropriate gap model as there are two sources of insertions and deletions. Sequencing error predominantly causes very short randomly distributed indels while longer indels are caused by genomic structural variations. A linear gap model appropriately models indels originating from sequencing error, but cannot account for longer indels from genomic variation as these large blocks occur as a single unit, not as the combination of multiple single base insertions or deletions. With affine gap models the gap-open penalty falsely causes short indels from sequencing error to cluster together for noisy long-reads, and has only little effect on longer indels, especially in repetitive regions of the genome. With the convex scoring model of NGMLR, extending an indel is penalized proportionally less the longer the indel is. Therefore, the convex scoring model encourages large alignment gaps, such as those occurring from a structural variation, to be grouped together into contiguous stretches (extending a large indel has relatively low cost), while small indels, which commonly occur as sequencing errors, remain separate. Instead of scanning the full cell and row while filling the alignment matrix, we use two additional matrixes to store indel length estimations for each cell. Furthermore, we use the initial sub-segment alignments to identify the part of the alignment matrix that is most likely to contain the correct alignment and skip all other cells of the matrix during alignment computation. Sniffles: Robust Detection of Structural Variations from Long-read Alignments. Sniffles operates within and between the long-read alignments to infer SVs. It applies five major steps. Sniffles first estimates the parameters to adapt itself to the underlying data set, such as the distribution in alignment scores and distances between indels and mismatches on the read, as well as the ratios of the best and second best alignments scores. Sniffles then scans the read alignments and segments to determine if they potentially represent SVs. Putative SVs are clustered and scored based on the number of supporting reads, the type and length of the SV, consistency of the SV composition, and other features. Sniffles optionally genotypes the variant calls to identify homozygous or heterozygous SVs. Sniffles optionally provides a clustering of SVs based on the overlap with the same reads, especially to detect nested variants. In the following, we focus on the methods that are unique to Sniffles, which are the detection and analysis of alignment artifacts to reduce falsely called variants and the clustering of variants. Putative Variant Scoring The high error rate of the long-reads induces many alignments that falsely appear as SVs. Sniffles addresses these by scoring each putative variant using several characteristics that we have determined to be the most relevant to detecting SVs. The two main user thresholds are the number of high quality reads supporting the variant as well as the standard deviation of the coordinates in the start and stop breakpoint across all supporting reads. about all high quality reads that do not include a SVs in a binary file. This includes those reads that support the reference sequence that pass the thresholds for MQ and alignment score ratio. After the detection of SVs, the VCF file is read in, and Sniffles constructs a self-balancing tree of the variants. With this information, Sniffles then computes the fraction of reads that support each variant versus those that support the reference. Variants below the minimum allele frequency  are considered unreliable; variants with high allele frequency are considered homozygous; and variants with an intermediate allele frequency are considered heterozygous. Note that Sniffles does not fully phase the haplotypes, as it does not consider SNPs or small indels, but rather identifies SVs that occur together. If this option is enabled, Sniffles stores the names of each read that supports a SVs in a hash table keyed by the read name, with the list of SVs associated with that read name as the value. The hash table is used to find reads that span more than one event, and later to cluster reads that span one or more of the same variants. In this way Sniffles can cluster two or more events, even if the distance between the events is larger than the read length. Future work will include a full phasing of hapolotypes including SVs, SNPs and other small variants.";Alignment
KSlam;k-SLAM is a highly efficient algorithm for the characterization of metagenomic data. Unlike other ultrafast metagenomic classifiers, full sequence alignment is performed allowing for gene identification and variant calling in addition to accurate taxonomic classification. A k-mer based method provides greater taxonomic accuracy than other classifiers and a three orders of magnitude speed increase over alignment based approaches. The use of alignments to find variants and genes along with their taxonomic origins enables novel strains to be characterized. kSLAM’s speed allows a full taxonomic classification and gene identification to be tractable on modern large data sets. A pseudo-assembly method is used to increase classification accuracy for species which have high sequence homology within their genus. k-SLAM is a metagenomic classifier that uses a sequence alignment method to infer taxonomy and identify genes. Reads and database genomes are split into short k-mers which are added to a list and sorted so that identical k-mers are placed next to one another. Iteration through the list allows k base overlaps between reads and genomes to be found, along with alignment position. The overlaps are then verified with a full Smith-Waterman pairwise sequence alignment. Neighboring alignments are chained together along each genome into a pseudo-assembly, this allows reads that map to low complexity and conserved regions to still be classified precisely as the chains often extend into unique sequence. Low scoring alignments are screened and taxonomy is inferred from the lowest common taxonomic ancestor of the valid overlaps. Alignments are also used to infer genes and variants. The sorted-list method of finding k-mer overlaps allows great speed and efficient parallelisation on modern hardware. k-mer based alignment For the following analysis assume: k is an integer chosen at compile time and a k-mer is a sequence of k nucleotides. Each read is split up into overlapping k-mers and the k-mers are added to a list. Each genome is split into non-overlapping k-mers and the k-mers are added to the same list. The list is sorted lexicographically, placing identical k-mers next to one another. The list is iterated over, finding overlaps between reads and genomes. For each of the overlaps found, a full Smith-Waterman pairwise local sequence alignment is performed to ensure the overlap is valid and to find any variants. Alignments with a score lower than a user-chosen cutoff are screened . k-mers are stored along with their offsets from the start of the sequence, the identifier of the sequence from which they were extracted and a flag that is set if the k-mer has been reverse-complemented. In order to find overlaps on both strands, k-SLAM compares each k-mer with its reverse complement and stores only the lexicographically smallest to save memory. A similar k-mer based method using lexicographic sorting and spaced k-mer seeds, albeit for protein alignments, was independently discovered and used in DIAMOND. Paired-end reads As read length is very important for taxonomic specificity, k-SLAM is designed to work with paired end reads of any insert size. Paired reads are treated initially as two single reads, which have their overlaps and alignments found using the above k-mer method. As bacterial sequence is often repetitive, it is highly likely that each end of the paired read aligns to multiple places on the same genome, hence a method is needed for detecting which pairings of these alignments are valid. For each read/genome pair, all of the alignments are sorted by offset from the start of the genome. The algorithm then makes alignment pairs from each R1's nearest neighbor R2s and vice-versa. This allows only a small subset of pairs to be considered instead of working with all possible pairs. Sequencing technologies. k-SLAM is designed to work with data from all of the most used sequencing technologies. There are, however, some constraints on the reads that affect accuracy. The reads need to be longer than the length of the k-mer and have a sufficiently low error rate such that there is at least one error free k-mer in each read. This allows a k base overlap to be found that can then be verified with a full Smith–Waterman alignment. Longer reads will produce more alignments and greater taxonomic specificity. Taxonomic specificity is also improved by using paired-end reads. A lower error rate allows a longer k-mer and hence a shorter compute time. k-SLAM has been tested and found to be accurate on Illumina HiSeq and MiSeq platforms as well as 454 and Ion Torrent data. Pseudo-assembly Due to the similarity between the genomes of different bacterial species, there is a large probability that each read will map to more than one genome, this makes inferring taxonomy difficult as reads often map to long sections of conserved sequence. k-SLAM attempts to solve this problem by grouping reads that map to adjacent locations on the same genome together into ‘pseudo-assemblies’. A new alignment score is calculated for each chain, taking into account per base similarity, chain length and depth of coverage. These long chains of reads often extend beyond conserved sections and into regions specific to one particular strain. This allows all reads within the chain to be assigned to the lowest possible taxonomic level. Following is a description of the k-SLAM pseudo-assembly algorithm as applied to each genome: For each genome, sort the alignments by start position. Form chains of alignments that overlap. Inferring taxonomy. k-SLAM infers taxonomy using a lowest common ancestor technique similar to that in MEGAN. For each read, a score cutoff is calculated by multiplying the highest alignment score by a user chosen constant and all alignments below this cutoff are screened. Taxonomy is chosen based on the lowest common ancestor in the taxonomy tree of the remaining alignments. A matching gene is also inferred for each read from the position of the alignment along the genome. Inferring genes Genes are inferred using the GenBank format annotations. For each non-screened alignment, the gene with the most overlapping bases is chosen. For the XML output, genes with identical names, protein IDs or products that are assigned to the same taxonomy are combined into a single entry with an associated count.;Classification
Centrifuge;"Centrifuge is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species on desktop computers. The system uses an indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini FM index, optimized specifically for the metagenomic classification problem. Centrifuge requires a relatively small index and classifies sequences at very high speed, allowing it to process the millions of reads from a typical high-throughput DNA sequencing run within a few minutes. Together, these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers. Because of its space-optimized indexing schemes, Centrifuge also makes it possible to index the entire NCBI nonredundant nucleotide sequence database with an index, in contrast to k-mer-based indexing schemes, which require far more extensive space. Database sequence compression We implemented memory-efficient indexing schemes for the classification of microbial sequences based on the FM-index, which also permits very fast search operations. We further reduced the size of the index by compressing genomic sequences and building a modified version of the FM-index for those compressed genomes, as follows. First, we observed that for some bacterial species, large numbers of closely related strains and isolates have been sequenced, usually because they represent significant human pathogens. As expected, the genomic sequences of strains within the same species are likely to be highly similar to one another. We leveraged this fact to remove such redundant genomic sequences, so that the storage size of our index can remain compact even as the number of sequenced isolates for these species increases. First, we choose the two genomes that are most similar among all genomes. We define the two most similar genomes as those that share the greatest number of k-mers after k-mers are randomly sampled at a rate of  from the genomes of the same species. In order to facilitate this selection process, we used Jellyfish to build a table indicating which k-mers belong to which genomes. Using the two most similar genomes allows for better compression as they tend to share larger chunks of genomic sequences than two randomly selected genomes. We then compared the two most similar genomes using nucmer , which outputs a list of the nearly or completely identical regions in both genomes. As a result of this concatenation procedure, we obtained dramatic space reductions for many species. The FM-index provides a means to exploit both large and small k-mer matches by enabling rapid search of k-mers of any length, at speeds comparable to those of k-mer table indexing algorithms. Using this FM-index, Centrifuge classifies DNA sequences as follows. Centrifuge begins with a short exact match and extends the match as far as possible. The algorithm then resumes the search and stops at the next mismatch. The segment in the middle of the read is found in species. Note that only exact matches are considered throughout this process, which is a key factor in the speed of the algorithm. We perform the same procedure for the reverse complement of the read which compared to the forward strand. Based on the exact matches found in the read and its reverse complement, Centrifuge then classifies the read using only those mappings. Centrifuge then scores each species. Centrifuge chooses the strand that gives the maximum score, rather than using the summed score on both strands, which might bias it toward palindromic sequences. Centrifuge can assign a sequence to multiple taxonomic categories; by default, it allows up to five labels per sequence. In order to reduce the number of assignments, Centrifuge traverses up the taxonomic tree. First, it considers the genus that includes the largest number of species. It then replaces these three species with the genus. If more than five taxonomic labels had remained, Centrifuge would repeat this process for other genera and subsequently for higher taxonomic units until it reduced the number of labels to five or fewer. Abundance analysis In addition to per-read classification, Centrifuge performs abundance analysis at any taxonomic rank . Because many genomes share near-identical segments of DNA with other species, reads originating from those segments will be classified as multiple species. Simply counting the number of the reads that are uniquely classified as a given genome (ignoring those that match other genomes) will therefore give poor estimates of that species abundance.";Classification
Clark;"The problem of supervised DNA sequence classification arises in several fields of computational molecular biology. Although this problem has been extensively studied, it is still computationally challenging due to the size of the datasets that modern sequencing technologies can produce. We introduce CLARK a novel approach to classify metagenomic reads at the species or genus level with high accuracy and high speed. Extensive experimental results on various metagenomic samples show that the classification accuracy of CLARK is better or comparable to the best state-of-the-art tools and it is significantly faster than any of its competitors. In its fastest single-threaded mode CLARK classifies, with high accuracy, about 32 million metagenomic short reads per minute. CLARK can also classify BAC clones or transcripts to chromosome arms and centromeric regions. CLARK is a versatile, fast and accurate sequence classification method, especially useful for metagenomics and genomics applications. Building target-specific k-mer sets CLARK accepts inputs in fasta/fastq format; alternatively the input can be given as a text file containing the k-mer distribution . CLARK first builds an index from the target sequences, unless one already exists for the specified input files. If a user wants to classify objects at the genus level , or another taxonomic rank, is expected to generate targets by grouping genomes of the same genus (or with the same taxonomic label). This strategy represents a major difference with other tools, such as LMAT, KRAKEN. The index is a hash-table storing, for each distinct k-mer , the ID for the target containing w, the number of distinct targets containing w, and the number of occurrences of w in all the targets. This hash-table uses separate chaining to resolve collisions . CLARK then removes any k-mer that appears in more than one target, except in the case of chromosome arm assignment. In the latter case, k-mers shared by the two arms of the same chromosome are used to define centromeric regions of overlap. Also, k-mers in the index may be removed based on their number of occurrences if the user has specified a minimum number of occurrences. These rare k-mers tend to be spurious from sequencing errors. Other metagenomic classifiers like KRAKEN and LMAT do not offer this protection against noise, which is very useful when target sequences are reads or low-quality assemblies. Then, the resulting sets of target-specific k-mers are stored in disk for the next phase. Observe that CLARK is faster than NBC and KRAKEN to create the index, and it uses less RAM and disk space than KRAKEN for classifying objects. The concept of “target-specific k-mers” is similar to the notion of “clade-specific marker genes” or “genome-specific markers” . While CLARK uses exact matching to identify the target-specific k-mers derived from any region in the genome, the authors in disregard intergenic regions. CLARK uses exact matching. Sequence classification In the full mode, once the index containing target-specific k-mers has been created, CLARK creates a dictionary that associates k-mers to targets. Then, CLARK iteratively processes each object for each object sequence o CLARK queries the index to fetch the set of k-mers in o. A hit is obtained when a k-mer matches a target-specific k-mer set. Object o is assigned to the target that has the highest number of hits. The rationale to remove common k-mers between targets at any taxonomy level defined by the user is that they increase the noise in the classification process. If they were present, more targets could obtain the same number of hits which would complicate the assignment. If such conflicts can be avoided, then there is no need to query the taxonomy tree, and find, for example, the lowest common ancestor taxons for “conflicting nodes” to resolve them as it is done in other tools.";Classification
kaiju;"AMetagenomics emerged as an important field of research not only in microbial ecology but also for human health and disease, and metagenomic studies are performed on increasingly larger scales. While recent taxonomic classification programs achieve high speed by comparing genomic k-mers, they often lack sensitivity for overcoming evolutionary divergence, so that large fractions of the metagenomic reads remain unclassified. Here we present the novel metagenome classifier Kaiju, which finds maximum (in-)exact matches on the protein-level using the Burrows–Wheeler transform. We show in a genome exclusion benchmark that Kaiju classifies reads with higher sensitivity and similar precision compared with current k-mer-based classifiers, especially in genera that are underrepresented in reference databases. We also demonstrate that Kaiju classifies up to 10 times more reads in real metagenomes. Kaiju can process millions of reads per minute and can run on a standard PC.
Kaiju classifies individual metagenomic reads using a reference database comprising the annotated protein-coding genes of a set of microbial genomes. We employ a search strategy, which finds maximal exact matching substrings between query and database using a modified version of the backwards search algorithm in the BWT. The BWT is a text transformation that converts the reference sequence database into an easily searchable representation, which allows for exact string matching between a query sequence and the database in time proportional to the length of the query. While in the context of read mapping, MEMs have been used as a fast method for identifying seeds of mapping regions in the reference genome, for example, we use MEMs to quickly find those sequences in the reference database that share the longest possible subsequence with the query. Backtracking through the BWT can be sped up by using a lookup table for occurrence counts of each alphabet letter, which was first proposed by Ferragina and Manzini and is often called FM index. Kaiju employs a sparse representation of this table by using checkpoints, which allows for decreasing the otherwise large memory requirement due to the size of the amino acid alphabet. The initial suffix array used for calculating the BWT is also implemented as a sparse suffix array with adjustable size, which further reduces the index size with only little impact on runtime, because the suffix array is only needed for determining the name of the database sequence once the best match for a read is found. Thus, Kaiju is the first program to efficiently use the BWT and FM index on a large protein database, allowing querying large sets of sequencing reads. First, Kaiju translates each read into the six possible reading frames, which are then split at stop codons into amino acid fragments. These fragments are sorted by length, and, beginning with the longest fragment, queried against the reference database using the backwards search in the BWT. Given a query fragment of length n and the minimum required match length m, the backwards search is started from all positions between n and n  m in the query and the longest MEM is retained. If one or more matches of length are found, m is set to l and the next fragment in the ordered list is queried against the database if its length is at least l, otherwise the search stops. Once the search is finished and one or more matches are found, the taxon identifier from the corresponding database sequence is retrieved from the suffix array and printed to the output. If equally long matches are found in multiple taxa, Kaiju determines their LCA from the taxonomic tree and outputs its taxon identifier. Thus, each read is always classified to the lowest possible taxonomic level given the ambiguity of the search result.";Classification
KrakenUniq;False-positive identifications are a significant problem in metagenomics classification. We present KrakenUniq, a novel metagenomics classifier that combines the fast k-mer-based classification of Kraken with an efficient algorithm for assessing the coverage of unique k-mers found in each species in a dataset. On various test datasets, KrakenUniq gives better recall and precision than other methods and effectively classifies and distinguishes pathogens with low abundance from false positives in infectious disease samples. By using the probabilistic cardinality estimator HyperLogLog, KrakenUniq runs as fast as Kraken and requires little additional memory. KrakenUniq was developed to provide efficient k-mer count information for all taxa identified in a metagenomics experiment. The main workflow is as follows: As reads are processed, each k-mer is assigned a taxon from the database. KrakenUniq instantiates a HyperLogLog data sketch for each taxon and adds the k-mers to it. After classification of a read, KrakenUniq traverses up the taxonomic tree and merges the estimators of each taxon with its parent. In its classification report, KrakenUniq includes the number of unique k-mers and the depth of k-mer coverage for each taxon that it observed in the input data. Efficient k-mer cardinality estimation using the HyperLogLog algorithm Cardinality is the number of elements in a set without duplicates, the number of distinct words in a text. An exact count can be kept by storing the elements in a sorted list or linear probing hash table, but that requires memory proportional to the number of unique elements. When an accurate estimate of the cardinality is sufficient, however, the computation can be done efficiently with a very small amount of fixed memory. The HyperLogLog algorithm, which is well suited for k-mer counting , keeps a summary or sketch of the data that is sufficient for precise estimation of the cardinality and requires only a small amount of constant space to estimate cardinalities up to billions. The method centers on the idea that long runs of leading zeros, which can be efficiently computed using machine instructions, are unlikely in random bitstrings. When the read classification is finished, the taxon sketches are aggregated up the taxonomy tree by taking the maximum of each register value. The resulting sketches are the same as if the k-mers were counted at their whole lineage from the beginning. KrakenUniq then computes cardinality estimates using a formula, which has theoretical and practical advantages and does not require empirical bias correction factors .;Classification
LiveKraken;In metagenomics, Kraken is one of the most widely used tools due to its robustness and speed. Yet, the overall turnaround time of metagenomic analysis is hampered by the sequential paradigm of wet and dry lab. In urgent experiments, it can be crucial to gain a timely insight into a dataset. Here, we present LiveKraken, a real-time read classification tool based on the core algorithm of Kraken. LiveKraken uses streams of raw data from Illumina sequencers to classify reads taxonomically. This way, we are able to produce results identical to those of Kraken the moment the sequencer finishes. While the number of classified reads grows over time, false classifications appear in negligible numbers and proportions of identified taxa are only affected to a minor extent. Originally, Kraken has a linear workflow. Sequencing reads are read from FASTA or FASTQ files and subsequently classified using a pre-computed database. Since the reads are independent of each other, they can be processed in parallel. The lowest common ancestor  classification results found for each read are written to Kraken’s tabular report file. To make this workflow fit for the purpose of live taxonomic classification, similar to the approach taken in HiLive , a new sequence reader module was implemented which allows reading sequencing data from Illumina’s binary basecall format. LiveKraken can be used to analyze continuously and refine the metagenomic sample composition, using the same database structure as the original Kraken. Illumina sequencers process all reads in parallel in so called cycles, appending one base to all reads per cycle. For each cycle, BCL files are produced in Illumina’s BaseCalls directory, which is declared as input for LiveKraken instead of FASTA or FASTQ files. New data is collected by the BCL sequencing reader module in user-specified intervals of j sequencing cycles, starting with the first k-mer of size k. The collected data is sent to the classifier which refines the stored partial classification with the new sequence information. Temporary data structures of Kraken are stored for each read, such as the LCA list, a list of ambiguous nucleotides and the number of k-mer occurrences in the database. This leads to an overall increase of memory consumption proportional to the number of LCAs found for each read sequence. Additionally, and crucial for the iterative refinement, a variable is stored that is holding the position up to which each read was classified. After each refinement step, output in the same tabular format as known from Kraken is produced. This enables early classification while also ensuring that the classification output after reading the data from the last sequencing cycle is exactly the same that Kraken would produce. LiveKraken uses the same command line interface as Kraken.;Classification
Marvel;Here we present MARVEL, a tool for prediction of double-stranded DNA bacteriophage sequences in metagenomic bins. MARVEL uses a random forest machine learning approach. We show that three simple genomic features extracted from contig sequences were sufficient to achieve a good performance in separating bacterial from phage sequences: gene density, strand shifts, and fraction of significant hits to a viral protein database. We compared the performance of MARVEL to that of VirSorter and VirFinder, two popular programs for predicting viral sequences. Our results show that all three programs have comparable specificity, but MARVEL achieves much better performance on the recall (sensitivity) measure. This means that MARVEL should be able to identify many more phage sequences in metagenomic bins than heretofore has been possible. Feature Extraction and Classifier Development As previous studies have shown, genomic features such as DNA k-mer profiles and GC content can be strong signals in linking or differentiating genome sequences from bacteria and viruses . However, it is known that phages try to mimic host genome sequences in order to overcome their defenses . This causes classifiers based on k-mer frequencies to have poor performance in terms of overall accuracy and especially recall. In other words, when one of these classifiers identifies a phage genome, it is almost always correct, but it is likely to miss a majority of new phages present in environmental samples. Seeking more robust features, we focused our efforts on characteristics related to genome structure and protein translational mechanisms of each organism. Such characteristics require a second layer of information, which may be added by utilization of results from gene prediction programs, such as Prodigal and GeneMark . Therefore, we evaluated phage and bacterial genomes according to six of these genomic features extracted from the baseline dataset of RefSeq complete genomes. These six features are: average gene length, average spacing between genes, density of genes, frequency of strand shifts between neighboring genes, ATG relative frequency, and fraction of genes with significant hits against the pVOGs database . we tried different machine learning approaches based on the six features listed above. Specifically: support vector machine (SVM), logistic regression, neural networks, and random forest. Classifiers were evaluated using the training set as well as k-fold cross-validation, with the result that random forest was the best approach for our target prediction. Similar findings about suitability of random forest classifiers in bioinformatics have also been reported. Performance Comparison of MARVEL, VirSorter, and VirFinder Each contig of a simulated bin was individually given as input to VirSorter and VirFinder. For a given tool, an entire bin was considered to be a positive prediction in case at least one of its contigs were predicted as viral note that in our experimental set-up, there are no bins with both bacterial and viral sequences. A contig was considered viral if predicted in categories I and II for VirSorter, and if the q-value was less than or equal to  for VirFinder. Tests were performed for different fragment lengths and in 30 randomly sampled replicates of 100 bins 50 bacteria and 50 dsDNA phages. Average values of true positive rate, specificity, and accuracy were compared using the Wilcoxon signed-rank test and were considered significant if the p-value was less than.;Classification
SMART;Next generation sequencing technology has enabled characterization of metagenomics through massively parallel genomic DNA sequencing. The complexity and diversity of environmental samples such as the human gut microflora, combined with the sustained exponential growth in sequencing capacity, has led to the challenge of identifying microbial organisms by DNA sequence. We sought to validate a Scalable Metagenomics Alignment Research Tool SMART, a novel searching heuristic for shotgun metagenomics sequencing results. After retrieving all genomic DNA sequences from the NCBI GenBank, species were indexed using 4 base pair hashtable shards. A MapReduce searching strategy was used to distribute the search workload in a computing cluster environment. In addition, a one base pair permutation algorithm was used to account for single nucleotide polymorphisms and sequencing errors. Simulated datasets used to evaluate Kraken, a similar metagenomics classification tool, were used to measure and compare precision and accuracy. Finally using a same set of training sequences we compared Kraken, CLARK, and SMART within the same computing environment. Utilizing 12 computational nodes, we completed the classification of all datasets in under 10 min each using exact matching with an average throughput of reads classified per minute. With permutation matching, we achieved sensitivity greater than 83 and precision greater than 94 with simulated datasets at the species classification level. We demonstrated the application of this technique applied to conjunctival and gut microbiome metagenomics sequencing results. In our head to head comparison, SMART and CLARK had similar accuracy gains over Kraken at the species classification level, but SMART required approximately half the amount of RAM of CLARK. SMART is the first scalable, efficient, and rapid metagenomics classification algorithm capable of matching against all the species and sequences present in the NCBI GenBank and allows for a single step classification of microorganisms as well as large plant, mammalian, or invertebrate genomes from which the metagenomic sample may have been derived.;Classification
Taxonomer;"High-throughput sequencing enables unbiased profiling of microbial communities, universal pathogen detection, and host response to infectious diseases. However, computation times and algorithmic inaccuracies have hindered adoption. We present Taxonomer, an ultrafast, web-tool for comprehensive metagenomics data analysis and interactive results visualization. Taxonomer is unique in providing integrated nucleotide and protein-based classification and simultaneous host messenger RNA  transcript profiling. Using real-world case-studies, we show that Taxonomer detects previously unrecognized infections and reveals antiviral host mRNA expression profiles. To facilitate data-sharing across geographic distances in outbreak settings, Taxonomer is publicly available through a web-based user interface. Taxonomer enables rapid, accurate, and interactive analyses of metagenomics data on personal computers and mobile devices. Taxonomer is an ultrafast, user-friendly, web-based metagenomic sequence analysis tool. It enables novel analysis modalities in an easy-to-use fashion including: comprehensive panmicrobial detection and discovery;  host messenger RNA  response profiling;  interactive result visualization; and  access through a web-based user interface, which eliminates the need for specialized hardware or expertise. These applications are enabled through a modular design based on four integrated tools: Binner, Classifier, Protonomer, and Afterburner . Taxonomer can be used in the analysis of DNA and/or RNA sequencing data as well as for short reads and longer contigs assembled from metagenomics datasets. Taxonomer operates at speeds comparable to the fastest tools, Kraken and CLARK , which provide only some of Taxonomer’s functionality. Unlike Kraken and CLARK, Taxonomer supports integrated nucleotide and protein-based classification for detection of highly diverse viral sequences at faster speeds than alignment-based tools with similar functionality. Raw FASTA, FASTQ, or SRA files (with or without gzip compression) are the input for Taxonomer. For paired-end data, mate pairs are analyzed jointly. Taxonomer consists of four main modules. The Binner module categorizes bins reads into broad taxonomic groups (host and microbial) followed by comprehensive microbial and host gene expression profiling at the nucleotide Classifier module or amino acid-level Protonomer and Afterburner modules. Normalized host gene expression gene-level read counts and microbial profiles can be downloaded. Classifier module Nucleotide-level classification in Taxonomer is based on exact k-mer matching. Taxonomer uses databases that are optimized for rapid k-mer queries that store every reference in which a k-mer is found as well as an associated k-mer weight for every reference. Each read is assigned to the reference that has the maximum total k-mer weight. In the case of a tie, the query sequence is assigned to the taxonomic lowest common ancestor LCA. The classifier module is used for rRNA-based bacterial and fungal characterization and host mRNA expression profiling. Protonomer module Taxonomer uses a novel a non-degenerate mapping scheme between amino acids and corresponding, artificial DNA sequences to facilitate mapping in protein space with the same algorithm used for classification in nucleotide space. Query reads are translated into all six reading frames based on the same non-degenerate translation scheme and classified in all frames. K-mer weighting and read classification assignment are performed as with the Classifier module. Protonomer is used to classify viruses in protein space because of their high mutation rates, genetic variability, and incomplete reference databases. Afterburner To increase recovery of distantly homologous viral proteins, Taxonomer offers two options. First, unclassified reads can be further analyzed using the Afterburner module, a degenerate k-mer matching engine that employs a collapsed amino-acid alphabet. In a manner similar to that employed by DIAMOND, we used k-means clustering on the BLOSUM matrix to generate a compressed amino acid alphabet. By using the collapsed amino acid alphabet, Taxonomer achieves higher sensitivity in classification with sequences that are more diverged at the expense of a higher false-positive rate when compared with Protonomer.";Classification
DisCVR;High-throughput sequencing  enables most pathogens in a clinical sample to be detected from a single analysis, thereby providing novel opportunities for diagnosis, surveillance, and epidemiology. However, this powerful technology is difficult to apply in diagnostic laboratories because of its computational and bioinformatic demands. We have developed DisCVR, which detects known human viruses in clinical samples by matching sample k-mers (twenty-two nucleotide sequences) to k-mers from taxonomically labeled viral genomes. DisCVR was validated using published HTS data for eighty-nine clinical samples from adults with upper respiratory tract infections. These samples had been tested for viruses metagenomically and also by real-time polymerase chain reaction assay, which is the standard diagnostic method. DisCVR detected human viruses with high sensitivity  and specificity , and was able to detect mixed infections. Moreover, it produced results comparable to those in a published metagenomic analysis of 177 blood samples from patients in Nigeria. DisCVR has been designed as a user-friendly tool for detecting human viruses from HTS data using computers with limited RAM and processing power, and includes a graphical user interface to help users interpret and validate the output. The k-mer databases A k-mer is a short sequence of k nucleotides. A k-mer dataset is generated iteratively by sliding a window of size k along a sequence one nucleotide at a time. Extracting k-mers and counting their frequencies in a set of sequences can be computationally intensive, especially when k is large and the sequences are numerous. Dedicated k-mer counting programs, such as Jellyfish and Khmer , can be incorporated into abundance-based tools in order to optimize speed. KAnalyze was chosen for integration into DisCVR because the k-mers it generates are sorted lexicographically, thus making the search for matches very efficient. KAnalyze also uses the canonical representation of a k-mer, which is lexicographically the smaller of a k-mer and its reverse complement. DisCVR also allows the user to create customized databases and sets of reference sequences using the command-line utility scripts provided with the DisCVR distribution. The database build module involves selecting the relevant viral dataset, collecting the k-mers, and removing those that are shared with the host or are of low complexity. Each remaining k-mer is then identified with a taxonomic tag and an indication of the number of times it occurs in the sequences. The k-mers are further subdivided into those that exist in a single virus and those that exist in multiple viruses. These assignments are made at the level of species and strain and are used in the output to illustrate the degree of specificity of the k-mers matching a virus. Sample classification To analyze an HTS dataset, the file is loaded into DisCVR via the GUI. The k-mers are extracted and their frequencies are calculated, the single copy k-mers, which are mainly attributed to sequencing errors and low-complexity k-mers, which commonly give confounding matches that have nothing to do with homology , are filtered out, and the remaining k-mers are compared with the chosen virus k-mer database. As the number of k-mers in the sample can be enormous, various data structures were considered to optimize the classification on machines with limited RAM. DisCVR uses a fast searching algorithm that groups similar k-mers together. Briefly, the k-mers in the virus database are divided among smaller sub-files according to the first five nucleotides. The same procedure is used to divide the k-mers derived from the entire HTS dataset. Searching commences by loading the corresponding sub-files from the virus k-mer database and the sample k-mers into memory, and performing a binary search for the presence of each sample k-mer among the database k-mers. Only matched k-mers are retrieved. Finally, DisCVR displays a straightforward list of all the virus hits detected, along with summary statistics and taxonomic information on the sample k-mers. DisCVR helps the user to assess the significance of the findings by facilitating an examination of k-mer distribution allowing up to three mismatches across a reference sequence representing the target genome. As an alternative, it also incorporates an examination of sequence read distribution carried out by using Tanoti , which is a BLAST guided, reference based short read aligner that is particularly tolerant of mismatches. In each case, the output is a graph showing the depth and coverage of k-mers or sequence reads across the reference genome and a summary of statistics for the mapping results.;VirusDetection
Metavisitor;"Metavisitor is a software package that allows biologists and clinicians without specialized bioinformatics expertise to detect and assemble viral genomes from deep sequence datasets. The package is composed of a set of modular bioinformatic tools and workflows that are implemented in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules. Metavisitor works with DNA, RNA or small RNA sequencing data over a range of read lengths and can use a combination of de novo and guided approaches to assemble genomes from sequencing reads. We show that the software has the potential for quick diagnosis as well as discovery of viruses from a vast array of organisms. Importantly, we provide here executable Metavisitor use cases, which increase the accessibility and transparency of the software, ultimately enabling biologists or clinicians to focus on biological or medical questions. Metavisitor consists of a set of Galaxy tools that can be combined to retrieve up-to-date nucleotide as well as protein sequences of viral genomes deposited in Genbank and index these sequences for subsequent alignments; extract sequencing reads that do not align to the host genomes, known symbionts or parasites;  perform de novo assembly of these reads using assembly tools available in Galaxy, align the de novo contigs against the viral nucleotide or protein blast databases using blastn or blastx, respectively, and generate reports from blast outputs to help in known viruses diagnosis or in candidate virus discovery;  use CAP3 , blast and viral scaffolds for selected viruses to generate guided final viral sequence assemblies of blast sequence hits. Below, we group analysis steps in functional tasks  and provide details on the Metavisitor tools. These tasks are linked together to build full workflows adapted to the analysis of the use cases described in the result section. The Get reference viral sequences task is performed using the Retrieve FASTA from NCBI tool that sends a query string to the Genbank database  and retrieves the corresponding nucleotide or protein sequences. For the viral nucleotide and protein sequences , we used the tool to query Genbank and retrieve viruses sequences filtered out from cellular organisms and bacteriophage sequences . However, users can change the tool settings by entering query strings that fit their specific needs. Bowtie as well as bowtie2 indexes of the nucleotide sequences have been generated in the Mississippi Galaxy instance using the corresponding data manager Galaxy tools. Finally, users can upload their own viral nucleotide and protein sequences using ftp and transfer them to a Galaxy history, where they can use the Galaxy data manager tools to produce the blast and bowtie indexes necessary for Metavisitor.  Prepare data The “Prepare data” task processes Illumina sequencing datasets in order to optimize the subsequent de novo assembly of viral sequencing reads. Fastq files of sequence reads are first clipped from library adapters and converted to fasta format using our tool “Clip adapter” tool . The clipped reads may be further converted to a fasta file of unique sequences headed by a character string that contains a unique identifier and the number of times that the sequences were found in the dataset, thus reducing the size of the dataset without loss of information. Clipped reads are then depleted from non-viral sequences by sequential alignments to the host genome, to other genomes from known or potential symbionts and parasites, as well as to PhiX174 genome sequences which are commonly used as internal controls in Illumina sequencing and may contaminate the datasets. The sequence reads that did not match the reference genomes are retained and returned as a fasta file that can be used subsequently by a de novo assembly tool. Note that these subtraction steps can be skipped when the host genome is not known or if the aim of the user is to discover endogenous viral elements.  Assembly, Blast and Parsing De novo assembly. In the task Assemble, Blast and Parse , retained RNA sequences are subjected to de novo assembly. For short reads , we tested several rounds of de novo assembly by Velvet using the Oases software package For reads between 50 and 100, we also used the Oases. we used the Trinity assembly software which is available as a Galaxy tool and was reported to performs well with long reads. Trinity as well as SPAdes assembly softwares were also tested as alternate. It is noteworthy that users can adapt a Metavisitor workflow using any assembly software available in the Galaxy tool shed. Blast. Next, de novo assembled contigs are aligned to both nucleotide and protein BLAST databases built from the viral reference sequences using the blastn or blastx Galaxy tools. Finally, the Blast aligned sequences file contains contigs that produced significant blast hits, whereas the Blast unaligned sequences file contains those that did not.  Blast Guided Scaffolding This last task allows to integrate hit sequences matching a candidate virus into a virus scaffold . First, blastn or blastx hits are retrieved from the hits file using the tool Pick Fasta sequences and the appropriate query string. Next, these hit sequences can be further clustered in longer contigs using the “cap3 Sequence Assembly” Galaxy tool  adapted from CAP3. Finally, if there are still multiple unlinked contigs at this stage, they can be integrated in the matched viral sequence taken as a scaffold . This scaffolding is achieved by retrieving the viral sequence from the NCBI nucleotide database to be used as the backbone of the scaffold, generating a blast index from this sequence and aligning the contigs to this index with blastn or tblastx tools running the blast_to_scaffold tools , taking as inputs the contigs, the viral guide sequence and the blastn or blastx output .";VirusDetection
VirFind;"Next generation sequencing (NGS) has revolutionized virus discovery. Notwithstanding, a vertical pipeline, from sample preparation to data analysis, has not been available to the plant virology community. We developed a degenerate oligonucleotide primed RT-PCR method with multiple barcodes for NGS, and constructed VirFind, a bioinformatics tool specifically for virus detection and discovery able to: (i) map and filter out host reads, (ii) deliver files of virus reads with taxonomic information and corresponding Blastn and Blastx reports, and (iii) perform conserved domain search for reads of unknown origin. The pipeline was used to process more than 30 samples resulting in the detection of all viruses known to infect the processed samples, the extension of the genomic sequences of others, and the discovery of several novel viruses. VirFind was tested by four external users with datasets from plants or insects, demonstrating its potential as a universal virus detection and discovery tool.
VirFind was developed as an automated tool to process NGS outputs.. Briefly, NGS sequence files are converted to fasta format. Sequences are then trimmed at both 5 and 3 ends to remove any adapters and primers, and collapsed using FASTX-Toolkit and seq_crumbs . Host sequences are removed from further processing after mapping to reference genomes using Bowtie 2 . De novo sequence assembly is performed on unmapped reads using Velvet with k-mer . Short sequences may lead to false positives in Blast and for this reason only contigs and singlets are subjected to Blastn search against the GenBank nt database. Hits to GenBank nt are filtered out with virus and non-virus fasta reads together with their corresponding Blastn reports in tabular format. Sequences without any matches are then subjected to Blastx search against all GenBank virus protein sequences. CAP3 assemblies are also constructed on top of the Blast outputs. users need to complete a sequence submission form that contains the following options:  trimming of adapter primer, mapping to reference genomes to remove host sequences, cut-off e-values of the Blastn and Blastx steps to define sequence relatedness to those found in GenBank, and  conserved domain search of the remaining unmatched sequences. Sequence files are then uploaded to the VirFind ftp server for analysis. When all steps are completed, output files are compressed and mailed to users with information on how to download results from the server. Virus detection and discovery NGS dataset were each spiked with a random virus viroid GenBank sequence which was used to test the ability of VirFind to detect a single copy virus-like sequence.";VirusDetection
VirusFinder;"Next generation sequencing technologies allow us to explore virus interactions with host genomes that lead to carcinogenesis or other diseases; however, this effort is largely hindered by the dearth of efficient computational tools. Here, we present a new tool, VirusFinder, for the identification of viruses and their integration sites in host genomes using NGS data, including whole transcriptome sequencing RNA-Seq, whole genome sequencing , and targeted sequencing data. VirusFinder’s unique features include the characterization of insertion loci of virus of arbitrary type in the host genome and high accuracy and computational efficiency as a result of its well-designed pipeline. the pipeline of VirusFinder, which overall follows a three-step procedure: preprocessing, virus detection, and virus integration site detection. Preprocessing VirusFinder’s input can either be raw sequencing reads in Fastq format or an alignment file in BAM format. If user provides only raw sequencing reads, VirusFinder will first use the alignment tool Bowtie to map these reads to a human reference genome. VirusFinder runs Bowtie 2 in its sensitive end-to-end mode, in which Bowtie 2 does not trim characters from short reads in order to achieve high alignment speed. With the alignment file generated by Bowtie 2 or provided by the user, VirusFinder then garners all reads unmapped to the human reference genome for downstream analysis. Here in this step, user is allowed to provide the sequence of the virus being examined as an input parameter of VirusFinder. VirusFinder will skip step of the pipeline if user provides the virus sequence. Virus detection This step is used to detect the specific type of viruses present in the sample. This step will be skipped if user supplies the virus sequence to VirusFinder. If the virus type is unknown, however, VirusFinder first aligns the unmapped reads collected in step 1 to a virus database. The current version of VirusFinder uses the same virus database, as the one included with the RINS package. This virus database contains viruses of all known classes . User can replace virus.fa with an alternative virus database, Genome Information Broker for Viruses , which collects virus reference sequences, or a smaller set of viruses of user interest. Next, VirusFinder de novo assembles the reads aligned to the virus DB into contigs and maps contigs to both the human genome and the virus database. All contigs that are mapped to the human genome are discarded. The alignment scores of the nonhuman contigs, which align only to the virus DB, are then used to rank the viruses, to which they are mapped. The sequence of the top ranking virus is then applied to the next analysis step. It may be worth mentioning that our virus detection method as described here used RINS as a starting point. However, different from RINS that identifies viruses by recruiting all reads mapped to the virus database, which can at the same time align to the human genome, VirusFinder utilizes only the reads mapped to the virus DB and unmapped to the human genome for virus detection.";VirusDetection
VirusSeeker;"The advent of Next Generation Sequencing  has vastly increased our ability to discover novel viruses and to systematically define the spectrum of viruses present in a given specimen. Such studies have led to the discovery of novel viral pathogens as well as broader associations of the virome with diverse diseases including inflammatory bowel disease, severe acute malnutrition and AIDS. Critical to the success of these efforts are robust bioinformatic pipelines for rapid classification of microbial sequences. Existing computational tools are typically focused on either eukaryotic virus discovery or virome composition analysis but not both. Here we present VirusSeeker, a BLAST-based NGS data analysis pipeline designed for both purposes. VirusSeeker has been successfully applied in several previously published virome studies. Here we demonstrate the functionality of VirusSeeker in both novel virus discovery and virome composition analysis. VirusSeeker The VS-Virome pipeline is controlled by a master Perl script. The input to the pipeline is a directory path. The directory holds sequencing data from one or multiple samples each with two files containing FASTQ format sequence reads The preprocessing of sequence files consists of the following steps: trim adapter and or primer sequences using cutadapt ; join paired end reads together to form a longer read if they overlap by defined criteria using fastq-join in the ea-utils package quality filtering of reads trim low quality nucleotides, poly A sequences, remove reads with low average quality score) to obtain good quality sequences using PRINSEQ; Remove redundant sequences. Identical or nearly-identical sequences are frequently present in NGS data, either due to the sheer depth of NGS or because many of the pre-sequencing sample preparation methods involve PCR amplification. To reduce the computing cost of downstream analysis and to reduce amplification artifacts, CD-HIT is used to cluster similar sequences. The default parameters in VS-Virome are set to cluster sequences that share ≥95% identity over 95% of the sequence length.";VirusDetection
Truffle;"The use of next-generation sequencing for plant virus detection is rapidly expanding, necessitating the development of bioinformatic pipelines to support analysis of these large datasets. Pipelines need to be easy implementable to mitigate potential insufficient computational infrastructure and/or skills. In this study user-friendly software was developed for the targeted detection of plant viruses based on e-probes. It can be used for both custom e-probe design, as well as screening preloaded probes against raw NGS data for virus detection. The pipeline was compared to de novo assembly-based virus detection in grapevine and produced comparable results, requiring less time and computational resources. The software, named Truffle, is available for the design and screening of e-probes tailored for user-specific virus species and data, along with preloaded probe-sets for grapevine virus detection.
Trimmed reads were assembled into contigs using CLC Genomics Workbench. The minimum contig length was set to 250 nts while automatic bubble-size and word-size detection was applied. To determine the viral status of the samples all contigs were first aligned using blastn from Blastþ against GenBank's nt database, using default parameters. Contigs, which could not be annotated with blastn, were further analysed using tblastx against the same nt database also using default parameters. Filtered reads were additionally submitted to VirFind , applying default parameters, to determine the viral status. Firstly, probes can be designed which are customised for a user's specific virus species of interest. For probe design the genome of the target virus is first compared to that of a closely related virus using NUCmer , which forms part of the MUMmer package , to identify homologous genomic regions. Unique targetspecific regions, 20 nts and longer, are extracted and serve as candidate probes. After removing sequences containing homooligomers the candidate probes are aligned to NCBI's online GenBank nt database, removing all probes that hit any sequence other than the virus of interest. An alignment with an e-value of or less is considered a hit. The remaining probes form the virus-specific e-probes. A decoy set of sequences is also created, which comprises of the reverse sequences of the e-probes. For the second application of Truffle, Blast is used determine the viral status of a sample. The probes and decoys are aligned, with blastn , against a database composed of the raw NGS data. A score is generated for each probe and decoy based on the number of hits, e-value and percentage of query coverage . Depending on the nature of the score-data one of the following statistical tests is performed to compare the sets of probe and decoy scores, the parametric student t-test for normally distributed data with equal variance, the Welch's t-test for normally distributed data with unequal variance or the Wilcoxon Ranksum test for data which are not normally distributed. Samples with a p-value smaller than or equal to  are considered to be positive for a specific virus, while samples with a p-value greater than of equal to  are considered to be negative . Samples rendering a p-value between these two margins are only suspected to be positive and indicated as such.";VirusDetection
PhiSpy;"Prophages are phages in lysogeny that are integrated into, and replicated as part of, the host bacterial genome. These mobile elements can have tremendous impact on their bacterial hosts’ genomes and phenotypes, which may lead to strain emergence and diversification, increased virulence or antibiotic resistance. However, finding prophages in microbial genomes remains a problem with no definitive solution. The majority of existing tools rely on detecting genomic regions enriched in protein-coding genes with known phage homologs, which hinders the de novo discovery of phage regions. In this study, a weighted phage detection algorithm, PhiSpy was developed based on seven distinctive characteristics of prophages, i.e. protein length, transcription strand directionality, customized AT and GC skew, the abundance of unique phage words, phage insertion points and the similarity of phage proteins. The first five characteristics are capable of identifying prophages without any sequence similarity with known phage genes. PhiSpy locates prophages by ranking genomic regions enriched in distinctive phage traits, which leads to the successful prediction of prophages in complete bacterial genomes with a false-negative rate and  false-positive rate.
PhiSpy has four steps. Each step is described below. Calculation of different characteristics The first step calculates different parameters for the whole genome. The calculation of these parameters depends on a group of genes rather than a single gene. Therefore, for a complete genome, these parameters were computed using a sliding window of n genes. Transcription strand orientation. For a given window size, the genes were partitioned in such a way so that all consecutive genes in a particular partition pointed in the same direction. The sum of the number of genes in the two largest partitions was taken for the window to maximize the number of consecutive genes in the same direction. Abundance of phage words. A ‘word’ is defined as a set of 12 consecutive base pairs. Each gene was split into long non-overlapping words four consecutive amino acids each. A unique phage word library was built based on the bacterial genomes that have well-annotated prophages. The second step of PhiSpy is to classify a window as a bacterial or a prophage window using random forests. A random forest is a classification algorithm that consists of multiple independent decision trees. The random forest requires a training set with multiple variables to build the forest of decision trees. In this case, there were five parameters whose values vary among distantly related genomes. If the similarities between two genomes were evolutionary significant, then they were considered as closely related genomes; otherwise, they were considered as distantly related genomes. Therefore, for every group of closely related genomes, a different training set was constructed. Training test set. Processing the final rank for each gene The third step of PhiSpy provides a prediction status—either 0 for non prophage genes or 1 for prophage genes for each gene in the genome. Therefore, the final rank of a particular gene was measured by taking the average rank of the window in which the gene participated. Evaluation of the prediction The final step is to define the att sites for the predicted prophages and the overall evaluation of the prophages. When phages integrate into their hosts’ genome, they are usually bounded by two att sites a short repeated sequence that flanks the insertion site.";VirusDetection
PHAST;PHAge Search Tool PHAST is a web server designed to rapidly and accurately identify, annotate and graphically display prophage sequences within bacterial genomes or plasmids. It accepts either raw DNA sequence data or partially annotated GenBank formatted data and rapidly performs a number of database comparisons as well as phage ‘cornerstone’ feature identification steps to locate, annotate and display prophage sequences and prophage features. Relative to other prophage identification tools, PHAST is faster and more sensitive. It is also able to process and annotate both raw DNA sequence data and Genbank files, provide richly annotated tables on prophage features and prophage ‘quality’ and distinguish between intact and incomplete prophage. PHAST also generates downloadable, high quality, interactive graphics that display all identified prophage components in both circular and linear genomic views. PHAST is an integrated search and annotation tool that combines genome-scale ORF prediction and translation via GLIMMER, protein identification via BLAST matching and annotation by homology, phage sequence identification via BLAST matching to a phage-specific sequence database, tRNA identification, attachment site recognition and gene clustering density measurements using density-based spatial clustering of applications with noise DBSCAN and sequence annotation text mining. In addition to these basic operations, PHAST also evaluates the completeness of the putative prophage, tabulates data on the phage or phage-like features and renders the data into several colorful graphs and charts.;VirusDetection
PhageFinder;"Phage_Finder, a heuristic computer program, was created to identify prophage regions in completed bacterial genomes. Using a test dataset of bacterial genomes whose prophages have been manually identified, Phage_Finder found  the regions, resulting in false positive and false negative prophages. A search of complete bacterial genomes predicted putative prophage regions, accounting for the total bacterial DNA. Analysis of the putative attachment sites revealed tRNAs are targets for integration slightly more frequently  than intergenic or intragenic  regions, while tmRNAs were targeted in the regions. Mapping of the insertion point on a consensus tRNA molecule revealed novel insertion points on the side of the loop, the side of the anticodon loop and the anticodon. A novel method of constructing phylogenetic trees of phages and prophages was developed based on the mean of the BLAST score ratio  of the phage/ prophage proteomes. This method verified many known bacteriophage groups, making this a useful tool for predicting the relationships of prophages from bacterial genomes. Identification of prophage regions One of the original intentions of Phage_Finder was to have a program that can distinguish between largely intact, possibly functional prophages versus small regions or clusters of prophage remnants and other mobile elements. It takes advantage of several features of functional prophages to filter out unwanted fragmented regions. The first analysis that Phage_Finder does is to count the number of valid BLASTP phage matches within a user-defined window size, sliding by a user-defined step size until the size of the genome is reached.The decision to include a gene within a particular phage region is made in the following order:  if the protein has a phage HMM hit, then include;  if it has a phage database BLASTP valid match, then include;  if the gene is a tRNA or tmRNA, then include; if either of the next three genes are tRNAs or tmRNAs, then include; if the gene has annotation that has been observed in known phages and there are at least three valid BLASTP phage database matches in the current window, then include ; if the gene has annotation that has been observed in known phages and there are at least two valid BLASTP phage database matches in the next window, then include; if there are at least three valid BLASTP phage database matches in the next window, then include; lastly, if there are at least two valid BLASTP matches in the current step and the current gene is before the matching gene, then include every gene up to the gene with the database match. Degenerate regions are determined after analysis of putative attachment sites. A region is defined as type prophage if: it has a core HMM match or  it has a lytic HMM match and a tail HMM match and an integrase HMM match. If the region has a lytic HMM match and a tail HMM match, but no integrase HMM match, then the region is defined as type bacteriocin, which is analogous to the phage-like bacteriocins (pyocins) in P.aeruginosa or monocins in L.monocytogenes .";VirusDetection
DeepVirFinder;The recent development of metagenomic sequencing makes it possible to massively sequence microbial genomes including viral genomes without the need for laboratory culture. Existing reference-based and gene homology-based methods are not efﬁcient in identifying unknown viruses or short viral sequences from metagenomic data. Here we developed a reference-free and alignment-free machine learning method, DeepVirFinder, for identifying viral sequences in metagenomic data using deep learning. Enlarging the training data with additional millions of puriﬁed viral sequences from metavirome samples further improved the accuracy for identifying virus groups that are under-represented. Applying DeepVirFinder to real human gut metagenomic samples, we identiﬁed viral sequences belonging to bins in patients with colorectal carcinoma . Ten bins were found associated with the cancer status, suggesting viruses may play important roles in CRC. Powered by deep learning and high throughput sequencing metagenomic data, DeepVirFinder signiﬁcantly improved the accuracy of viral identiﬁcation and will assist the study of viruses in the era of metagenomics.  Thus, we designed a deep learning methods using ConvNets, where the filters in ConvNets are able to capture sequence patterns. The filters are represented in the form of weight matrices. This representation is similar to PWM. The generalization from k-mers to ConvNets provides a more general model and potentially gives better prediction accuracy. The objective function is to minimize the binary crossentropy loss between the predicted score Yfinal and the true labels (0 for prokaryotic sequences, and 1 for virus sequences). The training dataset is iteratively fed into the model in batches of size 150. One iteration of finishing feding batches of the whole training dataset is called one epoch. The parameters in the neural networks were updated through back-propagation using Adam optimization algorithm for stochastic gradient descent with learning rate . Dropout regularization of rate  are applied after the max pooling layer, and after the fully connected layer, to reduce overfitting in neural networks by randomly dropping out a few dimensions. This convolutional neural network has three critical hyper parameters, the length of motifs , the number of motifs, and the number of epochs for training. The first two determine the complexity of the model, and the third one controls the balance between overfitting and underfitting. To find the best parameter settings, we trained the model with different combinations of the three parameters using the data before January 2014, and evaluated the model performance using AUROC on the validation dataset. Collection of metavirome datasets To achieve high prediction accuracy, a deep learning algorithm needs a large amount of training data. Though a large number of training sequences were obtained from RefSeq, there is a potential to enlarge the training dataset by including viral sequences from metavirome sequencing data. Metavirome sequencing targets at sequencing mainly viruses by removing prokaryotic cells in samples using the physical filters. Metavirome sequencing does not rely on culturing viruses in the lab, so it is able to capture both cultivated and uncultivated viruses, representing the true viral diversity.;VirusIdentification
ViraMiner;Despite its clinical importance, detection of highly divergent or yet unknown viruses is a major challenge. When human samples are sequenced, conventional alignments classify many assembled contigs as “unknown” since many of the sequences are not similar to known genomes. In this work, we developed ViraMiner, a deep learning-based method to identify viruses in various human biospecimens. ViraMiner contains two branches of Convolutional Neural Networks designed to detect both patterns and pattern-frequencies on raw metagenomics contigs. The training dataset included sequences obtained from 19 metagenomic experiments which were analyzed and labeled by BLAST. The model achieves significantly improved accuracy compared to other machine learning methods for viral genome classification.. To our knowledge, this is the first machine learning methodology that can detect the presence of viral sequences among raw metagenomic contigs from diverse human samples. We suggest that the proposed model captures different types of information of genome composition, and can be used as a recommendation system to further investigate sequences labeled as “unknown” by conventional alignment methods. Exploring these highly-divergent viruses, in turn, can enhance our knowledge of infectious causes of diseases. The goal of those analyses was to detect viral genomes or other microorganisms in diseased individuals or in their matched control subjects. Bioinformatics All the sequencing experiments were processed and analyzed using a benchmarked bioinformatics workflow . To summarize, we start analysis with quality checking where reads are filtered based on their Phred quality score. Afterwards, quality checked reads that align to human, bacterial, phage and vector DNA with identity of their length are subtracted from further analysis using BWA-MEM . The reads that are left are normalized and then assembled using the IDBA-UD , Trinity , SOAPdenovo, and SOAPdenovo-Trans assemblers. Machine learning methods Convolutional neural networks (CNNs). Convolutional neural networks (CNNs) are a type of feed-forward neural networks . In addition to fully-connected layers that have all-to-all connections, CNNs by definition also contain convolutional layers. Convolutional layers have partial connectivity and use the same parameters repeatedly. In supervised learning settings, CNNs learn to perform a task by observing input-output pairs. The learning is achieved by minimizing the error (between model’s output and the known true output) via gradient descent . CNNs are most widely used in image processing, but have been successfully applied in various fields , including analysis of biological sequences such as DNA and amino acid sequences . The convolutional layers treat their inputs as multidimensional arrays. . A convolutional layer is characterized by a set of learnable filters that are convolved along the input DNA sequence (or along the width and height of an image). Filters are smaller than the input and can be applied at multiple locations along the input. At each position where the filter is applied, a dot product between the weights of the filter and the local input values is computed. The dot products resulting from applying the same filter to different positions along the sequence form a feature map. A convolutional layer usually applies many filters and hence its output is a set of feature maps, one per filter. On this output, further computations can be performed such as applying an activation function, pooling or adding further layers. The parameter values in the filters, like all other learnable parameters in the model, are optimized by the gradient descent algorithm. CNN architectures for predicting the viral nature of sequences. In this work we use a convolutional neural network called ViraMiner to predict whether a given DNA sequence is of a viral origin or not. This model contains two convolutional branches that contribute different types of information to the final fully-connected decision-making layer. This model is partly based on the DeepVirFinder (DVF) architecture. The DVF model is a Convolutional Neural Network (CNN) that applies the max operator on the outputs of each convolutional filter (on each feature map).;VirusIdentification
VirFinder;Identifying viral sequences in mixed metagenomes containing both viral and host contigs is a critical first step in analyzing the viral component of samples. Current tools for distinguishing prokaryotic virus and host contigs primarily use gene-based similarity approaches. Such approaches can significantly limit results especially for short contigs that have few predicted proteins or lack proteins with similarity to previously known viruses. We have developed VirFinder, the first k-mer frequency based, machine learning method for virus contig identification that entirely avoids gene-based similarity searches. VirFinder instead identifies viral sequences based on our empirical observation that viruses and hosts have discernibly different k-mer signatures. VirFinder’s performance in correctly identifying viral sequences was tested by training its machine learning model on sequences from host and viral genomes .VirFinder had significantly better rates of identifying true viral contigs than VirSorter, the current state-of-the-art gene-based virus classification tool, when evaluated with either contigs subsampled from complete genomes or assembled from a simulated human gut metagenome. For example, for contigs subsampled from complete genomes, VirFinder had higher than VirSorter for contigs, respectively, at the same false positive rates as VirSorter, thus VirFinder works considerably better for small contigs than VirSorter. VirFinder furthermore identified several recently sequenced virus genomes that VirSorter did not and that have no nucleotide similarity to previously sequenced viruses, demonstrating VirFinder’s potential advantage in identifying novel viral sequences. The first dataset, referred to as the “training set”, has 78 samples comprised of 40 samples from 31 healthy patients and 38 samples from 25 liver cirrhosis patients. The second dataset, referring to as the “testing set”, has 230 samples comprised of 103 samples from 83 healthy patients and 127 samples from 98 liver cirrhosis patients. Megahit  was used to cross-assemble the 78 sample training dataset using the default settings since the 230 sample dataset was too large for assembly. COCACOLA was used to separately cluster viral contigs predicted by VirFinder and VirSorter based on sequence tetranucleotide frequencies and contig coverages normalized by contig length and number of mapped reads in samples. Contig coverages (RPKMs) were determined by mapping sample reads with Bowtie2 using the default settings and were averaged for each bin. Averaged bin RPKMs were used to train a classification model to classify the disease status . A logistic regression model with lasso regularization was used in order to enhance the prediction accuracy and interpretability. Thus, a subset of viral bins was chosen to achieve the best prediction accuracy. To assess the classification model, the average RPKM of bins in the second dataset with 230 samples were used to test the classification model, and ROC curves were used for evaluation. Two-way hierarchical clustering was performed using the average RPKM coverages of the 116 VirFinder contig bins using all 78 training set samples and 78 samples randomly selected from the 230 sample testing dataset. Distances were computed using Euclidean distance and were clustered with complete linkage method in R. Blast analyses were used to assess if predicted viral contigs assembled from the cirrhosis study samples had similarity to previously reported sequences. Blastn and blastp searches were performed with default settings against NCBI’s non-redundant nucleotide (nt) and protein (nr) databases .;VirusIdentification
VirNet;"Viral reads identification is one of the important steps in metagenomic data analysis. It shows up the diversity of the microbial communities and the functional characteristics of microorganisms. There are various tools that can identify viral reads in mixed metagenomic data using similarity and statistical tools. However, the lack of available genome diversity is a serious limitation to the existing techniques. In this work, we applied natural language processing approaches for document classification in analyzing metagenomic sequences. Text featurization is presented by treating DNA similar to natural language. These techniques reveal the importance of using the text feature extraction pipeline in sequence identification by transforming DNA base pairs into a set of characters with a term frequency and inverse document frequency techniques. Various machine learning classification algorithms are applied to viral identification tasks such as logistic regression and multi-layer perceptron. Moreover, we compared classical machine learning algorithms with VirFinder and VirNet, our deep attention model for viral reads identification on generated fragments of viruses and bacteria for benchmarking viral reads identification tools. Then, as a verification of our tool, It was applied to a simulated microbiome and virome data for tool verification and real metagenomic data of Roche 454 and Illumina for a case study.
Viral sequence identification can be formulated as Natural Language Processing  problem . Every single nucleotide can be considered as a word or a character in the sequence similar to the sequence of words in the natural language. Moreover, a group of nucleotide, such as consecutive three nucleotide codon can be treated as a word. There is an analogy between DNA sequences and text. A sequence of English words can be classified in sports category, similarly, DNA sequence of viruses can be identified as well. Text classification is one of the essential tasks in NLP domain. There is a wide range of applications using text classification algorithms, such as topic modeling , spam detection, language detection , and sentiment analysis . Before building text classification models, The text has to be converted into numerical variables, which can represent the text content. Representation of this text is required based on the type of task to be applied. VirNet was experimented fragments with various lengths. These fragments were generated from our testing set of viruses and prokaryotes RefSeq genomes. VirNet outperformed VirFinder on all fragments lengths. It presents the ability of VirNet model to predict viral reads in mixed metagenomic data. The accuracy of VirNet was on , whereas VirFinder was . Furthermore, VirNet was able to predict the short fragments , while VirFinder (VF) is not able to identify viral reads less than because it was trained by  as a shortest fragment length., the sensitivity of our model to the length of fragment. The accuracy was dropped by decreasing the fragment’s length.";VirusIdentification
VirSorter;"Viruses of microbes impact all ecosystems where microbes drive key energy and substrate transformations including the oceans, humans and industrial fermenters. However, despite this recognized importance, our understanding of viral diversity and impacts remains limited by too few model systems and reference genomes. One way to fill these gaps in our knowledge of viral diversity is through the detection of viral signal in microbial genomic data. While multiple approaches have been developed and applied for the detection of prophages (viral genomes integrated in a microbial genome), new types of microbial genomic data are emerging that are more fragmented and larger scale, such as Single-cell Amplified Genomes (SAGs) of uncultivated organisms or genomic fragments assembled from metagenomic sequencing. Here, we present VirSorter, a tool designed to detect viral signal in these different types of microbial sequence data in both a reference-dependent and reference-independent manner, leveraging probabilistic models and extensive virome data to maximize detection of novel viruses. Performance testing shows that VirSorter’s prophage prediction capability compares to that of available prophage predictors for complete genomes, but is superior in predicting viral sequences outside of a host genome (i.e., from extrachromosomal prophages, lytic infections, or partially assembled prophages). Furthermore, VirSorter outperforms existing tools for fragmented genomic and metagenomic datasets, and can identify viral signal in assembled sequence (contigs) as short as 3kb, while providing near-perfect identification (>95% Recall and 100% Precision) on contigs of at least 10kb. Because VirSorter scales to large datasets, it can also be used in “reverse” to more confidently identify viral sequence in viral metagenomes by sorting away cellular DNA whether derived from gene transfer agents, generalized transduction or contamination. Finally, VirSorter is made available through the iPlant Cyberinfrastructure that provides a web-based user interface interconnected with the required computing resources. VirSorter thus complements existing prophage prediction softwares to better leverage fragmented, SAG and metagenomic datasets in a way that will scale to modern sequencing. Given these features, VirSorter should enable the discovery of new viruses in microbial datasets, and further our understanding of uncultivated viral communities across diverse ecosystems.
VirSorter sequence pre-processing VirSorter was inspired by previous algorithms and tools developed to detect prophages (viral sequences integrated in cellular genomes), especially Prophinder . For each genome and/or contigs for draft genomes provided as raw nucleotide sequences, the initial stages of VirSorter include a detection of circular sequences , gene prediction on each sequence with MetageneAnnotator , and selection of all sequences with more than 2 genes predicted. VirSorter also removes all poor-quality predicted protein sequences likely originating from gene prediction across low-complexity or poorly defined genome regions and yielding false-positive matches when compared to protein domain databases. Predicted protein sequences are then compared to PFAM and to the viral database selected by the user with hmmsearch and blastp and each gene is affiliated to its most significant hit based on alignment score. Thresholds for significant hits are as follows: minimum score of 40 and maximum E-value of 10−05 for hmmsearch, and minimum score of 50 and maximum E-value of 10−03 for blastp. VirSorter metrics computation Following the sequence pre-processing, viral regions are detected through computation of multiple metrics using sliding windows. The metrics used are presence of viral hallmark genes ,  enrichment in viral-like genes genes with best hit against the viral reference database, either RefSeqABVir or Viromes,  depletion in PFAM affiliated genes,  enrichment in uncharacterized genes  predicted genes with no hits either in PFAM or the viral reference database,  enrichment in short genes genes with a size within the 10% shorter genes of the genome, and depletion in strand switching change of coding strand between two consecutive genes.";VirusIdentification
VirusDetect;"Accurate detection of viruses in plants and animals is critical for agriculture production and human health. Deep sequencing and assembly of virus-derived small interfering RNAs has proven to be a highly eﬃcient approach for virus discovery. Here we present VirusDetect, a bioinformatics pipeline that can eﬃciently analyze largescale small RNA  datasets for both known and novel virus identiﬁcation. VirusDetect performs both reference-guided assemblies through aligning sRNA sequences to a curated virus reference database and de novo assemblies of sRNA sequences with automated parameter optimization and the option of host sRNA subtraction. The assembled contigs are compared to a curated and classiﬁed reference virus database for known and novel virus identiﬁcation, and evaluated for their sRNA size proﬁles to identify novel viruses. Extensive evaluations using plant and insect sRNA datasets suggest that VirusDetect is highly sensitive and eﬃcient in identifying known and novel viruses.
The VirusDetect package is implemented in Perl. BWA is employed to align siRNA reads to the reference virus sequences, host sequences or assembled virus contigs. For reference guided assembly, SAMtools is used to process BWA alignments and generate per-position alignment information in pileup format, which is used to guide the construction of virus contigs. De novo assembly of viral siRNAs is performed using Velvet. VirusDetect uses the BLAST program  to compare assembled contigs against virus reference nucleotide and protein sequence databases. Perl modular Bio::Graphics is used to generate the track images according to the BLAST result of virus contigs. The efficiency of VirusDetect in identifying novel viruses based on siRNA profiles was evaluated using an sRNA dataset from the D. melanogaster ovary somatic sheet (OSS) cell line. VirusDetect identified a novel potyvirus, Brazilian weed virus , from a weed sRNA dataset.  Size distribution of total sRNAs and siRNAs derived from BWVY.  siRNA distribution across the genome of BWVY in both positive and negative  strands.  Alignments of virus contigs assembled from different depths of sRNAs to the genome of BWVY identified in the weed sample. First, using the homology-based approaches, VirusDetect identified all six known viruses that were reported. VirusDetect was able to detect additional viruses in the cell line,";VirusIdentification
VIP;"Identification and discovery of viruses using next-generation sequencing technology is a fast-developing area with potential wide application in clinical diagnostics, public health monitoring and novel virus discovery. However, tremendous sequence data from NGS study has posed great challenge both in accuracy and velocity for application of NGS study. Here we describe VIP (“Virus Identification Pipeline”), a one-touch computational pipeline for virus identification and discovery from metagenomic NGS data. VIP performs the following steps to achieve its goal: (i) map and filter out background-related reads, (ii) extensive classification of reads on the basis of nucleotide and remote amino acid homology, (iii) multiple k-mer based de novo assembly and phylogenetic analysis to provide evolutionary insight. We validated the feasibility and veracity of this pipeline with sequencing results of various types of clinical samples and public datasets. VIP has also contributed to timely virus diagnosis (~10 min) in acutely ill patients, demonstrating its potential in the performance of unbiased NGS-based clinical studies with demand of short turnaround time.
VIP has a set of fixed external software and database dependencies and user-defined custom parameters. The pipeline accepts cross-platform results generated from 454, ion torrent and Illumina with a variety of formats such as FastQ, FastA, SAM and BAM alignment formats. Reads are handled by concatenating the files into a single file for streamlined analysis. Data import and quality control Raw NGS short read data can be imported by trans-formatted into FastQ format using PICARD . VIP will determine the encoding version of the input data. This is necessary to make sure those differences in the way the quality scores were generated from different sequencing platforms are properly taken into consideration during preprocessing. VIP can also accept FastA format raw NGS data. The quality control step, however, will only perform sequence-based strategies, such as the complexity and length as main factors. Generally, the quality control step consists of trimming low-quality and adapter reads, removing low-complexity sequences using the DUST algorithm and retaining reads of trimmed length  using PRINSEQ. In fast mode, Bowtie2 alignments are first performed against the host DB followed by removing the host-related sequences. The remaining reads are subject to ViPR/IRD nucleotide DB. In sense mode, the initial alignment against host DB and bacteria DB is followed by subtraction of reads mapped. Background-subtracted reads are then subject to a virus subset of NCBI nt DB. Extensive classification and coverage map Previous reports suggested that viruses have the potential to mutate rapidly or jump between species. Reads from the mutation region of these viruses might be unclassified or classified into other species or strains. In order to avoid the misclassifications caused by mutations, we applied a two-steps computational alignment strategy for classification in VIP . In the first step, all matched reads will be assigned to a specific gene identifier after nucleotide amino acid alignment. These reads are therefore taxonomically classified to genus level by lookup of matched GI from the NCBI taxonomy database by SQL .  According to the GI, the scientific names for each reference record, which are composed of the species, genus and family information, are achieved and appended to the alignment results. Secondly, reads classified under genus are automatically mapped to the most likely reference genome as follows. Abundance of reference sequences that are selected during nucleotide alignment corresponding to that genus are calculated and sorted. In other words the higher abundance of a genome suggested the higher possibility to recover its genome. All the reference sequences with the following key words are kept:  complete genomes; complete sequences; or complete cds. Assigned reads are directly mapped to all nucleotide reference sequences selected using optimal BLASTn. The optimal score strategy is most suitable for sequences with low conserved ratio. For each genus, coverage map(s) for the reference sequence(s) were then generated. Phylogenetic analysis and multiple k-mer based de novo assembly The construction of a phylogenetic tree allows us to visualize the underlying genealogy between the contiguous sequences and reference sequences. In order to perform a phylogenetic analysis of candidate viruses in a certain viral genus, a backbone with high quality and wide spectrum is indispensable. For a genus, sequences with Refseq standard sunder that genus and the reference sequence which is used to generate the coverage map are selected to carry out multiple sequence alignment to build a backbone using MAFFT. The de novo assembly step benefits from the classification method in VIP for significant reduction of complexity of reads. Still de novo assemblies from virus samples, especially RNA viruses, into a genome sequence is challenging due to extremely uneven read depth distribution caused by amplification bias in the inevitable reverse transcription and PCR amplification process during library preparations. We present the Multiple-k method in which various k-mer lengths are used for de novo assembly with Velvet Oases. In case that sparse reads do not overlap sufficiently to permit de novo assembly into longer contiguous sequences, assigned reads and contigs are retained if they are the most appropriate empirical longer than the average length of the candidate reads. Finally the largest contig after de novo assembly is added into the backbone to generate phylogenetic tree by unweighted pair-group method with arithmetic means  and visualized by Environment for Tree Exploration.";VirusIdentification
VSDToolkit;Detection and preventing entry of exotic viruses and viroids at the border is critical for protecting plant industries trade worldwide. Existing post entry quarantine screening protocols rely on time-consuming biological indicators and/or molecular assays that require knowledge of infecting viral pathogens. Plants have developed the ability to recognise and respond to viral infections through Dicer-like enzymes that cleave viral sequences into specific small RNA products. Many studies reported the use of a broad range of small RNAs encompassing the product sizes of several Dicer enzymes involved in distinct biological pathways. Here we optimise the assembly of viral sequences by using specific small RNA subsets. We sequenced the small RNA fractions of plants held at quarantine glasshouse facilities in Australia and New Zealand. Benchmarking of several de novo assembler tools yielded SPAdes using a kmer of 19 to produce the best assembly outcomes. We also found that de novo assembly using  small RNAs can result in chimeric assemblies of viral sequences and plant host sequences. Such non-specific assemblies can be resolved by using t small RNAs subsets. Among the selected samples, we identified contigs with sequence similarity to viruses and viroids in samples. Most of the viruses were assembled using virus-derived siRNAs , except for one Citrus endogenous pararetrovirus that was more efficiently assembled using long viRNAs. All three viroids found in this study were fully assembled using viRNAs. Optimised analysis workflows were customised within the Yabi web-based analytical environment. We present a fully automated viral surveillance and diagnosis web-based bioinformatics toolkit that provides a flexible, user-friendly, robust and scalable interface for the discovery and diagnosis of viral pathogens. We have implemented an automated viral surveillance and diagnosis  bioinformatics toolkit that produces improved viruses and viroid sequence assemblies. The VSD toolkit provides several optimised and reusable workflows applicable to distinct viral pathogens. We envisage that this resource will facilitate the surveillance and diagnosis of viral pathogens in plants, insects and invertebrates. The VSD toolkit has three versions of the virus and viroid detection workflows , with users able to choose from three subsets of small RNA read lengths . Existing automated workflows can be reused or modified and saved . Additional workflows such as the detecting novel viroids and mapping reads onto a reference genome are also available, and can be run as a separate job, or added to the virus and viroid detection workflows  Virus and viroid detection workflow Files of small RNA reads in fastq format are first uploaded through the ‘Files’ tab in Yabi. users then navigate to the ‘Design’ tab, where they choose the saved workflow of interest . Users are also able to build their own workflows, by simply dragging and dropping tools into the workflow area. The first stage of the workflow is the ‘Select file’ tool. The fastq file of interest is then selected. If an adapter trimming step needs to be performed, users can add in the ‘fastx_clipper’ tool to the workflow, and perform quality control checks using the ‘fastQC’ tool . Reads then undergo quality trimming through the content dependent read trimming tool ConDeTri , which trims and removes reads with low quality scores .;VirusIdentification
VirusHunter;"Quick and accurate identification of microbial pathogens is essential for both diagnosis and response to emerging infectious diseases. The advent of next-generation sequencing technology offers an unprecedented platform for rapid sequencing-based identification of novel viruses. We have developed a customized bioinformatics data analysis pipeline, VirusHunter, for the analysis of Roche/454 and other long read Next generation sequencing platform data. To illustrate the utility of VirusHunter, we performed Roche/454 GS FLX titanium sequencing on two unclassified virus isolates from the World Reference Center for Emerging Viruses and Arboviruses (WRCEVA). VirusHunter identified sequences derived from a novel bunyavirus and a novel reovirus in the two samples respectively. Further sequence analysis demonstrated that the viruses were novel members of the Phlebovirus and Orbivirus genera. Both Phlebovirus and Orbivirus genera include many economic important viruses or serious human pathogens.
the workflow of the pipeline . Remove redundant sequences. Identical or nearly-identical sequences are frequently present in NGS data, either due to the sheer depth of NGS or because many of the pre-sequencing sample preparation methods involve PCR amplification.. Mask repetitive sequences and sequence quality control. Many eukaryotic genomes contain stretches of highly repetitive DNA sequences which cause problems in BLAST-based similarity searches and result in high rates of false-positive alignments. RepeatMasker is used to mask interspersed repeats and low complexity DNA sequences. A sequence fails the quality control criteria if it does not contain a stretch of at least consecutive Filtered sequence. Filtering host sequences. Sequences are subjected to BLASTn  alignment against the appropriate host genome . BLASTn output files are parsed using BioPerl. Any sequence that shares significant similarity is classified as ""Host"" and removed from further analysis. Any desired sequence database can be used for filtering. BLASTn against NCBI nt database. Sequences retained from the previous step are queried against the NCBI nt database using BLASTn. Sequences with significant hits are broadly classified as human, mouse, fungal, bacterial, phage, viral or other based on the taxonomy identity of the best BLAST hit. The NCBI gi – taxid data for nucleotide sequences are uploaded to a MySQL database. The gi number of the best BLAST hit is used to query the database to obtain the taxonomy ID which is in turn used to retrieve the full taxonomy lineage using BioPerl. In some instances, one query has two or more hits with the same e value. If a sequence aligns to both a virus and a sequence derived from another organism type with the same e value it is classified as “ambiguous”. All eukaryotic viral sequences are further classified into viral families based on the taxonomy ID of the best hit. BLASTx against NCBI nr database. Sequences retained from the previous step are queried against the NCBI nr database using BLASTx . BLASTx output files are parsed and sequences are phylotyped as described in the previous step. Sequences without any significant hit are placed in the “unassigned” category. Report of the Findings The final output of the VirusHunter pipeline is a single file summarizing all the viruses identified in each dataset in the input directory.";VirusIdentification
MagicBLAST;"Next-generation sequencing technologies can produce tens of millions of reads, often paired-end, from transcripts or genomes. But few programs can align RNA on the genome and accurately discover introns, especially with long reads. We introduce Magic-BLAST, a new aligner based on ideas from the Magic pipeline.
Magic-BLAST uses innovative techniques that include the optimization of a spliced alignment score and selective masking during seed selection. We evaluate the performance of Magic-BLAST to accurately map short or long sequences and its ability to discover introns on real RNA-seq data sets from PacBio, Roche and Illumina runs, and on six benchmarks, and compare it to other popular aligners. Additionally, we look at alignments of human idealized RefSeq mRNA sequences perfectly matching the genome.
We show that Magic-BLAST is the best at intron discovery over a wide range of conditions and the best at mapping reads longer than 250 bases, from any platform. It is versatile and robust to high levels of mismatches or extreme base composition, and reasonably fast. It can align reads to a BLAST database or a FASTA file. It can accept a FASTQ file as input or automatically retrieve an accession from the SRA repository at the NCBI. The Magic-BLAST algorithm has a structure similar to that of other BLAST programs [10]. It reads the data in batches and builds a “lookup table”, which is an index of word locations in the reads, 16-bases by default. It then scans the database sequences, usually a reference genome, for matches in the lookup table and attempts to extend selected initial matches to the length specified by the user (18 by default). The resulting matches form a seed for computation of local gapped alignments. Collinear local alignments are combined into spliced alignments. In order to be used as a seed, the original 18 base match must be completely contained within one exon (i.e., cannot span two exons). Consequently, exons shorter than the seed length cannot be captured, but they are rare (less than 0.2% of RefSeq exons), and most will be recognized by aligning in parallel on the known transcriptome. For paired reads, the best alignments are selected based on the alignment quality of the pair. For example, if one read of a pair maps equally well at two genomic sites, and the second read maps best at a single site, the read pair will be reported as mapping uniquely at the position dictated by the second read. In this way, the specificity of the mapping truly reflects the number of bases sequenced in the whole fragment, i.e. 200 bases specificity for 100 + 100 paired-end reads. Below, we present a detailed description of the above steps. Figure 1 presents an overview of these steps.
Most genomes contain interspersed repeats and gene families that complicate correct placement of reads in a genome. To avoid seeding to ambiguous positions, Magic-BLAST scans the reference sequence and counts 16-base words. Those words that appear in the reference database more than a user-specified number of times (by default 60) are not indexed in the lookup table, so that they never form a seed alignment. To make this procedure more efficient, only words present in the reads are counted. The cut-off number 60 was selected experimentally as the best trade-off between sensitivity and runtime for RNA-seq. Additionally, Magic-BLAST specifically masks out 16-base words that contain at least 15 A’s or 15 T’s, effectively avoiding seeding on poly-A tails. This approach is similar to soft masking in other BLAST programs.
Magic-BLAST computes a local alignment by extending exact word matches (18-bases by default) between a read and a reference sequence. We use a simplified greedy alignment extension procedure, previously used in Magic [7]. Starting with the seed, the alignment is extended until the first mismatch. Next, we attempt to characterize the mismatch as a substitution, insertion or deletion of one to three bases by recursively testing the quality of the alignment of the following few bases. This is done by applying successively a table of candidate alignment operations (Table 1) until the associated requirement is met. A requirement is that a specific number of bases must match within a given number of bases following the applied operation. The first operation whose requirement is met is applied to the alignment and the algorithm proceeds to the next position on both sequences. A single substitution is reported if no requirement is satisfied.  We use the X-drop algorithm [8] to stop the extension. At each position, we record the running alignment score. The algorithm stops at the end of a sequence or when the current score is smaller than the best score found so far by more than the most penalized gapped alignment operation (three-base gap in Table 1). The algorithm then backtracks to the position with the best score.

Because most reads align to a reference with few or no mismatches, this method is faster and more memory efficient than the dynamic programming-based extension procedure used in other BLAST programs. Moreover, this approach facilitates collection of traceback information at little additional cost. This method can be tuned to a given sequencing technology for an expected rate of mismatches or gaps simply by adapting Table 1. For example, in Roche 454 or PacBio, where insertions and deletions are more frequent than substitutions, one could switch to a modified table.

We compute an alignment score using the following system: 1 for each matching pair of bases, − 4 for a base substitution, zero for gap opening (either a read or reference sequence), and − 4 for each base of gap extension (insertion or deletion). A user can modify the mismatch and gap penalties. The quality coefficients present in the FASTQ file have no impact on the alignment score and are not exported in the SAM output.

About half the time, a matching base can be placed on either side of a gap, so the gap can slide at equal score. To avoid difficulties in SNP detection, Magic-BLAST by convention shifts the sliding bases upstream of the gap, in the orientation of the target.

Spliced alignments are found by combining collinear local alignments on a read and a reference sequence. Magic-BLAST constructs a chain of local alignments that maximizes the combined alignment score. It then updates the alignment extents so that the spliced alignment is continuous on the read and the intron donor and acceptor sites are, whenever possible, consistent with the canonical splice signals.

If two local alignments are continuous on a read (possibly with an overlap), then we first search for the canonical splice site (GT-AG or CT-AC) where the alignments meet. If this site is not found and each alignment has a score of at least 50, we search successively for the minor splice sites or their complement: GC-AG or CT-GC, AT-AC or GT-AT, then for any other non-canonical site. The first site found is accepted. The alignment score threshold of 50 was chosen because minor and non-canonical splice sites are rare, but pairs of di-nucleotides are frequently found in the genome. As a result, for reads shorter than 100 bases, Magic-BLAST conservatively only calls GT-AG introns.

Magic-BLAST also attempts to produce spliced alignments if a read has several local alignments separated by one to ten unaligned bases. First, we look for a splice signal within four bases of the end of the left alignment and, if found, we fix the beginning of the candidate intron. Second, we search for the corresponding end of intron signal at offsets that ensure a continuous alignment on the read, allowing at most one additional insertion or deletion. If this fails, the procedure is repeated with the end of the intron fixed and a search for the signal indicating the start of the intron. When the candidate splice signals are found, the alignments are trimmed or extended to the splice signals. The benefit of this method is that it correctly identifies introns even in the presence of a substitution, insertion, or deletion close to the intron boundaries. Because this procedure is very sensitive and can produce many spurious alignments, Magic-BLAST only allows the GT-AG signal in this situation.

The spliced alignment is scored with the same scoring system as the local alignment. There is no reward or penalty for splice sites and no preference is given to continuous versus spliced alignments. When mapping RNA to the genome, Magic-BLAST does not support the use of an annotation file or a two-pass method. If desired, one can map both on the genome and on an annotated transcriptome, then use the universal scoring system of Magic-BLAST to select the best alignment, be it genomic or transcriptomic, for each fragment. In this paper, we mapped only to the genome.

Magic-BLAST returns results in the Sequence Alignment/Map SAM/BAM format [19] or in a tab-delimited format similar to the tabular format in other BLAST programs, which is less standard but richer and easier to mine.";Mapping
MGmapper;"An increasing amount of species and gene identification studies rely on the use of next generation sequence analysis of either single isolate or metagenomics samples. Several methods are available to perform taxonomic annotations and a previous metagenomics benchmark study has shown that a vast number of false positive species annotations are a problem unless thresholds or post-processing are applied to differentiate between correct and false annotations. MGmapper is a package to process raw next generation sequence data and perform reference based sequence assignment, followed by a post-processing analysis to produce reliable taxonomy annotation at species and strain level resolution. An in-vitro bacterial mock community sample comprised of 8 genuses, 11 species and 12 strains was previously used to benchmark metagenomics classification methods. After applying a post-processing filter, we obtained 100% correct taxonomy assignments at species and genus level. A sensitivity and precision at 75% was obtained for strain level annotations. A comparison between MGmapper and Kraken at species level, shows MGmapper assigns taxonomy at species level using 84.8% of the sequence reads, compared to 70.5% for Kraken and both methods identified all species with no false positives. Extensive read count statistics are provided in plain text and excel sheets for both rejected and accepted taxonomy annotations. The MGmapper package consists of a pipeline of scripts to process FASTQ files as either single or paired-end reads to perform sequence mapping and taxonomy annotation against user defined reference sequence databases. MGmapper utilizes a number of publicly available programs: Cutadapt [17] for trimming and adaptor removal, BWA-mem [15] and SAMtools [18] to produce and process the reference based sequence alignments to one of many reference sequence databases. A short summary of the procedure is described below for paired-end sequence data, followed by more details outlined in the section “Fastq mapping procedure”.

Initially, a filtering step checks for properly paired reads, followed by trimming and adaptor removal. The biological relevant reads are obtained by always mapping to a PhiX bacteria phage and continuing with the subset of reads that do not align to the PhiX genome (commonly used as a control in Illumina sequencing runs). Next, sequence reads are mapped to user defined reference sequences and only properly paired reads are accepted, provided that both reads pass a lower alignment score threshold and relative alignment length. After mapping reads to all reference sequence databases (eg human, bacteria, fungi etc.), some reads may align to reference sequences in different databases and depending on the mapping mode (bestmode or fullmode explained further down) the best hit is identified and used to assign taxonomy. Taxonomy annotations (ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz) are added via lookup in a pre-made Kyoto Cabinet database (http://fallabs.com/kyotocabinet/) containing key, value pairs in form of the reference sequence name (the key) and full taxonomy path from strain to superfamily clades (the value). Finally, a post-processing step (section “post-processing”) identifies confident assignments at strain, species, genus or any user defined taxonomy clade up to superfamily.

MGmapper can map sequence reads against any nucleotide sequence database i.e. both genomic and gene sequence databases and for each database the mapping can be performed in either bestmode or fullmode. In bestmode, reads are assigned to only one reference sequence if it is the best hit that is observed when mapping to all databases specified for bestmode mapping. Best hit is identified based on the highest alignment score. In fullmode reads are assigned to a reference sequence even if a better hit is seen when mapping to another database. Typically the fullmode is used to search for sequences (e.g. a gene database), that may be a subset of another database (e.g. a full genome database). Analyzing a sample for both genomic bacterial composition and anti-microbial resistance genes is a situation where MGmapper should be run with the bacterial database specified for bestmode mapping and at the same time specifying the anti-microbial resistance gene database for fullmode mapping. The reason is that the resistance genes are or may be a subset of the bacterial genomic sequences and we want to assign a sequence read both a bacteria genome and also a resistance gene. If both databases were specified for bestmode mapping (bacteria, anti-microbial genes), then a read can only be assigned to one of the databases and if identical alignment scores are observed, then priority is to the database that was specified first. The MGmapper pipeline analysis is done in four main steps: I. Pre-processing of raw reads to remove potential positive control reads, II. Mapping of reads to one or more reference sequence databases and filtration of alignment hits, III. Identification the best hits, and IV. Post-processing of taxonomy annotations and preparation of excel and text files with insert-size distribution, size normalized abundances, read and nucleotide count statistics, depth, and coverage. 
An optional trimming and filtering of raw reads is performed by use of the Cutadapt [17] program. Users can skip this step if reads are already trimmed. Default setting is that reads are initially trimmed before searching for adaptor sequences (equivalent to the Cutadapt option—q). In addition, a read is discarded unless a minimum of 30 nucleotides remains after trimming. Trimmed reads are next paired up and singleton reads are removed when using the paired-end version of MGmapper. To this follows another cleaning process where reads from potential PhiX Control v3 adapter-ligated libraries are removed via BWA-mem [15] and SAMtools [18], as they may originate from a control for Illumina sequencing runs (http://www.illumina.com/products/phix_control_v3.html). The outcome is a cleaned set of reads that are believed to originate from the biological sample of interest. The number of reads in this set (noPhiX dataset) is set to 100% and used for calculation of R_abundance, a read count abundance measure.
FASTQ reads are first extracted from the noPhiX set and mapped to one or several reference databases via ‘bwa mem—t procs—M database’ marking shorter splits as secondary hits, which are then removed when piping to ‘samtools view -F 256 -Sb -f2’ in paired-end mode or ‘samtools view -F 260 –Sb’ in single-end mode i.e. keeping properly paired reads or mapped reads, respectively. Next, reads with insufficient alignment qualities are removed based on user-defined minimum alignment score (MAS) and minimum fraction of nucleotides being assigned with an ‘M’ state in the CIGAR format, where an ‘M’ indicates a match or mismatch. The user-defined threshold for fraction of matches+mismatches (FMM) is in relation to the full length of a read. In paired-end mapping both reads are removed if just one of them does not fulfill the filtering criteria. Default settings in the MGmapper programs are MAS = 30 and FMM = 0.8. At this step properly paired read may align to more than one reference sequences, located in different reference sequence databases. In bestmode a read pair can only be assigned to one reference sequence (section “Identification of the best hit”).
Having paired-end sequences, both the forward and the reverse fastq reads are aligned to a reference sequence, each with an associated alignment score. The sum of alignment scores (SAS) is used as a measure to identify the best hit for a read-pair. Typically, all input query reads are mapped to multiple reference sequence databases e.g. bacteria, virus, fungi, human and others. Thus a read-pair may map to multiple reference sequences from different databases and in bestmode the taxonomy annotation is only assigned to one best hit, namely the one with the highest SAS score.

For single-end reads mapped to several databases, the best hit is the one with the highest alignment score. In cases where a read or read-pair achieves identical alignment scores to reference sequences from different databases, the priority is given to the order by which the databases are specified by the user, and thus a read or read-pair can still be associated to one single reference sequence.
The fastq reads are mapped to multiple user-defined reference sequence databases. A tab-separated file is produced for each database including reference sequence hits with read count statistics provided at strain level. A strain is named according to the header name originating from the fasta file that was used to make the database. The tab-separated file is composed of 14+16 columns of read count statistics and annotations, where the latter are taxid and taxonomy clade name for 8 clades, i.e. strain, species, genus, family, order, class, order and superfamily.
The tab-separated files contains the unprocessed results as obtained by the BWA-mem [15] mapping and Samtools mpileup [18]. As false positive annotations are likely to be present, a subset of confident mapping results is obtained at a specified clade level (strain, species …superfamily) via a post-processing procedure described in the section below.

Post-processing.
A combination of four criteria (I-IV) is used to identify a positive taxonomy annotation.
At strain level all four criteria are imposed. At species level, the criteria IV is used in a pre-cycle, to identify the lowest S_Abundance for the selected species. The new S_Abundance threshold is used in a second round where criteria IV are omitted. At genus level or higher only criteria I, II and III are used.

Taxid values are used to identify strains belonging to the same species or species belonging to the same genus etc. All identifiers as shown in Table 2, are summed at clade levels higher than strain i.e. the S_Abundance value for a species is the sum of all strain S_Abundance values. It is likewise for R_Abundance, Size, Seq_Count, Nucleotides, Covered positions, Coverage Depth, ReadCount, ReadCount uniq and Mismatches.";Mapping
BWA;"the https://stackoverflow.com/questions/1994615/how-to-use-python-regex-to-match-words-beginning-with-hash-and-question-mark The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ∼10–20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. The prefix trie for string X is a tree where each edge is labeled with a symbol and the string concatenation of the edge symbols on the path from a leaf to the root gives a unique prefix of X. On the prefix trie, the string concatenation of the edge symbols from a node to the root gives a unique substring of X, called the string represented by the node. Note that the prefix trie of X is identical to the suffix trie of reverse of X and therefore suffix trie theories can also be applied to prefix trie. With the prefix trie, testing whether a query W is an exact substring of X is equivalent to finding the node that represents W, which can be done in O(|W|) time by matching each symbol in W to an edge, starting from the root. To allow mismatches, we can exhaustively traverse the trie and match W to each possible path. We will later show how to accelerate this search by using prefix information of W. Figure 1 gives an example of the prefix trie for ‘GOOGOL’. Figure 3 gives a recursive algorithm to search for the SA intervals of substrings of X that match the query string W with no more than z differences (mismatches or gaps). Essentially, this algorithm uses backward search to sample distinct substrings from the genome. This process is bounded by the D(·) array where D(i) is the lower bound of the number of differences in W[0,i]. The better the D is estimated, the smaller the search space and the more efficient the algorithm is. A naive bound is achieved by setting D(i)=0 for all i, but the resulting algorithm is clearly exponential in the number of differences and would be less efficient. CalculateD(W) z←0 j←0 for i=0 to |W|−1 do if W[j,i] is not a substring of X then z←z+1 j←i+1 D(i)←z Fig. 4. Equivalent algorithm to calculate D(i). The CalculateD procedure in Figure 3 gives a better, though not optimal, bound. It is conceptually equivalent to the one described in Figure 4, which is simpler to understand. We use the BWT of the reverse (not complemented) reference sequence to test if a substring of W is also a substring of X. Note that to do this test with BWT string B alone would make CalculateD an O(|W| 2) procedure, rather than O(|W|) as is described in Figure 3. To understand the role of D, we come back to the example of searching for W =LOL in X =GOOGOL$ (Fig. 1). If we set D(i)=0 for all i and disallow gaps (removing the two star lines in the algorithm), the call graph of InexRecur, which is a tree, effectively mimics the search route shown as the dashed line in Figure 1. However, with CalculateD, we know that D(0)=0 and D(1)=D(2)=1. We can then avoid descending into the ‘G’ and ‘O’ subtrees in the prefix trie to get a much smaller search space. The algorithm in Figure 3 guarantees to find all the intervals allowing maximum z differences. It is complete in theory, but in practice, we also made various modifications. First, we pay different penalties for mismatches, gap opens and gap extensions, which is more realistic to biological data. Second, we use a heap-like data structure to keep partial hits rather than using recursion. The heap-like structure is prioritized on the alignment score of the partial hits to make BWA always find the best intervals first. The reverse complemented read sequence is processed at the same time. Note that the recursion described in Figure 3 effectively mimics a depth-first search (DFS) on the prefix trie, while BWA implements a breadth-first search (BFS) using this heap-like data structure. Third, we adopt an iterative strategy: if the top interval is repetitive, we do not search for suboptimal intervals by default; if the top interval is unique and has z difference, we only search for hits with up to z+1 differences. This iterative strategy accelerates BWA while retaining the ability to generate mapping quality. However, this also makes BWA’s speed sensitive to the mismatch rate between the reads and the reference because finding hits with more differences is usually slower. Fourth, we allow to set a limit on the maximum allowed differences in the first few tens of base pairs on a read, which we call the seed sequence. Given 70 bp simulated reads, alignment with maximum two differences in the 32 bp seed is 2.5× faster than without seeding. The alignment error rate, which is the fraction of wrong alignments out of confident mappings in simulation (see also Section 3.2), only increases from 0.08% to 0.11%. Seeding is less effective for shorter reads. For SOLiD reads, BWAconverts the reference genome to dinucleotide ‘color’ sequence and builds the BWT index for the color genome. Reads are mapped in the color space where the reverse complement of a sequence is the same as the reverse, because the complement of a color is itself. For SOLiD pairedend mapping, a read pair is said to be in the correct orientation if either of the two scenarios is true: (i) both ends mapped to the forward strand of the genome with the R3 read having smaller coordinate; and (ii) both ends mapped to the reverse strand of the genome with the F3 read having smaller coordinate. Smith–Waterman alignment is also done in the color space. 2.7.1 Ambiguous bases Non-A/C/G/T bases on reads are simply treated as mismatches, which is implicit in the algorithm (Fig. 3). Non-A/C/G/T bases on the reference genome are converted to random nucleotides. Doing so may lead to false hits to regions full of ambiguous bases. Fortunately, the chance that this may happen is very small given relatively long reads. We tried 2 million 32 bp reads and did not see any reads mapped to poly-N regions by chance. 2.7.2 Paired-end mapping BWA supports paired-end mapping. It first finds the positions of all the good hits, sorts them according to the chromosomal coordinates and then does a linear scan through all the potential hits to pair the two ends. Calculating all the chromosomal coordinates requires to look up the suffix array frequently. This pairing process is time consuming as generating the full suffix array on the fly with the method described above is expensive. To accelerate pairing, we cache large intervals. This strategy halves the time spent on pairing. In pairing, BWA processes 256K read pairs in a batch. In each batch, BWA loads the full BWA index into memory, generates the chromosomal coordinate for each occurrence, estimates the insert size distribution from read pairs with both ends mapped with mapping quality higher than 20, and then pairs them. After that, BWA clears the BWT index from the memory, loads the 2 bit encoded reference sequence and performs Smith–Waterman alignment for unmapped reads whose mates can be reliably aligned. Smith– Waterman alignment rescues some reads with excessive differences. 2.7.3 Determining the allowed maximum number of differences Given a read of length m, BWA only tolerates a hit with at most k differences (mismatches or gaps), where k is chosen such that <4% of m-long reads with 2% uniform base error rate may contain differences more than k. With this configuration, for 15–37 bp reads, k equals 2; for 38–63 bp, k =3; for 64–92 bp, k =4; for 93–123 bp, k =5; and for 124–156 bp reads, k =6. 2.7.4 Generating mapping quality scores For each alignment, BWA calculates a mapping quality score, which is the Phred-scaled probability of the alignment being incorrect. The algorithm is similar to MAQ’s except that in BWA we assume the true hit can always be found. We made this modification because we are aware that MAQ’s formula overestimates the probability of missing the true hit, which leads to underestimated mapping quality. Simulation reveals that BWA may overestimate mapping quality due to this modification, but the deviation is relatively small. For example, BWA wrongly aligns 11 reads out of 1 569 108 simulated 70 bp reads mapped with mapping quality 60. The error rate 7×10−6 (= 11/1 569 108) for these Q60 mappings is higher than the theoretical expectation 10−6.";Mapping
SOAP2;"SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20–30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome. The new program uses the Burrows Wheeler Transformation (BWT) compressed index instead of the seed algorithm that was used in the previous version for indexing the reference sequence in the main memory. Use of BWT substantially improved alignment speed; additionally, it significantly reduced memory usage. Big eukaryotic genomes always consist of a large number of repetitive sequences (e.g. 45% of the human genome). Suffix trees and suffix arrays are considered the most appropriate methods for indexing DNA sequence, through which only one alignment is needed to for repetitive sequence with multiple identical copies in the genome. The complexity in space and time of such index construction has limited such algorithm usage in only small genomes. But the recent development of compressed indexing has reduced the space complexity from O(n) bytes to O(n) bits. Among these is the BWT (Burrow and Wheeler, 1994), a reversible data compression algorithm, which was found to be the most efficient. The space complexity of BWT is n/4 bytes, and only 1 GB memory in RAM is required for indexing the whole human genome. This algorithm has been used for efficient whole-genome comparison and indexing for Smith–Waterman local alignment to the human genome. Using this in our alignment method, we determined an exact match, by constructing a hash table to accelerate searching for the location of a read in the BWT reference index. For example, if we use a 13mer on the hash, then the reference index would be partitioned into 226 blocks, and very few search interactions are sufficient to identify the exact location inside the block. For inexact (both mismatch and indel) alignment, we applied a ‘split-read strategy’. To allow one mismatch, a read was split into two fragments. The mismatch can exist in, at most, one of the two fragments at the same time. Likewise, we split a read into three fragments to search for hits that allow two mismatches. This enumeration algorithm was used to identify mutation sites on the reads. In paired-end alignment mode, we first independently aligned the two reads belonging to a pair then searched for the pair of hits with the correct orientation relationship and proper distance. Similar to SOAP, we preferentially select the best hit of each read or read pair, which have the lowest number of mismatches or small gaps. In general practice, a user can also choose the option to report all hits that satisfy their selected preset similarity rate. For most analyses, to guarantee alignment accuracy, we recommend allowing at most two mismatches or one continuous gap in the high-quality part of a read. For the low-quality regions of a read (the 3 -end, which can contain a higher rate of sequencing errors), we provided an option that allows more mismatches within this defined 3 -end region. Since sequencing read length is getting longer and longer, the SOAP2 program is now compatible with read lengths up to 1024 bp. We evaluated the performance of this software on a dataset containing one million read pairs generated from a human Asian individual (Wang et al., 2008). Although SOAP2 was designed for improved Illumina GA sequencing with read length over 50 bp, we chose a 44 bp read length in this evaluation, which is compatible with the tools SOAP (Li,R. et al., 2008), MAQ (Li,H. et al., 2008) and the recently developed BWT-based alignment tool Bowtie (Langmead et al., 2009). SOAP2 takes 7200 s to build a BWT index for the human reference genome, which is 12 times slower than building the seed index that was implemented in SOAP. Thus, we prebuild the index on a hard disk, and then load it into RAM directly when starting a new alignment job for that genome. The memory usage was reduced from 14.7 GB in SOAP to 5.4 GB in SOAP2. SOAP2 was more than 20 times faster than SOAP and MAQ with similar amount of reads aligned (Table 1). SOAP2 and Bowtie have comparable speed on aligning single-end reads, while Bowtie cannot always find the best alignment hits and cannot align paired-end reads (Langmead et al., 2009). It should be made aware that the alignment sensitivity was determined by sequencing quality and the parameter setting of each alignment tool, so the percent of reads aligned would vary in different datasets. SOAP2 supports multiple input and output file formats. The reference sequence can be loaded as either a text or a gzipped FASTA format, and the query reads can be in either FASTA or FASTQ format. The output formats include a SOAP tab-delimited text table, a gzipped text table, a Sequence Alignment/Map (SAM) format and its binary equivalent (BAM) that is recommended by the 1000 Genomes Consortium, and a Consed format that fits with the assembly viewer. As SOAP2 is specifically designed for ultrafast alignment of short reads onto a reference sequence for large-scale resequencing projects, we have developed a companion assembler for consensus assembly of the sequenced individual based on the alignment of the reads on the reference sequence. The assembler has been included in the SOAP software package and is also freely available from the website. With this, we can detect SNPs by comparing the assembled sequence to the reference genome. The assembler uses Bayes’ theorem to infer the genotype of each base pair from the aligned reads and sequencing quality scores. The estimated SNP rate between the sequenced individual and the reference genome is used as prior probability, the raw sequencing qualities were recalibrated according to the alignment, the reads generated from potential duplicate clones were removed, and finally the genotype was called from the posterior probabilities with a Phred-like score transformed from the probability to indicate its accuracy. The tool has been used in analyzing the Asian genome data and showed over 99.9% accuracy (Wang et al., 2008).";Mapping
MAQ;"New sequencing technologies promise a new era in the use of DNA sequence. However, some of these technologies produce very short reads, typically of a few tens of base pairs, and to use these reads effectively requires new algorithms and software. In particular, there is a major issue in efficiently aligning short reads to a reference genome and handling ambiguity or lack of accuracy in this alignment. Here we introduce the concept of mapping quality, a measure of the confidence that a read actually comes from the position it is aligned to by the mapping algorithm. We describe the software MAQ that can build assemblies by mapping shotgun short reads to a reference genome, using quality scores to derive genotype calls of the consensus sequence of a diploid genome, e.g., from a human sample. MAQ makes full use of mate-pair information and estimates the error probability of each read alignment. Error probabilities are also derived for the final genotype calls, using a Bayesian statistical model that incorporates the mapping qualities, error probabilities from the raw sequence quality scores, sampling of the two haplotypes, and an empirical model for correlated errors at a site. Both read mapping and genotype calling are evaluated on simulated data and real data. MAQ is accurate, efficient, versatile, and user-friendly. To map reads efficiently, MAQ first indexes read sequences and scans the reference genome sequence to identify hits that are extended and scored. With the Eland-like (A.J. Cox, unpubl.) hashing technique, MAQ, by default, guarantees to find alignments with up to two mismatches in the first 28 bp of the reads. MAQ maps a read to a position that minimizes the sum of quality values of mismatched bases. If there are multiple equally best positions, then one of them is chosen at random. In this article, we will call a potential read alignment position a hit. The algorithm MAQ uses to find the best hit is quite similar to the one used in Eland. It builds multiple hash tables to index the reads and scans the reference sequence against the hash tables to find the hits. By default, six hash tables are used, ensuring that a sequence with two mismatches or fewer will be hit. The six hash tables correspond to six noncontiguous seed templates (Buhler 2001; Ma et al. 2002). Given 8-bp reads, for example, the six templates are 11110000, 00001111, 11000011, 00111100, 11001100, and 00110011, where nucleotides at 1 will be indexed while those at 0 are not. By default, MAQ indexes the first 28 bp of the reads, which are typically the most accurate part of the read. In alignment, MAQ loads all reads into memory and then applies the first template as follows. For each read, MAQ takes the nucleotides at the 1 positions of the template, hashes them into a 24-bit integer, and puts the integer together with the read identifier into a list. When all the reads are processed, MAQ orders the list based on the 24-bit integers, such that reads with the same hashing integer are grouped together in memory. Each integer and its corresponding region are then recorded in a hash table with the integer as the key. We call this process indexing. At the same time that MAQ indexes the reads with the first template, it also indexes the reads with the second template that is complementary to the first one. Taking two templates at a time helps the mate-pair mapping, which will be explained in the section below. After the read indexing with the two templates, the reference will be scanned base by base on both forward and reverse strands. Each 28-bp subsequence of the reference will be hashed through the two templates used in indexing and will be looked up in the two hash tables, respectively. If a hit is found to a read, MAQ will calculate the sum of qualities of mismatched bases q over the whole length of the read, extending out from the 28-bp seed without gaps (the current implementation has a read length limit of 63 bp). MAQ then hashes the coordinate of the hit and the read identifier into another 24-bit integer h and scores the hit as q224 + h. In this score, h can be considered as a pseudorandom number, which differentiates hits with identical q: If there are multiple hits with the same q, the hit with the smallest h will be identified as the best, effectively selecting randomly from the candidates. For each read, MAQ only holds in memory the position and score of its two best scored hits and the number of 0-, 1-, and 2-mismatch hits in the seed region. When the scan of the reference is complete, the next two templates are applied and the reference will be scanned once again until no more templates are left. Using six templates guarantees to find seed hits with no more than two mismatches, and it also finds 57% of hits with three mismatches. In addition, MAQ can use 20 templates to guarantee finding all seed hits with three mismatches at the cost of speed. In this configuration, 64% of seed hits with four mismatches are also found, though our experience is that these hits are not useful in practice. MAQ jointly aligns the two reads in a read pair and fully utilizes the mate-pair information in the alignment. In the paired-end alignment mode, MAQ will by default build six hash tables for each end (12 tables in total). In one round of indexing, MAQ indexes the first end with two templates and the second end also with two templates. Four hash tables, two for each end, will be put in memory at a time. In the scan of the reference, when a hit of a read is found on the forward strand of the reference sequence, MAQ appends its position to a queue that always keeps the last two hits of this read on the forward strand. When a hit of a read is found on the reverse strand, MAQ checks the queue of its mate and tests whether its mate has a hit on the forward strand within a maximum allowed distance ahead of the current read. If there is one, MAQ will mark the two ends as a pair. In this way, MAQ jointly maps the reads without independently storing all the potential hits of each end. For each end, MAQ will only hold in memory two hash tables corresponding to two complementary templates (e.g., 11110000 and 00001111 for 8-bp reads). This strategy guarantees that any hit with no more than one mismatch can be always found in each round of the scan. Holding more hash tables in memory would help to find pairs containing more mismatches, but doing this would also increase memory footprint. Paired-end mapping qualities are derived from single end mapping qualities. There are two different cases when a pair can be wrongly mapped. In the first case, one of the two ends is wrongly aligned and the other is correct. This scenario may happen if a repetitive sequence appear twice or more in a short region. In the second instance, a pair is wrong because both ends are wrong at the same time. In MAQ, if there is a unique pair mapping in which both ends hit consistently (i.e., in the right orientation within the proper distance), we give the mapping quality Qp = Qs1 +Qs2 to both reads, assuming independent errors. If there are multiple consistent hit pairs, we take their single end mapping qualities as the final mapping qualities. Detecting short indels MAQ first aligns reads with the ungapped alignment algorithm described above and then finds short indels by utilizing matepair information. Given a pair of reads, if one end can be mapped with confidence but the other end is unmapped, a possible scenario is that a potential indel interrupts the alignment of the unmapped read. For this unmapped read, we can apply a standard Smith-Waterman gapped alignment (Smith and Waterman 1981) in a region determined by the aligned read. The coordinate and the size of the region is estimated from the distribution of all the aligned reads by taking the mean separation of read pairs plus or minus twice the standard deviation. As Smith-Waterman will only be applied to a small fraction of reads in short regions, efficiency is not a serious issue. MAQ also generates in silico mutated diploid sequences by adding random mutations to the known reference sequence. The human reference genome does not contain heterozygotes, but when we resequence a human sample and map reads to the reference genome, we will see both homozygous and heterozygous variants in comparison to the reference.  Consequently, on the condition that a site is different from the reference, the probability of a heterozygote is always 2/3, regardless of the allele frequency f, assuming the sample comes from the same population as the reference. Based on this observation, we can simulate a diploid genome as follows. We first used the reference genome as the two preprocessed haplotypes. We then generated a set of polymorphic sites, randomly selected two thirds of them as heterozygotes, and took the rest as homozygotes. At a heterozygous site, we randomly selected one haplotype and mutated the base into another one; on a homozygous site, we mutated both haplotypes. Both substitutions and indels can be simulated in this way. This simulation ignores linkage disequilibrium between variants. Although coalescent-based simulation (Hudson 2002) gives a more accurate long-range picture, the procedure described here is sufficient for the evaluation of the variant calling method for a single individual. From a known sequence, paired-end reads can be simulated with insert sizes drawn from a normal distribution and with base qualities drawn from the empirical distribution estimated from real sequence data. Sequencing errors are introduced based on the base quality. With sufficiently large data, we are able to estimate the position-specific distributions of base qualities and the correlation between adjacent qualities as well. An order-one Markov chain is constructed, based on these statistics, to capture the fact that low-quality bases tend to appear at the 3-end of a read and to appear successively along a read. Alignment for Applied Biosystems SOLiD reads SOLiD reads are presented in the color space, which comprises four colors with each color representing four types of combinations of two adjacent nucleotides. The SOLiD sequencing machine gives the last primer nucleotide base and the color read sequence. This information makes it possible to write down the nucleotide read sequence based on the meaning of colors. However, a single color error will completely change the nucleotide sequencing following that error. Mapping reads in the color space is preferable to mapping in the nucleotide space. To map reads in the color space, we need to convert the reference sequences into color sequences and to perform the alignment in the color space. Between the color alignment and nucleotide alignment, the main difference is that the complement of a color is identical to itself, and therefore in the color space, reads coming from the reverse strand of the reference only need to be reversed without complementation. Most alignment programs can be adapted to perform such an alignment with little effort. Another difference is for paired-end reads. In SOLiD sequencing, the two ends of a read pair should always come from the same strand, instead from two different strands like Illumina sequencing. MAQ is able to map SOLiD mate-pair reads to the reference, but it has to trim off the primer nucleotide base and the following color because currently MAQ cannot work with color sequences and nucleotide sequences at the same time. Trimming the first color is equivalent to using reads 1 bp shorter, which should not greatly affect the alignment results.";Mapping
RMAP;"Second-generation sequencing has the potential to revolutionize genomics and impact all areas of biomedical science. New technologies will make re-sequencing widely available for such applications as identifying genome variations or interrogating the oligonucleotide content of a large sample (e.g. ChIP-sequencing). The increase in speed, sensitivity and availability of sequencing technology brings demand for advances in computational technology to perform associated analysis tasks. The Solexa/Illumina 1G sequencer can produce tens of millions of reads, ranging in length from ~25–50 nt, in a single experiment. Accurately mapping the reads back to a reference genome is a critical task in almost all applications. Two sources of information that are often ignored when mapping reads from the Solexa technology are the 3' ends of longer reads, which contain a much higher frequency of sequencing errors, and the base-call quality scores. To investigate whether these sources of information can be used to improve accuracy when mapping reads, we developed the RMAP tool, which can map reads having a wide range of lengths and allows base-call quality scores to determine which positions in each read are more important when mapping. We applied RMAP to analyze data re-sequenced from two human BAC regions for varying read lengths, and varying criteria for use of quality scores. Our results indicate that significant gains in Solexa read mapping performance can be achieved by considering the information in 3' ends of longer reads, and appropriately using the basecall quality scores. The RMAP tool we have developed will enable researchers to effectively exploit this information in targeted re-sequencing projects. We treat the mapping problem as approximately matching a set of patterns in a text – the set of patterns being the reads, and the text being the genome. This problem has been well studied, and several general algorithmic strategies have emerged for solving it (see [13] for a detailed treatment). The major motivation for developing the RMAP algorithm was to incorporate base-call quality scores to weight mismatches and improve mapping accuracy. In addition to having high mapping accuracy, RMAP was designed under the restrictions that it must be capable of (1) mapping reads with length exceeding 50 bases (for the applications discussed in the introduction), (2) allowing the number of mismatches to be controlled (not being restricted to a small fixed number), and (3) completing mapping tasks under reasonable time constraints on widely available computing hardware. The algorithm implemented in RMAP uses the filtration method described by [14]. For reads of length n, and mapping with up to k mismatches, each read is partitioned into k + 1 contiguous seeds (each seed is a substring of the read, and has length &#x230A;n/(k + 1)&#x230B;). Because there can only be k mismatches in a mapping, and there are k + 1 seeds for each read, any mapping must have at least one seed with no mismatches. The algorithm first identifies locations in the genome where the seeds match exactly. Exact matching can be done much more quickly than approximate matching, and evaluating the approximate match between a read and a genomic region only needs to be done for those regions surrounding an exactly-matching seed. To efficiently implement the filtration strategy, RMAP preprocesses the set of reads, building a hash-table (which we refer to as the seed-table) indexed by the seeds. The table entry for a particular seed lists all reads containing that seed, along with the offset of that seed within the read. For a set of r reads, each having length n, if k mismatches are allowed in the search, the seed table has size O(4n/k + rk). The mapping proceeds by scanning the genome, with a window of size equal to the seed size. Each segment of the genome is tested as a seed by hashing that segment to determine the set of reads that must be compared in their entirety with a larger genomic region surrounding the segment of the genome currently being scanned. This is a common strategy to implement the filtration stage of approximate matching. The influence of the size of the genome in the time complexity of RMAP is therefore linear, and importantly the space complexity of RMAP is independent of the size of the genome. The step of comparing the full read to portions of the genome where a seed has been found is implemented to require time that is logarithmic in the length of the reads. The comparison takes advantage of bit-wise operations, and the reads are encoded in a binary format (see additional file 4 for supplementary method). A series of logical operations produce a vector indicating the locations of mismatches between the read and the genomic segment being compared, and the weight of the bit-vector indicating mismatches computed using a well-known technique. RMAP is sufficiently fast that several million reads can be mapped to a mammalian genome in one day on a computer with a single processor. No portion of the reference genome is maintained by RMAP, and the size of the seed table dominates the space requirements. Because these requirements are sufficiently small, RMAP can be run on widely available hardware. This includes the nodes typically used in cluster computers, and allows the processing to be easily and effectively parallelized by simply partitioning the set of reads. On a test data set generated by randomly sampling one million 50 nt segments (simulated reads) from the hg18 genome, and randomly changing up to 4 bases in each read, our current implementation of RMAP was able to map the reads back to the hg18 genome in 140 minutes using roughly 620 MB of memory.";Mapping
GSNAP;"Next-generation sequencing captures sequence differences in reads relative to a reference genome or transcriptome, including splicing events and complex variants involving multiple mismatches and long indels. We present computational methods for fast detection of complex variants and splicing in short reads, based on a successively constrained search process of merging and filtering position lists from a genomic index. Our methods are implemented in GSNAP (Genomic Short-read Nucleotide Alignment Program), which can align both single- and paired-end reads as short as 14 nt and of arbitrarily long length. It can detect shortand long-distance splicing, including interchromosomal splicing, in individual reads, using probabilistic models or a database of known splice sites. Our program also permits SNP-tolerant alignment to a reference space of all possible combinations of major and minor alleles, and can align reads from bisulfite-treated DNA for the study of methylation state. In comparison testing, GSNAP has speeds comparable to existing programs, especially in reads of ≥70 nt and is fastest in detecting complex variants with four or more mismatches or insertions of 1–9 nt and deletions of 1–30 nt. Although SNP tolerance does not increase alignment yield substantially, it affects alignment results in 7–8% of transcriptional reads, typically by revealing alternate genomic mappings for a read. Simulations of bisulfiteconverted DNA show a decrease in identifying genomic positions uniquely in 6% of 36 nt reads and 3% of 70 nt reads. We view alignment as a search problem over a space of genomic regions in the reference sequence, or combinations of regions if gaps are allowed. (Although a reference sequence may consist of chromosomes, contigs, transcripts or artificial segments, we simplify our discourse by referring to it as a ‘genome’.) Searching involves the steps of generating, filtering and verifying candidate genomic regions, and its efficiency depends on designing the generation and filtering steps to produce as few candidates as possible. Several alignment programs, including MAQ (Li et al., 2008a), RMAP (Smith et al., 2008), SeqMap (Jiang and Wong, 2008) and RazerS (Weese et al., 2009), preprocess the reads and then generate and filter candidate genomic regions by scanning a read index against the genome. For large genomes, it is more efficient to pre-process the genome rather than the reads to create genomic index files, which provide genomic positions for a given oligomer. Genomic indexing also permits parts of reads to be aligned to arbitrary genomic regions, needed for long-distance splice detection. Indexing need be done only once for each reference sequence, with the resulting index files usable by each new dataset. Oligomers of all lengths can be indexed using a suffix array or its compressed BWT equivalent, as used in Bowtie, BWA and SOAP2, which can represent a reference sequence compactly, in 2 GB for a human-sized genome of 3 billion nt. However, when only a single oligomer length q is needed by an algorithm, a simple hash table (Ning et al., 2001) or q-gram index (Rasmussen et al., 2006) applied to the genome will suffice (Fig. 2A). This data structure consists of an offset file (or lookup table) of all possible q-mers, with pointers to a position file (or occurrence table) containing a list of genomic positions for each q-mer. For our search algorithm to work most efficiently, it is important that each position list in the position file be pre-sorted, which allows intersections to be computed quickly among multiple q-mer lookups. The intersection process also requires the positions in each position list to be adjusted at run time for its location in the given read, so they correspond to the diagonals in an alignment matrix between genome and read. Although our alignment algorithm could potentially work with another data structure that provides genomic positions for a given q-mer, a suffix array would require the additional step of sorting each position list at run time. A set of n sorted lists can be merged in time O(l logn), where l is the sum of list lengths, by using a heap-based multiway merging procedure (Knuth, 1973). A merging procedure can produce not only a list of candidate genomic regions, but also information about the count and read location of the position lists that support each region. This support information can provide evidence about the number of mismatches in the read, and can therefore be used to filter out candidate regions. To use multiway merging effectively, our algorithm depends on another idea, that of successive score constraints. For a given read, our program is designed to report the ‘best’ alignment or alignments, those with the lowest score based on mismatches plus an opening gap penalty for an indel or a splice. Therefore, our search process is constrained successively by an increasing score level K, starting from K =0 for an exact match, and ending either with a successful alignment at some K or at a maximum score level specified by the user. In addition to finding the best alignment, a constrained search process can also find suboptimal alignments, by continuing the search at successive score levels beyond the first, or optimal, one that yields an alignment. Our algorithm could also find an exhaustive set of alignments up to a given score level by searching at that score level and reporting all results. Depending on the score constraint K and the read length L, a multiway merging process can be formulated in two different ways to generate and filter genomic regions. For low values of K involving none or a few mismatches relative to L, we apply a merging procedure based on a spanning set of oligomers, which filters genomic regions based on the count of q-mers that support the region. For higher levels of K involving more mismatches, we apply a merging procedure based on a complete set of oligomers, which filters genomic regions based on the pattern of q-mers that support the region. Both the count- and pattern-based criteria provide lower bounds on the number of mismatches present in a read or part of a read. If a lower bound exceeds the given score constraint K of allowed mismatches, the read may be filtered out and need not be verified against the genome to determine the actual number of mismatches. A hash table is relatively large, requiring 12 GB to represent a human-sized genome if every overlapping oligomer is indexed. Accordingly, SOAP (Li et al., 2008b) requires 14 GB of memory to process a human-sized genome. Although modern computers generally have sufficient physical memory to query such large hash tables, smaller data structures can speed up programs by using memory paging and caching resources more effectively. We can reduce the size of a hash table by sampling the genomic oligomers that are indexed in the table. In our program, we index 12mers every 3 nt in the genome, which reduces the size of a human genomic hash table to 4 GB. As a result, our algorithm is designed to use a hash table sampled every 3 nt and still achieve full sensitivity as if every overlapping oligomer were indexed. A hash table indexing scheme can be extended to align major and minor alleles equally well in SNP-tolerant alignment. (For ease of discussion, we refer to the alleles in the reference sequence as ‘major’ and their corresponding alternate versions as ‘minor’, regardless of their actual frequencies in a population.) Because a hash table represents the genome in q-mer pieces, it can represent the enormous space of all combinations of major and minor alleles in a relatively straightforward way. To construct a SNP-tolerant hash table, we scan the genome and process each sampled genomic q-mer that contains one or more SNPs, by generating each possible combination of the major and minor alleles contained within and duplicating this genomic position for each generated q-mer. Finally, we re-sort the position list for each q-mer (Fig. 2B). A lookup in this hash table of any combination of major and minor alleles in a q-mer at a given genomic position will all contain the desired position. Our experience shows that a SNP-tolerant hash table is only slightly larger than the original. When we incorporate the 12 million SNPs from dbSNP version 129 into human genome version 36, the hash table increases in size from 3.8 to 4.0 GB. Our construction algorithm requires that the computer have sufficient memory to store the hash table, thereby requiring 4 GB for a human-sized genome.";Mapping
mrsFAST;"High throughput sequencing (HTS) platforms generate unprecedented amounts of data that introduce challenges for processing and downstream analysis. While tools that report the ‘best’ mapping location of each read provide a fast way to process HTS data, they are not suitable for many types of downstream analysis such as structural variation detection, where it is important to report multiple mapping loci for each read. For this purpose we introduce mrsFAST-Ultra, a fast, cache oblivious, SNP-aware aligner that can handle the multi-mapping of HTS reads very efficiently. mrsFAST-Ultra improves mrsFAST, our first cache oblivious read aligner capable of handling multi-mapping reads, through new and compact index structures that reduce not only the overall memory usage but also the number of CPU operations per alignment. In fact the size of the index generated by mrsFAST-Ultra is 10 times smaller than that of mrsFAST. As importantly, mrsFAST-Ultra introduces new features such as being able to (i) obtain the best mapping loci for each read, and (ii) return all reads that have at most n mapping loci (within an error threshold), together with these loci, for any user specified n. Furthermore, mrsFAST-Ultra is SNPaware, i.e. it can map reads to reference genome while discounting the mismatches that occur at common SNP locations provided by db-SNP; this significantly increases the number of reads that can be mapped to the reference genome. Notice that all of the above features are implemented within the index structure and are not simple post-processing steps and thus are performed highly efficiently. Finally, mrsFAST-Ultra utilizes multiple available cores and processors and can be tuned for various memory settings. Our results show that mrsFAST-Ultra is roughly five times faster than its predecessor mrsFAST. In comparison to newly enhanced popular tools such as Bowtie2, it is more sensitive (it can report 10 times or more mappings per read) and much faster (six times or more) in the multi-mapping mode. Furthermore, mrsFAST-Ultra has an index size of 2GB for the entire human reference genome, which is roughly half of that of Bowtie2. mrsFAST-Ultra is a seed and extend aligner in the sense that it works in two main stages: (i) it builds an index from the reference genome for exact ‘anchor’ matching and (ii) it computes all anchor matchings for each of the reads in the reference genome through the index and extends each match to both left and right; it reports the overall alignment if it is within the user defined error threshold. Indexing In the indexing step, mrsFAST-Ultra slides a window of size k = r/(e + 1) (where r is the read length and e is the user defined error threshold) through the reference genome and identifies all occurrences of each k-mer present in the genome. For small values of k, mrsFAST-Ultra’s genome index is an array of all possible k-mers in lexicographic order. For each k-mer, the index keeps an array of all locations the k-mer is observed in the reference genome. In case the value of k is prohibitively large, only a prefix of user defined size  (for each k-mer) is used for indexing. For each such -mer, its locations on the reference genome are then sorted with respect to the k − -mers following it. (In fact, for most applications, even keeping track of all k − -mers following a particular -mer is not necessary: we just hash these k − -mers via a simple checksum scheme.) For further compacting the index, the reference genome itself is first converted to a 3 bit per base encoding. The genome sequence is stored in 8 byte long machine words implying that each machine word contains 21 bases. In addition, the index of the reference genome actually does not keep every occurrence of each k-mer, but rather keeps how many occurrences of each k-mer is present in the genome. The actual locations of the k-mers (seeds) are recalculated each time the reference is loaded. This reduces the I/O requirements of mrsFAST-Ultra significantly. One may think that such a set up would increase the overall running time of the search step but the savings from I/O reduction significantly offsets the cost of recalculating the k-mer locations on the fly. Overall, the storage requirement of the index we construct for the human reference genome is 2GB, including the reference genome sequence itself. This represents a 10-fold improvement in the index storage requirement of the original mrsFAST. Search In this step, mrsFAST-Ultra processes the reads from an input HTS data set and computes ‘all’ locations on the reference genome that can be aligned to each read within the user-defined error threshold e. mrsFAST-Ultra is a fully sensitive aligner meaning that it guarantees to find and report all mapping locations of a given read within e mismatches. mrsFAST-Ultra achieves this by partitioning the read into e + 1 non-overlapping fragments of length k for a given error threshold e. Due to the pigeon hole principle, at least one of these fragments should have an exactly matching k-mer of the reference genome in each location the read can be mapped to. The search step then validates whether each location of the reference genome with an exact k-mer match of the read is indeed a mapping location. In order to perform the search step as fast as possible, mrsFAST-Ultra loads the genome index (see above) to the main memory and computes the locations of each k-mer on-the fly––for significant savings in I/O. For each k-mer, the number of locations in the reference genome is already stored in the index, thus we can preallocate the required memory for each array that keeps the locations of a given k-mer. Once this extended reference genome index is set up in the main memory, the remaining memory is allocated for the reads. At each subsequent stage, mrsFAST-Ultra retrieves sufficiently many (unprocessed) reads that can fit in the main memory and searches them in the reference genome simultaneously. (Alternatively, the user can specify an upper bound on the memory usage.) These reads are also indexed with respect to the e + 1 non-overlapping fragments of size k it extracts from each read. Basically, for each possible fragment of length k, the read index keeps the read ID, the fragment number and the direction the fragment is observed in the read. Once the read index is set, it is compared to the reference genome index, in a divide and conquer fashion as per mrsFAST, in order to achieve cache obliviousness. In other words, for each possible k-mer, the list of its occurrences in the reference genome is compared against the list of its occurrences among the reads in a divide-and-conquer fashion (rather than linear fashion) to ensure an optimal cache performance at any level of the cache structure, within a factor 2 (14). Because mrsFAST-Ultra aims to be fully sensitive, it needs to verify whether each reference genome location and each corresponding read that have the same k-mer have indeed an alignment within the user defined error tolerance. Note that, the value of k, set to r/(e + 1) can be too big for creating an index that has an entry for every possible k-mer from the four letter deoxyribonucleic acid (DNA) alphabet. Thus, the primary indexing is performed on a prefix of length  = 12 for each k-mer and all locations/reads that share this prefix are further sorted according to the k − - mer succeeding this prefix. This is achieved by hashing the k − -mer through a simple checksum scheme. As a result, the divide-and-conquer comparison of reference genome locations and reads is performed on those entries that have the same -mer and the same checksum value for the succeeding k − -mer. The comparison for each genomic location and a read involves the calculation of the Hamming distance between the read and the k-mer location in the genome, extended by the appropriate length towards left and right. Before calculating the Hamming distance, mrsFAST-Ultra applies another filter that compares the number of As, Cs, Gs and Ts in the read and the genomic locus; if the total number of symbol differences is more than 2e, then we do not need to compute the Hamming distance explicitly as it will be at least e + 1––above the error threshold. In comparison to the original mrsFAST, our new search strategy significantly reduces the number of Hamming distance calculations that is the main bottleneck for the search step. When combined with reduced I/O (due to compact index representation) and the introduction of new filters, this implies a five factor reduction in the overall running time of search. SNP awareness The user has the option of setting mrsFAST-Ultra to tolerate known SNP locations in the mappings: i.e. in this mode, SNPs in an alignment location simply do not contribute to the error count in the Hamming distance computation provided that a SNP location’s base quality is above user-defined threshold and it is matching the alternate allele. For this feature, mrsFAST-Ultra parses dbSNP file in VCF4 format (35) and generates a compact structure that it uses for mapping. Although conceptually simple, this feature is highly desired by users as it significantly reduces the number of reads that can not be mapped to anywhere in the reference genome. In this mode, mrsFAST-Ultra reports the number of SNPs in addition to the number of mismatches per mapping location. Best and limited mapping mrsFAST-Ultra provides the user the option of returning a single best mapping locus per read––which it performs much faster than computing all mapping loci. As per BWA, Bowtie2, SRmapper and others, a best mapping location (on the reference genome) is considered to be one which has the smallest number of differences with the read and in the case of a tie one is chosen at random and assigned a low mapping quality. In addition, mrsFAST-Ultra has the option to return only mapping loci of reads which map to at most n locations within the user-defined error threshold. These features help the users to control the mapping multiplicity––which can grow prohibitively for further downstream analysis.";Mapping
Velvet;"We have developed a new set of algorithms, collectively called “Velvet,” to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25–50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of ∼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies. The reads are first hashed according to a predefined k-mer length. This variable k is limited on the upper side by the length of the reads being hashed, to allow for a small amount of overlap, usually k = 21 for 25-bp reads. Smaller k-mers increase the connectivity of the graph by simultaneously increasing the chance of observing an overlap between two reads and the number of ambiguous repeats in the graph. There is therefore a balance between sensitivity and specificity determined by k (cf. Methods). For each k-mer observed in the set of reads, the hash table records the ID of the first read encountered containing that k-mer and the position of its occurrence within that read. Each k-mer is recorded simultaneously to its reverse complement. To ensure that each k-mer cannot be its own reverse complement, k must be odd. This first scan allows us to rewrite each read as a set of original k-mers combined with overlaps with previously hashed reads. We call this new representation of the read’s sequence the “roadmap.” A second database is created with the opposite information. It records, for each read, which of its original k-mers are overlapped by subsequent reads. The ordered set of original k-mers of that read is cut each time an overlap with another read begins or ends. For each uninterrupted sequence of original k-mers, a node is created. Finally, reads are traced through the graph using the roadmaps. Knowing the correspondence between original k-mers and the newly created nodes, Velvet proceeds from one node to the next, creating a new directed arc or incrementing an existing one as appropriate at each step. After constructing the graph, it is generally possible to simplify it without any loss of information. Blocks are interrupted each time a read starts or ends. This leads to the formation of “chains” of blocks, or linear connected subgraphs. This fragmentation of the graph costs memory space and lengthens calculation times. These chains can be easily simplified. Whenever a node A has only one outgoing arc that points to another node B that has only one ingoing arc, the two nodes (and their twins) are merged. Iteratively, chains of blocks are collapsed into single blocks. The simplification of two nodes into one is analogous to the conventional concatenation of two character strings, and also to some string graph based methods (Myers 2005). This straightforward transformation involves transferring arc, read, and sequence information as appropriate. Errors are corrected after graph creation to allow for simultaneous operations over the whole set of reads. In our framework, errors can be due to both the sequencing process or to the biological sample, for example, polymorphisms. Distinguishing polymorphisms from errors is a post-assembly task. A naive approach to error removal would be to use the difference between the expected coverage of genuine sequences and that of random errors. Therefore removing all the low coverage nodes (and their corresponding arcs) would remove the errors. However, this relies on the differences being due to genuine errors and not to biological variants present at a reasonable frequency in the sample, and errors being randomly distributed in the reads. Instead, Velvet focuses on topological features. Erroneous data create three types of structures: “tips” due to errors at the edges of reads, “bulges” due to internal read errors or to nearby tips connecting, and erroneous connections due to cloning errors or to distant merging tips. The three features are removed consecutively. A “tip” is a chain of nodes that is disconnected on one end. Removing these tips is a straightforward task. Discarding this information results in only local effects, as no connectivity is disrupted. Nonetheless, some restraint must be applied to the process to avoid eroding genuine sequences that are merely interrupted by a coverage gap. To deal with this issue, we define two criteria: length and minority count. A tip will only be removed if it is shorter than 2k. This arbitrary cutoff length was chosen because it is greater than the length in k-mers of an individual very short read. Erroneous constructs involving entire short reads are presumably extremely rare. In the case of long reads, this cutoff is set to be the maximum length tip that can be created by two nearby mistakes. A tip longer than 2k therefore represents either genuine sequence or an accumulation of errors that is hard to distinguish from novel sequence. In the latter case, clipping the read tips more stringently might be necessary. We define “minority count” as the property that, at the node where the tip connects to the rest of the graph, the arc leading to that tip has a multiplicity inferior to at least one of the other arcs radiating out of that junction node. In other words, starting from that node, going through the tip is an alternative to a more common path. This ensures that, at the local level, tips are removed in increasing order of multiplicity. Velvet progressively uncovers chains of high coverage nodes that are not destroyed by virtue of the previous criteria, thus preserving the graph from complete erosion. Velvet iteratively removes tips from the graph under these two criteria. When there are no more tips to remove, the graph is simplified once again.";Assembly
MetaVelvet;"An important step in ‘metagenomics’ analysis is the assembly of multiple genomes from mixed sequence reads of multiple species in a microbial community. Most conventional pipelines use a single-genome assembler with carefully optimized parameters. A limitation of a single-genome assembler for de novo metagenome assembly is that sequences of highly abundant species are likely misidentified as repeats in a single genome, resulting in a number of small fragmented scaffolds. We extended a single-genome assembler for short reads, known as ‘Velvet’, to metagenome assembly, which we called ‘MetaVelvet’, for mixed short reads of multiple species. Our fundamental concept was to first decompose a de Bruijn graph constructed from mixed short reads into individual sub-graphs, and second, to build scaffolds based on each decomposed de Bruijn sub-graph as an isolate species genome. We made use of two features, the coverage (abundance) difference and graph connectivity, for the decomposition of the de Bruijn graph. For simulated datasets, MetaVelvet succeeded in generating significantly higher N50 scores than any single-genome assemblers. MetaVelvet also reconstructed relatively low-coverage genome sequences as scaffolds. On real datasets of human gut microbial read data, MetaVelvet produced longer scaffolds and increased the number of predicted genes. Information obtained from the DNA sequencer is a set of sequence fragments, called reads , rather than the entire genomic DNA sequence. Therefore, genome assembly is required to reconstruct the original genome sequence from sequence reads. Although each read is short, it is possible to reconstruct longer sequences, called contigs , by identifying an overlap between reads and merging the reads. Genome assembly is generally performed in the following steps: If a large amount of reads sufficient to ‘cover’ the genome are given to the assembly program, overlaps exist between the reads and the contigs are obtained by merging the reads. The term ‘coverage’ for a position in a contig is defined as the number of reads that overlap at that position. The ‘coverage’ of a ‘contig’ is defined to be the average of coverages for all positions in the contig. The previous conventional assembly method is based on the so-called ‘overlap graph’, where each read is assigned to a node and an edge connects two nodes if the corresponding reads overlap. The assembly problem is reduced to finding a path visiting every node exactly once in the overlap graph, that is, a Hamiltonian path problem. However, the Hamiltonian path problem is nondeterministic polynomial time-complete (NP-complete). Furthermore, the overlap-graph-based assembly method cannot work effectively when applied to very short reads generated from a next-generation sequencer, because there are so many short overlaps between short reads and most of these overlaps are false. Therefore, several de novo assembly methods based on de Bruijn graphs have been proposed for short reads generated from next-generation sequencers ( 15 , 18 , 19 , 20 ). A de Bruijn graph is a data structure that compactly represents an overlap between short reads. A notable difference between a de Bruijn graph and an overlap graph is that each k -mer (word of length k ) instead of a read is assigned to a node, and thus, the size of a de Bruijn graph becomes independent of the size of the input of reads. The detailed definition of de Bruijn graph is shown below. Given a set of sequence reads, the de Bruijn graph-based assemblers first break each read according to a predefined k -mer length. It is clear that two adjacent k -mers in the read overlap at k  − 1 nucleotides. Second, a directed graph (de Bruijn graph) is constructed from the given sequence reads as follows: each overlapping ( k  − 1)-mer is encoded into a node in the directed graph so that each k -mer is represented by a directed edge in the graph. Each k -mer is encoded into a directed edge that connects a node labeled the first ( k  − 1)-mer of the k -mer and a node labeled the second ( k  − 1)-mer. On the constructed de Bruijn graph, each read is mapped to a path traversing the graph. The MetaVelvet assembler consists of four major steps: [1] Construction of a de Bruijn graph from the input reads. [2] Detection of multiple peaks on k -mer frequency distribution. [3] Decomposition of the constructed de Bruijn graph into individual subgraphs. [4] Assembly of contigs and scaffolds based on the decomposed subgraphs. In Step [1], for a given set of mixed sequence reads generated from multiple species, MetaVelvet constructs the main de Bruijn graph using Velvet functions. In Step [2], MetaVelvet calculates the histogram of k -mer frequencies and detects multiple peaks on the histogram, each of which would correspond to the genome of one species in a microbial community. The expected frequencies of k -mer occurrences were shown to follow a Poisson distribution in a single-genome assembly ( 21 ) and the expected k -mer frequencies in metagenome assembly were shown to follow a mixture of Poisson distributions ( 12 ). Hence, MetaVelvet approximates the empirical histogram of k -mer frequencies by a mixture of Poisson distributions and detects multiple peaks in the Poisson mixture. Furthermore, MetaVelvet classifies every node into one peak of the Poisson mixture. In Step [3], MetaVelvet distinguishes a subgraph composed of nodes belonging to a same peak from the other subgraphs in the main de Bruijn graph. MetaVelvet identifies shared nodes (chimeric nodes) between two subgraphs and disconnects two subgraphs by separating the shared nodes. In step [4], MetaVelvet builds contigs and scaffolds based on the decomposed subgraphs using Velvet functions. The essential part of Step [3] is to design and develop an algorithm to identify and separate ‘chimeric nodes’ in the main de Bruijn graph. If two species contain a common or similar subsequence in their genomes, the main de Bruijn graph contains a node assigned to the subsequence with two incoming edges and two outgoing edges, one of which comes from one species and the other comes from the other species. On the other hand, if the genome of one species contains a repeat subsequence (that is, a subsequence with multiple occurrences in the genome), the de Bruijn graph also contains a node assigned to the repeat subsequence with two incoming edges and two outgoing edges. All other nodes in the main de Bruijn graph must have only one incoming edge and one outgoing edge. To distinguish the chimeric node from the repeat node, the method uses coverage difference. Although the origin nodes of two incoming edges for the repeat node have the same k -mer frequencies, the origin nodes of two incoming edges for the chimeric node belong to two different species and hence have different k -mer frequencies. The formal definition of ‘chimeric node’ is given as a crossing node satisfying the following three conditions: (i) (necessary condition) the number of incoming edges is 2 and the number of outgoing edges is 2; (ii) (sufficient condition) the origin nodes of two incoming edges ( a and b ) belong to two different peaks and the destination nodes of outgoing edges ( c and d ) also belong to the same two peaks as the origin nodes and (iii) (sufficient condition) the chimeric node has a confluent node coverage of the two origin nodes. More precisely, the node coverage of the candidate chimeric node should be between, where a .cov represents the node coverage of a node a and y is a parameter in MetaVelvet called ‘allowable coverage difference’.";Assembly
Rnnotator;"Comprehensive annotation and quantification of transcriptomes are outstanding problems in functional genomics. While high throughput mRNA sequencing (RNA-Seq) has emerged as a powerful tool for addressing these problems, its success is dependent upon the availability and quality of reference genome sequences, thus limiting the organisms to which it can be applied. Here, we describe Rnnotator, an automated software pipeline that generates transcript models by de novo assembly of RNA-Seq data without the need for a reference genome. We have applied the Rnnotator assembly pipeline to two yeast transcriptomes and compared the results to the reference gene catalogs of these organisms. The contigs produced by Rnnotator are highly accurate (95%) and reconstruct full-length genes for the majority of the existing gene models (54.3%). Furthermore, our analyses revealed many novel transcribed regions that are absent from well annotated genomes, suggesting Rnnotator serves as a complementary approach to analysis based on a reference genome for comprehensive transcriptomics. These results demonstrate that the Rnnotator pipeline is able to reconstruct full-length transcripts in the absence of a complete reference genome. Condition-specific reads were pooled together and identical reads were removed. After removing duplicate reads, read error filtering was performed using a rare k-mer filtering approach. The frequency of each k-mer was calculated using a hash table and reads containing rare k-mers were not used in the assembly. Rare k-mers were defined as those that occurred less than three times in the set of unique reads. Several rare k-mer read filtering strategies were tested in order to determine the effect of the read filtering. The three filtering strategies were: i) no filter applied, ii) filter applied after removing duplicate reads, and iii) filter applied before removing duplicate reads (Additional file 1). The order of filtering and duplicate read removal is significant since a k-mer is more likely to be a low abundant k-mer after duplicate read removal than before. We discovered that filtering reads prior to assembly reduces the runtime and memory required by the assembly at the cost of slightly decreasing the assembly quality. For assembly of short read Illumina sequences, the Velvet assembler was used in conjunction with the AMOS assembly package [10, 11]. Eight runs of velveth were executed in parallel (once for each hash length, 19 through 33). Next eight runs of velvetg were run in parallel with parameters: cov_cutoff = 1, exp_cov = auto. Prior to merging contigs, all duplicates were removed and contigs were combined into a single FASTA file. The minimus2 pipeline [11], a lightweight assembler which is part of the AMOS package, was run using REFCOUNT = 0 (other parameters default). The protocol used to split misassembled contigs using stranded RNA-Seq reads includes: i) splitting contigs with long stretches of less than three mapped reads which are longer than one read length, ii) orienting contigs in the correct mRNA sense strand orientation, iii) generating a consensus contig by counting the number of A,C,G,T residues at each base position. BWA [16] was used to align the reads to the assembled contigs. The UCSC Blat software [17] was used to align contigs to both genome and transcriptome references. For yeast datasets the maximum intron size was set to 5,000. In all cases, only the best hits were taken, unless there were multiple best-scoring hits. The score of each alignment was calculated by the formula: s = matches - mismatches, as recommended. A similar strategy was used when aligning gene models to contigs (SC5314), again only taking the best scoring hits. Gene fusion events were detected by first aligning contigs to the reference genome (outlined above). Genomic coordinates for each aligned contig were compared with the genomic coordinates of every annotated gene. A contig and gene were considered overlapping if they shared an overlap which was longer than 50% of the gene length. Contigs containing two or more such genes were identified as containing a gene fusion event. When performing the single-run Velvet assemblies and the Oases assemblies hash length 21 was used (28 to 34 base pair read lengths). All other parameters were set to the default parameter set. Contigs > = 100 bp in length were used for comparison against other assemblers. For the Multiple-k assemblies, eight Velvet assemblies were first performed. In order to have a fair comparison against the Rnnotator assemblies, the same hash lengths were used when running Velvet. The Multiple-k script was then run using the eight Velvet assemblies as input.";Assembly
Taipan;"The shorter and vastly more numerous reads produced by second-generation sequencing technologies require new tools that can assemble massive numbers of reads in reasonable time. Existing short-read assembly tools can be classified into two categories: greedy extension-based and graph-based. While the graph-based approaches are generally superior in terms of assembly quality, the computer resources required for building and storing a huge graph are very high. In this article, we present Taipan, an assembly algorithm which can be viewed as a hybrid of these two approaches. Taipan uses greedy extensions for contig construction but at each step realizes enough of the corresponding read graph to make better decisions as to how assembly should continue. We show that this approach can achieve an assembly quality at least as good as the graph-based approaches used in the popular Edena and Velvet assembly tools using a moderate amount of computing resources. Input to Taipan is a multi-set of reads R of length l each (consisting of the original reads plus their reverse complements), the minimal overlap parameter k(k <l), and a threshold T. R is stored in a hash table. This hash table allows efficient processing of queries of the form get_overlapping_reads(S,t) for a DNA string S and t∈{k,…, l}, which return all reads in R whose prefix t matches the suffix t of S. Taipan assembles a new contig by choosing a read from R as a seed. This seed is iteratively extended in 3′ direction by one base at a time until there are either insufficient overlapping reads or a repeat is found. Subsequently, the same algorithm is used in 5′ direction. The algorithm to extend contig S in 3′ direction by a single base works as follows. A set O of overlapping reads is retrieved from the hash table by calling get_overlapping_reads(S,t) for each t∈{k,…, l}. Afterwards the directed overlap graph G=(O, E) is constructed, where (ri, rj)∈E if suffix t of ri matches prefix t of rj for some t∈{k,…, l}. G′ = (O, E′) is then built by removing all associative edges from G ((ri, rj)∈E is associative, if there exist two other edges (ri, rk), (rk, rj)∈E)). The set P consisting of all vertex-disjoint paths of G′ is determined. P is analyzed to determine the single-base extension of S using threshold T as follows: (a) if P contains at least two paths p1, p2 with length (pi) ≥ T, i∈{1, 2}, then a repeat is found and extension of S is terminated; (b) if P contains exactly one path p with length (p) ≥ T, then S is extended by the first nucleotide of the first read in p; (c) if P contains no path p with length (p) ≥ 1, then there are insufficient overlapping reads and extension of S is terminated; (d) if none of the above cases apply, G′ is enlarged along each path p and the rules (a)–(d) are recursively applied (up to a maximum of T steps) to the enlarged graph. After the assembly of a new contig all reads that exactly match this contig are removed from R and the hash table. The implementation of Taipan gains efficiency by the fact that O only slightly changes for subsequent extensions and by using a sorted trie data structure to represent G⊥ ← and P. Choice of seeds can affect assembly results. Taipan selects an unassembled read with highest occurrence as seed in order to minimize extensions of reads containing sequencing errors.";Assembly
AByss;"Whole transcriptome shotgun sequencing data from non-normalized samples offer unique opportunities to study the metabolic states of organisms. One can deduce gene expression levels using sequence coverage as a surrogate, identify coding changes or discover novel isoforms or transcripts. Especially for discovery of novel events, de novo assembly of transcriptomes is desirable. Transcriptome from tumor tissue of a patient with follicular lymphoma was sequenced with 36 base pair (bp) single- and paired-end reads on the Illumina Genome Analyzer II platform. We assembled ∼194 million reads using ABySS into 66 921 contigs 100 bp or longer, with a maximum contig length of 10 951 bp, representing over 30 million base pairs of unique transcriptome sequence, or roughly 1% of the genome. The transcriptome data belongs to a patient who presented at 44 years of age with bulky stage II A intra-abdominal follicular, grade 1 non-Hodgkin lymphoma based on an inguinal lymph node biopsy. The staging bone marrow biopsy revealed no lymphoma. Initial treatment consisted of eight cycles of CVP-R (cyclophosphamide, vincristine, prednisone and rituximab) chemotherapy and produced a partial response. However, within 3 months symptomatic progression of lymphoma was evident within the abdomen and a repeat inguinal lymph node biopsy revealed residual grade 1 follicular lymphoma. We obtained informed consent from the patient, approved by the Research Ethics Board, and material from this biopsy was subjected to genomic analyses including whole transcriptome shotgun sequencing (WTSS). RNA was extracted from the tumour biopsy sample using AllPrep DNA/RNA Mini Kit (Qiagen, USA) and DNaseI (Invitrogen, USA) treated following the manufacturer's protocol. We generated three WTSS libraries, one from amplified complementary DNA (cDNA), another from the same amplified cDNA with normalization, and the last from unamplified cDNA, as follows. Double-stranded amplified cDNA was generated from 200 ng RNA by template-switching cDNA synthesis kit (Clontech, USA) using Superscript Reverse Transcriptase (Invitrogen, USA), followed by amplification using Advantage 2 PCR kit (Clontech, USA) in 20 cycle reactions. Custom biotinylated PCR primers containing MmeI recognition sequences were used to facilitate the removal of primer sequences from cDNA template for WTSS-Lite library construction. Normalized cDNA was generated from 1.2μg of the above amplified cDNA using Trimmer cDNA Normalization Kit (Evrogen, Russia) followed by amplification using the same biotinylated PCR primers, in a single 15 cycle reaction with Advantage 2 Polymerase (Clontech, USA). The normalized and amplified cDNA pool generated with the 1/2× duplex-specific nuclease (DSN) enzyme dilution was chosen to be the template for WTSS-Lite normalized library construction. For both WTSS-Lite and normalized WTSS-Lite libraries, the removal of amplification oligonucleotide templates from the cDNA ends was accomplished by the binding to M-280 Streptavidin beads (Invitrogen, USA), followed by MmeI digestion. The supernatant of digest was purified and prepared for library construction as follows: roughly 500 ng of cDNA template was sonicated for 5 min using a Sonic Dismembrator 550 (cup horn, Fisher Scientific, Canada), and size fractionated using 8% PAGE gel. The 100–300 bp size fraction was excised for library construction according to the Genomic DNA Sample Prep Kit protocol (Illumina, USA), using 10 cycles of PCR and purified using a Spin-X Filter Tube (Fisher Scientific) and ethanol precipitation. The library DNA quality was assessed and quantified using an Agilent DNA 1000 series II assay and Nanodrop 1000 spectrophotometer (Nanodrop, USA) and diluted to 10 nM. We assembled the reads using ABySS (Simpson et al., 2009). The ABySS algorithm is based on a de Bruijn di-graph representation of sequence neighborhoods (de Bruijn, 1946), where a sequence read is decomposed into tiled sub-reads of length k(k-mers) and sequences sharing k−1 bases are connected by directed edges. This approach was introduced to DNA sequence assembly by Pevzner et al. (2001) and was followed by others (Butler et al., 2008; Chaisson and Pevzner, 2008; Jackson et al., 2009; Zerbino and Birney, 2008). Although memory requirements for implementing de Bruijn graphs scale linearly with the underlying sequence, ABySS uses a distributed representation that relaxes these memory and computation time restrictions (Simpson et al., 2009). A de Bruijn graph captures the adjacency information between sequences of a uniform length k, defined by an overlap between the last and the first k−1 characters of two adjacent k-mers. ABySS starts by cataloging k-mers in a given set of reads and establishes their adjacency, represented in a distributed data format. The resulting graph is then inspected to identify potential sequencing errors and small-scale sequence variation. When a sequence has a read error, it alters all the k-mers that span it, which form branches in the graph. However, since such errors are stochastic in nature, their rate of observation is substantially lower than that of correct sequences. Hence, they can be discerned using coverage information, and branches with low coverage can be culled to increase the quality and contiguity of an assembly. This is especially true for genomic sequences. For transcriptomes, however, sequence coverage depth is a function of the transcript expression level it represents. Therefore, such culling needs to be performed with extra care. Accordingly, in the assembly stage, we applied trimming for those (false) branches when the absolute coverage levels were below a threshold of 2-fold (2×). In the analysis stage, we evaluated assembly branches using the local coverage information, as well as contig lengths. For instance, in a neighborhood where a contig C1 branches into contig C2 and C3, with coverage levels of x1, x2 and x3, and contig lengths l1, l2 and l3, respectively, if x1 and x2 are significantly higher than x3, and l3 is shorter than a threshold, then we assume that C3 is a false branch. Some repeat read errors and small-scale sequence variation between approximate repeats or alleles result in some of the branches merging back to the trunk of the de Bruijn graph. We call such structures ‘bubbles’, and remove them during the assembly. Since they may represent real albeit alternative sequence at that location, we preserve the information they carry by recording them in a special log file, along with the variant we leave in the assembly contig and their coverage levels. These log entries are later used to postulate effects of allelic variations on expression levels. After the false branches are culled and bubbles removed, unambiguously linear paths along the de Bruijn graph are connected to form the single end tag assembly (SET) contigs. The branching information is also recorded for the subsequent paired end tag assembly (PET) stage and further analysis. At the SET stage, every k-mer represented in the assembly contigs has a unique occurrence. Using that information, we apply a streamlined read-to-assembly alignment routine. We use the aligned read pairs to (i) infer read distance distributions between pairs in libraries that form our read set, and (ii) identify contigs that are in a certain neighborhood defined by these distributions. The adjacency and the neighborhood information are used by the PET routine to merge SET contigs connected by pairs unambiguously, while keeping the list of the merged SET contigs (or the pedigree information) in the FASTA header for backtracking. The adjacency, neighborhood and pedigree information, along with the contig coverage information are also used by our assembly visualization tool, ABySS-Explorer. Figure 1 shows the ABySS-Explorer representation of some SET contigs in a neighborhood. Note that both the edges and the nodes are polarized in accordance with the directionality of the contigs and the k − 1-mer overlaps between them, respectively. In the interactive view, when a user double-clicks on a contig, its direction and node connection polarizations flip to reflect the reverse complement. Paired end tags often resolve paths along SET contigs and they are subsequently merged in the PET stage. We indicate such merged contigs by dark gray paths in the viewer, when one of the contigs contributing to a merge is selected. The ABySS-Explorer representation encodes additional information including contig coverage (indicated by the edge thickness) and contig length (indicated by the edge lengths). A wave representation is used to indicate contig length such that a single oscillation corresponds to a user-defined number of nucleotides. A long contig results in packed oscillations, which obscure the arrowhead indicating its direction. To resolve this ambiguity, the envelope of the oscillations outlines a leaf-like shape with the thicker stem of the shape marking the start of the contig and the thinner tip pointing to its end. For example, contig 383 936 is a 627 bp long contig, which is much longer than the shortest contig 297 333 (29 bp), but its direction is still evident from its shape, with the thinner tip pointing to the right. We performed a parameter search for assembly optimization by varying k values in the range between 25 bp and 32 bp for the SET stage. Figure 2 shows some key statistics of our assemblies as a function of k. We picked the best assembly to be that for k=28, as the number of contigs drops significantly between k=27 and k=28, while the number of contigs 100 bp or longer do not increase, which indicate a substantial improvement in contiguity. Beyond k=28, the number of contigs in both categories keeps decreasing, but so does the assembly N50.";Assembly
ALLPATHS;"New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun “microreads.” For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80× coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads. As with unpaired reads, the first step is to use the reads to compute an approximation to the unipaths (see “Algorithmic Ingredients for Unpaired-Read Assembly,” above). We do this without ever computing the set of all overlaps between reads, since, as discussed in “The Challenge of Microread Assembly” above, this would be computationally prohibitive. The unipath computation ignores the pairing of reads. Now with the unipaths and read pairs in hand, we are ready to localize. The first step is to pick “seeds”: these are unipaths around which we will build assemblies of genomic regions. The ideal seed unipaths are long and of low copy number (ideally one). Copy number is inferred from read coverage of the unipaths. (Implementation for real reads will need to take account of deviations from even coverage that are characteristic of particular sequencing technologies.) Using read pairing, we can choose seeds judiciously, spacing them so that the regional assemblies will overlap by a few kilobases where possible, facilitating subsequent gluing (see Methods, “Finding Seed Unipaths”). It is not necessary to use every ideal unipath as a seed. The genomic region containing the seed and extending 10 kb on each side is called the “neighborhood” of the seed. Our goal is to assemble the neighborhood. To that end, we first build a collection of sequences (reads, unipaths, etc.) that lie mostly within it. First, define a collection of low-copy-number unipaths that partially cover the neighborhood. This is done by iterative linking (Fig. 2A). Each unipath is assigned coordinates relative to the seed, with error bars. Because the error bars are in general large, the precise order of the unipaths is unknown, and thus the structure is better thought of as a “cloud” rather than a “scaffold.” Next, we construct two sets of reads for the neighborhood: the “primary read cloud,” generally containing only reads whose true genomic locations are near the seed (but not all such reads), and the “secondary read cloud,” generally containing all the short-fragment read pairs near the seed, and some outsiders as well. In more detail, the primary read cloud consists of those reads incident upon one of the neighborhood unipaths, plus their partners, some of which reach into gaps (Fig. 2B). If both reads in a pair land entirely within high-copy-number unipaths, the pair will not be in the primary read cloud; thus, sufficiently repetitive read pairs are excluded. The secondary read cloud consists of all short-fragment read pairs (∼0.5 kb) from the entire data set for which the sequence of both reads could be assembled from reads in the primary cloud (Fig. 3). This operation pulls in some pairs from outside the neighborhood, but usually finds all shortfragment pairs from inside the neighborhood, including highly repetitive ones; thus, the secondary read cloud is complete but can be contaminated. In addition to completeness, the secondary read cloud has the advantage that it consists of short-fragment pairs, which generally have far fewer closures than longer-fragment pairs. Nevertheless, the same problem of “too many closures” (see “Algorithmic Ingredients for Paired-Read Assembly,” above) persists. There are parts of genomes that are locally repetitive, typically consisting of low-complexity sequence. Thus, even though localization allows us to zoom in on a small region of the genome (∼20 kb), and even though short-fragment (∼0.5 kb) read pairs capture a very small piece of this region, there are still many cases in which the number of closures of these pairs is too large to compute or store or use. This does not happen for E. coli (Table 2) but can happen for more complex genomes. The problem is compounded by the large number of short-fragment read pairs. To mitigate this problem, we take the short-fragment read pairs in the neighborhood (the secondary read cloud) and progressively merge these pairs together (see Methods, “ShortFragment Pair Merger”). The end result is that we obtain a smaller number of pairs, and the pairs themselves are more informative: the reads are lengthened (effectively turned into contigs), and the pair separations and their SDs are reduced. Because there are fewer pairs and they are more informative, they will have fewer closures. Next, we compute the closures of all the merged short-fragment pairs, using only the reads from these pairs. The resulting set of closure sequences should cover the neighborhood correctly, but in general will also include false closures that do not align perfectly to it. These closures are then used, as though they were reads, to walk a selection of mid-length (∼5 kb) read pairs from the primary read cloud. The closures of these mid-length read pairs are glued together, yielding a sequence graph: the assembly graph for the neighborhood. This gluing process works by iteratively joining closures that have long end-to-end overlapping stretches (Fig. 4). This process will join together some identical sequences that come from different parts of the genome. See Step 7, below, for how these may be subsequently pulled apart. As suggested in “Algorithmic Ingredients for Paired-Read Assembly,” above, both the local and the global fragmentwalking processes can blow up in sufficiently repetitive regions, by exceeding predetermined computational limits. The typical effect of this computational failure is that the neighborhood assembly contains a hole, where sequence from the neighborhood is completely missing. This effect will be seen in the final assemblies (see “Results for Assemblies of Simulated Data,” below), where, for example, a few percent of the genome may be absent. The local assemblies run in parallel. Once complete, their outputs (local assembly graphs) are glued together formally (Fig. 4), yielding a single sequence graph (see Methods, “Sequence Graphs”), which may have several components, depending on the number of chromosomes in the genome and the success of the assembly process. At this stage, if there are long perfect repeats, they are likely assembled on top of each other. These collapsed parts of the assembly may be pulled apart in the next step, provided that the repeat length is less than the longest library fragment size. This graph generally provides an imperfect representation of the genome, and can be improved. To do so, we first find all perfect placements of the error-corrected reads on the assembly graph. Then we find all consistent placements for read pairs. This step is time- and memory-intensive, and we note that it would not necessarily work in precisely the same fashion for mammalian-size genomes, where a read pair with two repetitive ends could have a huge number of placements on the genome. We next carry out a series of editing steps (Fig. 5) to create the final assembly, which is again a sequence graph. These editing steps remove detritus, eliminate ambiguity in some cases, and where possible pull apart regions where repeats are assembled on top of each other.";Assembly
Oases;"High-throughput sequencing has made the analysis of new model organisms more affordable. Although assembling a new genome can still be costly and difficult, it is possible to use RNA-seq to sequence mRNA. In the absence of a known genome, it is necessary to assemble these sequences de novo, taking into account possible alternative isoforms and the dynamic range of expression values. We present a software package named Oases designed to heuristically assemble RNA-seq reads in the absence of a reference genome, across a broad spectrum of expression values and in presence of alternative isoforms. It achieves this by using an array of hash lengths, a dynamic filtering of noise, a robust resolution of alternative splicing events and the efficient merging of multiple assemblies. It was tested on human and mouse RNA-seq data and is shown to improve significantly on the transABySS and Trinity de novo transcriptome assemblers. The Oases assembly process, explained in detail below and illustrated in Figure 1, consists of independent assemblies, which vary by one important parameter, the hash (or k-mer) length. In each of the assemblies, the reads are used to build a de Bruijn graph, which is then simplified for errors, organized into a scaffold, divided into loci and finally analyzed to extract transcript assemblies or transfrags. Once all of the individual k-mer assemblies are finished, they are merged into a final assembly. The Oases pipeline receives as input a preliminary assembly produced by the Velvet assembler (Zerbino and Birney, 2008) which was designed to produce scaffolds from genomic readsets. Its initial stages, namely hashing and graph construction can be used indifferently on transcriptome data. We only run these stages of Velvet to produce a preliminary fragmented assembly, containing the mapping of the reads onto a set of contigs. However, the later stage algorithms, Pebble and Rock Band, which resolve repeats in Velvet, are not used because they rely on assumptions related to genomic sequencing (Zerbino et al., 2009). Namely, the coverage distribution should be roughly uniform across the genome and the genome should not contain any branching point. These conditions prevent those algorithms from being reliable and efficient on RNA-seq data. After reading the contigs produced by Velvet, Oases proceeds to correct them again with a set of dynamic and static filters. The first dynamic correction is a slightly modified version of Velvet's error correction algorithm, TourBus. TourBus searches through the graph for parallel paths that have the same starting and end node. If their sequences are similar enough, the path with lower coverage is merged into the path with higher coverage, irrespective of their absolute coverage. In this sense, the TourBus algorithm is adapted to RNA-seq data and fluctuating coverage depths. However, for performance issues, the Velvet version of TourBus only visits each node once, meaning that it does not exhaustively compare all possible pairs of paths. Given the high coverage of certain genes, and the complexity of the corresponding graphs, with numerous false positive paths, it is necessary for Oases to exhaustively examine the graph, visiting nodes several times if necessary. In addition to this correction, Oases includes a local edge removal. For each node, an outgoing edge is removed if its coverage represents <10% of the sum of coverages of outgoing edges from that same node. This approach, similar to the one presented by Yassour et al. (2011), is based on the assumption that on high coverage regions, spurious errors are likely to reoccur more often. Finally, all contigs with less than a static coverage cutoff (by default 3×) are removed from the assembly. The rationale for this filter is that any transcript with such a low coverage cannot be properly assembled in the first place, so it is expedient to remove them from the assembly, along with many low coverage contigs created by spurious errors. The distance information between the contigs is then summarized into a set of distance estimates called a scaffold, as described in (Zerbino et al., 2009). Because a read in a de Bruijn graph can be split between several contigs, the distance estimate for a connection between two contigs can be supported by both spanning single reads or paired-end reads. The total number of spanning reads and pair-end reads confirming a connection is called its support. A connection which is supported by at least one spanning read is called direct, otherwise, it is indirect. Connections are assigned a total weight. It is calculated by adding 1 for each supporting spanning read and a probabilistic weight for each spanning pair, proportional to the likelihood of observing the paired reads at their observed positions on the contigs given the estimated distance between the contigs and assuming a normal insert length distribution model. Much like the contig correction phase, several filters are applied to the scaffold: static coverage thresholds for the very low coverage sequences and a dynamic coverage threshold that adapts to the local coverage depth. Because coverage is no longer indicative of the uniqueness of a sequence, contig length is used as an indicator. Based on the decreasing likelihood of high identity conservation as a function of sequence length (Whiteford et al., 2005), contigs longer than a given threshold [by default (50+k−1) bp] are labeled as long and treated as if unique and the other nodes are labeled as short. Connections with a low support (by default 3× or lower) or with a weight <0.1 are first removed. Two short contigs can only be joined by a direct connection with no intermediate gap. A short and a long contig can only be connected by a direct connection. Finally, connections between long contigs are tested against a modified version of the statistic presented in (Zerbino et al., 2009), which estimates how many read pairs should connect two contigs given their respective coverages and the estimated distance separating them (see Supplementary Material). Indirect connections with a support lower than a given threshold (by default 10% of this expected count) are thus eliminated. Oases then organizes the contigs into clusters called loci, as illustrated in Figure 1. This terminology stems from the fact that in the ideal case, where no gap in coverage or overlap with exterior sequences complicate matters, all the transcripts from one gene should be assembled into a connected component of contigs. Unfortunately, in experimental conditions, this equivalence between components and genes cannot be guaranteed. It is to be expected that loci sometimes represent fragments of genes or clusters of homologous sequences. Scaffold construction takes place in two stages similarly to the approach described by Butler et al. (2008). Long contigs are first clustered into connected components. These long nodes have a higher likelihood of being unique, therefore it is assumed that two contigs which belong to the same component also belong to the same gene. To each locus are added the short nodes which are connected to one of the long nodes in the cluster. For the following analyses to function properly, it is necessary to remove redundant long distance connections, and retain only connections between immediate neighbors, as seen in Figure 1. For example, it is common that two contigs which are not consecutive in a locus are connected by a paired-end read. A connection is considered redundant if it connects two nodes that are connected by a distinct path of connections such that the connection and the two paths have comparable lengths. The transitive reduction implemented in Oases is inspired from the one described in (Myers, 2005) but had to be adapted to the conditions of short read data. In particular, short contigs can be repeated or even inverted within a single transcript and form loops in the connection graph. Because of this, occasional situations arise where every connection coming out of a node can be transitively reduced by another one, thus removing all of them, and breaking the connectivity of the locus. To avoid this, a limit is imposed on the number of removed connections. If two connections have the capacity to reduce each other, the shortest one is preserved. The sequence information of the transcripts is now contained in the loci. These loci can be fragmented because of alternative splicing events which cause the de Bruijn graph to have a branch. Oases, therefore, analyses the topology of the loci to extract full length isoform assemblies. In many cases, the loci present a simple topology which can be trivially and uniquely decomposed as one or two transcripts. We define three categories of trivial locus topologies (Fig. 1): chains, forks and bubbles, which if isolated from any other branching point, are straightforward to resolve. These three topologies are easily identifiable using the degrees of the nodes. Oases, therefore, detects all the trivial loci and enumerates the possible transcripts for each of them. Because the above exact method only applies to specific cases, an additional robust heuristic method is applied to the remaining loci, referred to as complex loci. Oases uses a reimplementation of the algorithm described in (Lee, 2003), which efficiently produces a parsimonious set of putative highly expressed transcripts, assuming independence of the alternative splicing events. This extension of the algorithm is quite intuitive, since there is a direct analogy between the de Bruijn graph built from the transcripts of a gene and its splicing graph, as noted by Heber et al. (2002). Using dynamic programming, it enumerates heavily weighted paths through the locus graph in decreasing order of coverage, until either all the contigs of the locus are covered, or a specified number of transcripts is produced (by default 10). As in the transitive reduction phase, this algorithm had to be slightly modified to allow for loops in the putative splicing graph of the locus. Loops are problematic because their presence can prevent the propagation of the dynamic programming algorithm to all the contigs of a locus. When a loop is detected, it is broken at a contig which connects the loop to the rest of the locus, so as to leave a minimum number of branch points, as described in the Supplementary Material. De Bruijn graph assemblers are very sensitive to the setting of the hash length k. For transcriptome data, this optimization is more complex as transcript expression levels and coverage depths are distributed over a wide range. A way to avoid the dependence on the parameter k is to produce a merged transcriptome assembly of previously generated transfrags from Oases. Oases is run for a set of [kMIN,…,kMAX] values and the output transfrags are stored. All predicted transfrags from runs in the interval are then fed into the second stage of the pipeline, Oases-M, with a user selected kMERGE. A de Bruijn graph for kMERGE is built from these transfrags. After removing small variants with the Tourbus algorithm, any transfrag in the graph that is identical or included in another transfrag is removed. The final assembly is constructed by following the remaining transfrags through the merged graph";Assembly
SPADES;"The lion's share of bacteria in various environments cannot be cloned in the laboratory and thus cannot be sequenced using existing technologies. A major goal of single-cell genomics is to complement gene-centric metagenomic data with whole-genome assemblies of uncultivated organisms. Assembly of single-cell data is challenging because of highly non-uniform read coverage as well as elevated levels of sequencing errors and chimeric reads. We describe SPAdes, a new assembler for both single-cell and standard (multicell) assembly, and demonstrate that it improves on the recently released E+V−SC assembler (specialized for single-cell data) and on popular assemblers Velvet and SoapDeNovo (for multicell data). SPAdes generates single-cell assemblies, providing information about genomes of uncultivatable bacteria that vastly exceeds what may be obtained via traditional metagenomics studies. Below we outline the four stages of SPAdes, which deal with issues that are particularly troublesome in SCS: sequencing errors; non-uniform coverage; insert size variation; and chimeric reads and bireads: (1) Stage 1 (assembly graph construction) is addressed by every NGS assembler and is often referred to as de Bruijn graph simplification (e.g., bulge/bubble removal in EULER/Velvet). We propose a new approach to assembly graph construction that uses the multisized de Bruijn graph, implements new bulge/tip removal algorithms, detects and removes chimeric reads, aggregates biread information into distance histograms, and allows one to backtrack the performed graph operations. (2) Stage 2 (k-bimer adjustment) derives accurate distance estimates between k-mers in the genome (edges in the assembly graph) using joint analysis of distance histograms and paths in the assembly graph. (3) Stage 3 constructs the paired assembly graph, inspired by the PDBG approach. (4) Stage 4 (contig construction) was well studied in the context of Sanger sequencing (Ewing et al., 1998). Since NGS projects typically feature high coverage, NGS assemblers generate rather accurate contigs (although the accuracy deteriorates for SCS). SPAdes constructs DNA sequences of contigs and the mapping of reads to contigs by backtracking graph simplifications (see Section 8.6). Previous studies demonstrated that coupling various assemblers with error correction tools improves their performance (Pevzner et al., 2001; Kelley et al., 2010; Ilie et al., 2010; Gnerre et al., 2011).4 However, most error correction tools (e.g., Quake [Kelley et al., 2010]) perform poorly on single-cell data since they implicitly assume nearly uniform read coverage. Chitsaz et al. (2011) coupled Velvet-SC with the errorcorrection in EULER (Chaisson and Pevzner, 2008), resulting in the tool E + V-SC. In this article, SPAdes uses a modification of Hammer (Medvedev et al., 2011b) (aimed at SCS) for error correction and quality trimming prior to assembly.";Assembly
SOAPdenovo2;"There is a rapidly increasing amount of de novo genome assembly using next-generation sequencing (NGS) short reads; however, several big challenges remain to be overcome in order for this to be efficient and accurate. SOAPdenovo has been successfully applied to assemble many published genomes, but it still needs improvement in continuity, accuracy and coverage, especially in repeat regions. To overcome these challenges, we have developed its successor, SOAPdenovo2, which has the advantage of a new algorithm design that reduces memory consumption in graph construction, resolves more repeat regions in contig assembly, increases coverage and length in scaffold construction, improves gap closing, and optimizes for large genome. Benchmark using the Assemblathon1 and GAGE datasets showed that SOAPdenovo2 greatly surpasses its predecessor SOAPdenovo and is competitive to other assemblers on both assembly length and accuracy. We also provide an updated assembly version of the 2008 Asian (YH) genome using SOAPdenovo2. Here, the contig and scaffold N50 of the YH genome were ∼20.9 kbp and ∼22 Mbp, respectively, which is 3-fold and 50-fold longer than the first published version. The genome coverage increased from 81.16% to 93.91%, and memory consumption was ∼2/3 lower during the point of largest memory consumption. Dealing with sequencing error in NGS data is inevitable, especially for genome assembly applications, the outcome of which could be largely affected by even a small amount of sequencing error. Hence it is mandatory to detect and revise these sequencing errors in reads before assembly [2,7]. However, the error correction module in SOAPdenovo was designed for short Illumina reads (35–50 bp), which consumes an excessive amount of computational time and memory on longer reads, for example, over 150 GB memory running for two days using 40-fold 100 bp paired-end Illumina HiSeq 2000 reads. Thus, by a skillful exploitation of data indexing strategies, we redeveloped the module, which supports memory efficient long-k-mer error correction and uses a new space k-mer scheme to improve the accuracy and sensitivity (see Additional file 1: Supplementary Method 1 and Figures S1–S3). Simulation test shows that the new version runs efficiently and corrects more reads authentically (see Additional file 1: Tables S1 and S2). In DBG-based large-genome assembly, the graph construction step consumes the largest amount of memory. To reduce this in SOAPdenovo2, we implemented a sparse de Bruijn graph method [8] (see Additional file 1: Supplementary Method 2), where reads are cut into k-mers and a large number of the linear unique k-mers are combined as a group instead of being stored independently. Another important factor in the success of DBG-based assembly is k-mer size selection. Using a large k-mer has the advantage of resolving more repeat regions; whereas, use of small k-mers is advantageous for assembling low coverage depth and removing sequencing errors. To fully utilize both these advantages, we introduced a multiple k-mer strategy [9] in SOAPdenovo2 (see Additional file 1: Supplementary Method 3 and Figure S4). First, we removed sequencing errors using small k-mers for graph building, and then we rebuilt the graph using larger k-mers iteratively by mapping the reads back to the previous DBG to resolve longer repeats. Scaffold construction is another area that needs improvement in NGS de novo assembly programs [10]. In the original SOAPdenovo, scaffolds were built by utilizing PE reads starting with short insert sizes (∼200 bp) followed iteratively to large insert sizes (∼10 kbp) [3]. Although this iterative method greatly decreased the complexity of scaffolding and enabled the assembly of larger genomes, there remained many issues that resulted in lower scaffold quality and shorter length. For example, 1) the heterozygous contigs were improperly handled; 2) chimeric scaffolds erroneously built with the smaller insert size PE reads which then hindered the later steps to increase of scaffold length when adding PE reads with larger insert size; and 3) false relationships between contigs without sufficient PE information support were created occasionally. To improve this in SOAPdenovo2, the main changes during the scaffolding stage were as follows: 1) we detected heterozygous contig pairs using contig depth and local contig relationships. Under these conditions, only the contig with higher depth in the heterozygous pairs was kept in scaffold, which reduced the influence of heterozygosity on the scaffolds length; 2) chimeric scaffolds that were built using a smaller insert size library were rectified using information from a larger insert size library, and 3) we developed a topology-based method to reestablish relationships between contigs that had insufficient PE information support (see Additional file 1: Supplementary Method 4 and Figures S5–S7). Short reads enabled us to reconstruct large vertebrate and plant genomes, but the assembly of repetitive sequences longer than the read length still remain to be tackled. In scaffold construction, contigs with certain distance relationship, but without genotypes amid were connected with wildcards. The GapCloser module was designed to replace these wildcards using the context and PE reads information. In SOAPdenovo2, we have improved the original SOAPdenovo GapCloser module, which assembled sequences iteratively in the gaps to fill large gaps. At each iterative cycle, the previous release of GapCloser considered only the reads that could be aligned in current cycle. This method could potentially make for an incorrect selection at inconsistent locations with insufficient information for distinguishment due to the high similarity between repetitive sequences. For SOAPdenovo2, we developed a new method that considered all reads aligned during previous cycles, which allowed for better resolution of these conflicting bases, and thus improved the accuracy of gap closure.";Assembly
SSAKE;"Novel DNA sequencing technologies with the potential for up to three orders magnitude more sequence throughput than conventional Sanger sequencing are emerging. The instrument now available from Solexa Ltd, produces millions of short DNA sequences of 25 nt each. Due to ubiquitous repeats in large genomes and the inability of short sequences to uniquely and unambiguously characterize them, the short read length limits applicability for de novo sequencing. However, given the sequencing depth and the throughput of this instrument, stringent assembly of highly identical sequences can be achieved. We describe SSAKE, a tool for aggressively assembling millions of short nucleotide sequences by progressively searching through a prefix tree for the longest possible overlap between any two sequences. SSAKE is designed to help leverage the information from short sequence reads by stringently assembling them into contiguous sequences that can be used to characterize novel sequencing targets. DNA sequences in a single multi fasta file are read in memory, populating a hash table keyed by unique sequence reads with values representing the number of occurrences of that sequence in the set. A prefix tree is used to organize the sequences and their reverse-complemented counterparts by their first eleven 5′ end bases. The sequence reads are sorted by decreasing number of occurrences to reflect coverage and minimize extension of reads containing sequencing errors. Each unassembled read, u, is used in turn to nucleate an assembly. Each possible 3′ most k-mer is generated from u and is used for the search until the word length is smaller than a user-defined minimum, m, or until the k-mer has a perfect match with the 5′ end bases of read r. In that latter case, u is extended by the unmatched 3′ end bases contained in r, and r is removed from the hash table and prefix tree. The process of cycling through progressively shorter 3′-most k-mers is repeated after every extension of u. Since only left-most searches are possible with a prefix tree, when all possibilities have been exhausted for the 3′ extension, the complementary strand of the contiguous sequence generated (contig) is used to extend the contig on the 5′ end. The DNA prefix tree is used to limit the search space by efficiently binning the sequence reads. There are two ways to control the stringency in SSAKE. The first is to stop the extension when a k-mer matches the 5′ end of more than one sequence read (−s 1). This leads to shorter contigs, but minimizes sequence misassemblies. The second is to stop the extension when a k-mer is smaller than a user-set minimum word length (m). SSAKE outputs a log file with run information along with two multi fasta files, one containing all sequence contigs constructed and the other containing the unassembled sequence reads.";Assembly
GAML;"Resolution of repeats and scaffolding of shorter contigs are critical parts of genome assembly. Modern assemblers usually perform such steps by heuristics, often tailored to a particular technology for producing paired or long reads. We propose a new framework that allows systematic combination of diverse sequencing datasets into a single assembly. We achieve this by searching for an assembly with the maximum likelihood in a probabilistic model capturing error rate, insert lengths, and other characteristics of the sequencing technology used to produce each dataset. We have implemented a prototype genome assembler GAML that can use any combination of insert sizes with Illumina or 454 reads, as well as PacBio reads. Our experiments show that we can assemble short genomes with N50 sizes and error rates comparable to ALLPATHS-LG or Cerulean. While ALLPATHS-LG and Cerulean require each a specific combination of datasets, GAML works on any combination. We have introduced a new probabilistic approach to genome assembly and demonstrated that this approach can lead to superior results when used to combine diverse set of datasets from different sequencing technologies. Complex probabilistic models, like the one described in “Probabilistic model for sequence assembly”, were previously used to compare the quality of several assemblies [11–13]. In our work, we instead attempt to find the highest likelihood assembly directly. Of course, the search space is huge, and the objective function too complex to admit exact methods. Here, we describe an effective optimization routine based on the simulated annealing framework [19]. Our algorithm for finding the maximum likelihood assembly consists of three main steps: preprocessing, optimization, and postprocessing. In preprocessing, we decrease the scale of the problem by creating an assembly graph, where vertices correspond to contigs and edges correspond to possible adjacencies between contigs supported by reads. In order to make the search viable, we will restrict our search to assemblies that can be represented as a set of walks in this graph. Therefore, the assembly graph should be built in a conservative way, where the goal is not to produce long contigs, but rather to avoid errors inside them. In the optimization step, we start with an initial assembly (a set of walks in the assembly graph), and iteratively propose changes in order to optimize the assembly likelihood. Finally, postprocessing examines the resulting walks and splits some of them into shorter contigs if there are multiple equally likely possibilities of resolving ambiguities. This happens, for example, when the genome contains long repeats that cannot be resolved by any of the datasets. In the rest of this section, we discuss individual steps in more detail. Optimization by simulated annealing To find a high likelihood assembly, we use an iterative simulated annealing scheme. We start from an initial assembly 𝐴0 in the assembly graph. In each iteration, we randomly choose a move that proposes a new assembly 𝐴′ similar to the current assembly A. To further reduce the complexity of the assembly problem, we classify all contigs as either long (more than 500 bp) or short and concentrate on ordering the long contigs correctly. The short contigs are used to fill the gaps between the long contigs. Recall that each assembly is a set of walks in the assembly graph. A contig can appear in more than one walk or can be present in a single walk multiple times. Proposals of new assemblies are created from the current assembly using the following moves: Walk extension (Figure 1a) We start from one end of an existing walk and randomly walk through the graph, in every step uniformly choosing one of the edges outgoing from the current node. Each time we encounter the end of another walk, the two walks are considered for joining. We randomly (uniformly) decide whether we join the walks, end the current walk without joining, or continue walking. Local improvement (Figure 1b) We optimize the part of some walk connecting two long contigs s and t. We first sample multiple random walks starting from contig s. In each walk, we only consider nodes from which contig t is reachable. Then we evaluate these random walks and choose the one that increases the likelihood the most. If the gap between contigs s and t is too big, we instead use a greedy strategy where in each step we explore multiple random extensions of the walk of length around 200 bp and pick the one with the highest score. Repeat optimization We optimize the copy number of short tandem repeats. We do this by removing or adding a loop to some walk. We precompute the list of all short loops (up to five nodes) in the graph and use it for adding loops. Joining with advice We join two walks that are spanned by long reads or paired reads with long inserts. We first select a starting walk, align all reads to this walk and randomly choose a read which has the other end outside the walk. Then we find to which node this other end belongs to and join appropriate walks. If possible, we fill the gap between the two walks using the same procedure as in the local improvement move. Otherwise we introduce a gap filled with Ns. Disconnecting We remove a path through short contigs connecting two long contigs in the same walk, resulting in two shorter walks. Repeat interchange (Figure 1c) If a long contig has several incoming and outgoing walks, we optimize the pairing of incoming and outgoing edges. In particular, we evaluate all moves that exchange parts of two walks through this contig. If one of these changes improves the score, we accept it and repeat this step, until the score cannot be improved at this contig. At the beginning of each annealing step, the type of the move is chosen randomly; each type of move has its own probability. We also choose randomly the contig at which we attempt to apply the move. Note that some moves (e.g. local improvement) are very general, while other moves (e.g. joining with advice) are targeted at specific types of data. This does not contradict a general nature of our framework; it is possible to add new moves as new types of data emerge, leading to improvement when using specific datasets, while not affecting the performance when such data is unavailable. To obtain the assembly graph, we use Velvet with basic error correction and unambiguous concatenation of k-mers. These settings will produce very short contigs, but will also give a much lower error rate than a regular Velvet run. GAML with the default settings then uses each long contig as a separate walk in the starting assembly for the simulated annealing procedure. The assembly obtained by the simulated annealing procedure may contain walks with no evidence for a particular configuration of incoming and outgoing edges in the assembly graph. This happens for example if a repeat is longer than the span of the longest paired read. In this case, there would be several versions of the assembly with the same or very similar likelihood score. In the postprocessing step, we therefore apply the repeat interchange move at every possible location of the assembly. If the likelihood change resulting from such a move is negligible, we break the corresponding walks into shorter contigs to avoid assembly errors.";Assembly
SKESA;"SKESA is a DeBruijn graph-based de-novo assembler designed for assembling reads of microbial genomes sequenced using Illumina. Comparison with SPAdes and MegaHit shows that SKESA produces assemblies that have high sequence quality and contiguity, handles low-level contamination in reads, is fast, and produces an identical assembly for the same input when assembled multiple times with the same or different compute resources. SKESA has been used for assembling over 272,000 read sets in the Sequence Read Archive at NCBI and for real-time pathogen detection. Trimming of reads. Detection of parameters: A user should specify the option for whether the reads are paired or single and the compute resources available. All other parameters are determined internally by SKESA unless explicitly specified. Assembly using a specific k-mer size: In each iteration, the assembly process uses the DeBruijn graph for that k-mer size and an empty or current set of contigs. Multiple k-mer sizes are used. Short k-mers can assemble low-coverage areas of the genome while long k-mers can resolve repeats. Marking reads: This module decides reads that are used up and no longer needed for future iterations. After trimming of reads, the rest of the SKESA process uses trimmed reads only and we overload “read” to mean trimmed reads after this step. If input has paired reads, after iterating using k-mers up to mate length, any read still available for assembly has a mini-assembly performed treating its mates as ends of contigs. Assembled reads are used for generating three sets of k-mers that are longer than the mate size and up to the expected insert size. No explicit error correction of reads is done by SKESA as the heuristics of SKESA can handle the errors in a typical illumina read set. Next, we describe each of the five modules. Read trimming K-mer size of 19 is used for counting frequency of k-mers in the read set. If a k-mer is seen in at least Vf fraction of reads (default 0.05), it is considered suspect and used for trimming reads. Starting from the first k-mer in a mate and checking all consecutive k-mers, the first occurrence of a k-mer flagged as suspect trims the rest of the mate. Parameter detection SKESA builds a histogram for frequency of k-mers at the minimal k-mer length Kmin (default 21) seen in trimmed reads. Using the histogram, it decides the peak where the distribution around the peak likely corresponds to the k-mers from the genome being assembled. This distribution is used to estimate the genome size G. If no peak is detected, then 80% of the entire distribution is used as an estimate of G. Additional peaks present and distributions around those peaks are usually due to noise, repeats, or plasmids.";Assembly
Bracken;"Metagenomic experiments attempt to characterize microbial communities using high-throughput DNA sequencing. Identification of the microorganisms in a sample provides information about the genetic profile, population structure, and role of microorganisms within an environment. Until recently, most metagenomics studies focused on high-level characterization at the level of phyla, or alternatively sequenced the 16S ribosomal RNA gene that is present in bacterial species. As the cost of sequencing has fallen, though, metagenomics experiments have increasingly used unbiased shotgun sequencing to capture all the organisms in a sample. This approach requires a method for estimating abundance directly from the raw read data. Here we describe a fast, accurate new method that computes the abundance at the species level using the reads collected in a metagenomics experiment. Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN) uses the taxonomic assignments made by Kraken, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above. We demonstrate that Bracken can produce accurate species- and genus-level abundance estimates even when a sample contains multiple near-identical species. Our new method, Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN), estimates species abundances in metagenomics samples by probabilistically re-distributing reads in the taxonomic tree. Reads assigned to nodes above the species level are distributed down to the species nodes, while reads assigned at the strain level are re-distributed upward to their parent species. As we show below, Bracken can easily reestimate abundances at other taxonomic levels (e.g., genus or phylum) using the same algorithm. In order to re-assign reads classified at higher-level nodes in the taxonomy, we need to compute a probabilistic estimate of the number of reads that should be distributed to the species below that node. Reallocating reads from a genus-level node in the taxonomy to each genome below it can be accomplished using Bayes’ theorem, if the appropriate probabilities can be computed. To compute species abundance, any genome-level (strain-level) reads are simply added together at the species level. In cases where only one genome from a given species is detected by Kraken in the dataset, we simply add the reads distributed downward from the genus level (and above) to the reads already assigned by Kraken to the species level. In cases where multiple genomes exist for a given species, the reads distributed to each genome are combined and added to the Kraken-assigned species level reads. The added reads give the final species-level abundance estimates. This method can also estimate abundance for other taxonomic levels. In such cases, only higher nodes within the taxonomy tree undergo read distribution. After distributing reads downward, we estimate abundance for a node at the level specified by combining the distributed reads across all genomes within that node’s subtree.";AbundanceEstimation
mOTUs2;"Metagenomic sequencing has greatly improved our ability to profile the composition of environmental and host-associated microbial communities. However, the dependency of most methods on reference genomes, which are currently unavailable for a substantial fraction of microbial species, introduces estimation biases. We present an updated and functionally extended tool based on universal (i.e., reference-independent), phylogenetic marker gene (MG)-based operational taxonomic units (mOTUs) enabling the profiling of >7700 microbial species. As more than 30% of them could not previously be quantified at this taxonomic resolution, relative abundance estimates based on mOTUs are more accurate compared to other methods. As a new feature, we show that mOTUs, which are based on essential housekeeping genes, are demonstrably well-suited for quantification of basal transcriptional activity of community members. Furthermore, single nucleotide variation profiles estimated using mOTUs reflect those from whole genomes, which allows for comparing microbial strain populations (e.g., across different human body sites). The mOTU profiler version 2 (mOTUs2) is a stand-alone, open source, computational tool that estimates the relative abundance of known as well as genomically uncharacterized microbial community members at the species level using metagenomic shotgun sequencing data. The taxonomic profiling method is based on ten universally occurring, protein coding, single-copy phylogenetic marker genes (MGs), which were extracted from more than 25,000 reference genomes13 and more than 3100 metagenomic samples (Supplementary Data 1; in total ca. 367,000 non-redundant MG sequences). The MGs were grouped into >7700 MG-based operational taxonomic units (mOTUs) that represent microbial species, many of which (ca. 30%) still lack sequenced reference genomes. In addition to (i) taxonomic profiling, the tool allows for (ii) basal transcriptional activity profiling of community members using metatranscriptomic data as well as (iii) determining proxies for strain population genomic distances based on single-nucleotide variations (SNVs) within the phylogenetic marker genes that comprise mOTUs.
Generation and annotation of the mOTUs2 database The mOTUs2 profiler relies on a custom-built database of MG sequences extracted from reference genomes (ref-MGs) and from metagenomic samples (meta-MGs). The reference genomes were grouped into species-level clusters (specI clusters) and MG sequences from these reference genomes were grouped based on their specI affiliation into reference marker gene clusters (ref-MGCs). These ref-MGCs were augmented by meta-MGs and the remaining meta-MGs were clustered into meta-MGCs. MGCs of different MGs were subsequently grouped based on their specI affiliation or binned based on co-abundance analysis into reference genome-based mOTUs (ref-mOTUs) and metagenomic mOTUs (meta-mOTUs), respectively. The resulting mOTUs were quality-controlled, compiled into a sequence database for short-read mapping and taxonomically annotated.
To explore the phylogeny of mOTUs (ref-mOTUs and meta-mOTUs), a reference tree was reconstructed by combining the phylogenetic signal of the ten sets of marker genes selected (Supplementary Figure 4). For this, all marker genes were translated into amino acid sequences and analyzed using ETE Toolkit v3.1. 146. In particular, the program ete-build was used to run the following phylogenetic workflow: First, each set of marker proteins was independently aligned using ClustalOmega47. Next, alignment columns with less than three aligned residues were removed. Finally, the ten individual MG alignments were concatenated and used to infer a maximum likelihood phylogenetic tree using IQTree48 and the LG model. The mOTUs2 workflow for taxonomic profiling consists of three steps: alignment of metagenomic sequencing reads to MGs, estimation of read abundances for every marker gene cluster (MGC), and calculation of mOTU abundances. As input, mOTUs2 expects the user to provide quality controlled sequencing reads. These are aligned to the MGs of the mOTU database using BWA (mem algorithm, default parameters)42. The resulting alignments are filtered and only those with at least 97% nucleotide identity are retained. Further, alignments are filtered according to their lengths (default: 75 bp minimum alignment length; can be adjusted using the -l option).

Next, we compute the best alignment(s) for every insert (read pair) to the MGCs using BWA alignment scores. Inserts with a single highest scoring alignment are flagged as “unique alignments”, whereas inserts with multiple highest scoring alignments are flagged as “multiple alignments”. Subsequently, abundances for each MGC are calculated by summing up the number of all inserts flagged as unique alignments resulting in a unique alignment profile. Inserts flagged as multiple alignments are distributed among their best-scoring MGCs in accord with their respective abundances estimated based on the unique alignment profile. Thus, the final abundances are calculated as the sum of the unique abundance profiles and the distributed contributions of the inserts flagged as multiple alignments. In addition to these MGC insert counts, MGC base coverages are calculated by first summing up the total number of bases aligning to each MGC and then dividing by the respective gene lengths. Finally, the abundances of the mOTUs are calculated as the median of their respective MGC abundances (insert counts and base coverages). In order to reduce false positive results, we require a certain number of MGCs to be detected, that is to have metagenomic reads mapped to them (default: 3 MGs, -g option in mOTUs2). Although mOTUs2 is able to profile many organisms not yet represented by reference genomes, there are still around 25% of the MGCs that could not be binned into mOTUs (see section 2.5). Reads mapping to those MGCs are assigned to a group labelled as “unbinned” (shown as “-1” in mOTU abundance profiles). The abundance of this group is calculated as the median of unbinned MGCs summed by COG.";AbundanceEstimation
DiTASiC;"Current metagenomics approaches allow analyzing the composition of microbial communities at high resolution. Important changes to the composition are known to even occur on strain level and to go hand in hand with changes in disease or ecological state. However, specific challenges arise for strain level analysis due to highly similar genome sequences present. Only a limited number of tools approach taxa abundance estimation beyond species level and there is a strong need for dedicated tools for strain resolution and differential abundance testing. We present DiTASiC (Differential Taxa Abundance including Similarity Correction) as a novel approach for quantification and differential assessment of individual taxa in metagenomics samples. We introduce a generalized linear model for the resolution of shared read counts which cause a significant bias on strain level. Further, we capture abundance estimation uncertainties, which play a crucial role in differential abundance analysis. A novel statistical framework is built, which integrates the abundance variance and infers abundance distributions for differential testing sensitive to strain level. As a result, we obtain highly accurate abundance estimates down to sub-strain level and enable fine-grained resolution of strain clusters. We demonstrate the relevance of read ambiguity resolution and integration of abundance uncertainties for differential analysis. Accurate detections of even small changes are achieved and false-positives are significantly reduced. Superior performance is shown on latest benchmark sets of various complexities and in comparison to existing methods. DiTASiC is designed as a comprehensive approach for abundance estimation and differential abundance assessment of individual taxa. Thereby, the main focus is on distinguishing on the strain level with highly similar sequences and its corresponding challenges. The steps of the DiTASiC workflow are illustrated in Figure 1, it consists of three main parts: mapping, abundance estimation and differential abundance assessment. In the first two parts we built on some of the core ideas of our previously published tool GASiC (Lindner and Renard, 2013), while strongly improving on abundance quantification and introducing new methodology to address the critical aspects of variance of abundance estimates and differential abundance. In a metagenomics sample measured by NGS technologies we face millions to billions of reads which are derived from diverse taxa. DiTASiC relies on a pre-filtering of species by fast profiling tools such as Kraken (Wood and Salzberg, 2014), CLARK (Ounit et al., 2015), Kaiju (Menzel et al., 2016), or by using Mash (Ondov et al., 2016), a genome distance calculator, to reduce the number of potential reference genomes and keep the main focus on species expected in the data. Here, we specifically aim at revealing the picture on the highest available strain levels. In the first mapping step, all reads are assigned to the given references as a first attempt to decipher their potential origin. The number of hits per reference genome is counted. We refer to it as mapping abundance of a taxon. In the next step of abundance estimation, a new generalized linear model (GLM) is introduced for the resolution of shared read counts, which are crucial on strain level. As a result, more accurate abundance estimates are obtained for the different strains along with standard errors for abundance uncertainty. In the last section, the focus is on the comparison of whole metagenomics samples and the assessment of differential abundance of taxa. Thereby, we concentrate on a method to integrate the variance of abundance estimates. Abundances are transformed into distributions, divergence of distributions is used to infer differential events and corresponding P-values are calculated.";AbundanceEstimation
FastViromeExplorer;"With the increase in the availability of metagenomic data generated by next generation sequencing, there is an urgent need for fast and accurate tools for identifying viruses in host-associated and environmental samples. In this paper, we developed a stand-alone pipeline called FastViromeExplorer for the detection and abundance quantification of viruses and phages in large metagenomic datasets by performing rapid searches of virus and phage sequence databases. Both simulated and real data from human microbiome and ocean environmental samples are used to validate FastViromeExplorer as a reliable tool to quickly and accurately identify viruses and their abundances in large datasets. FastViromeExplorer, written in Java, has two main steps: (1) the read mapping step where all reads are mapped to a reference database, and (2) the filtering step where the mapping results are subjected to three major filters (detailed later) for output of the final results on virus types and abundances. The input of the read alignment step is raw reads (single-end or paired-end) in fastq format. FastViromeExplorer uses the reference database downloaded from NCBI containing 8,957 RefSeq viral genomes as default but can also use any updated or customized databases as reference. FastViromeExplorer incorporates the reference database as an input parameter, so that user can use any database of his choice as input. First, FastViromeExplorer calls kallisto (Bray et al., 2016) as a subprocess to map the input reads against the reference database. Kallisto was developed to map RNA-seq data to a reference transcriptome (all the transcripts for a genome) leveraging the pseudoalignment process and estimate the abundance of the transcripts using the Expectation-Maximization (EM) algorithm (Dempster, Laird & Rubin, 1977). As there is no actual sequence alignment of the entire read over the reference sequences, the pseudoalignment process enables read mapping to be both lightweight and superfast. Essentially, kallisto searches for exact matches for a short k-mer (default size 31 bp) between the metagenomic reads and the sequences in the virus/phage database. For example, kallisto was able to map and quantify 30 million paired-end RNA-seq reads from a human transcriptome sample in less than 10 min on a small laptop computer with a 1.3-GHz processor (Bray et al., 2016). In addition to the ultrafast speed, kallisto also gives accurate estimation of abundance of each transcript or reference sequence (Schaeffer et al., 2017; Soneson et al., 2016). Consequently, kallisto could provide an ideal tool for detection and quantification of viruses in metagenomic samples that commonly have tens of millions of reads, mapping of which using commonly used programs such as BLAST can be time-consuming and often infeasible without computer clusters. Therefore, FastViromeExplorer deploys kallisto for the purpose of read mapping and abundance estimation of the viruses. Since kallisto searches for exact matches for a short k-mer (default size 31 bp) between the metagenomic reads and the sequences in the virus/phage database, if a 31 bp match is found then the virus is detected. If multiple hits occur, then kallisto uses an EM algorithm to help resolve the redundancy and quantify the abundances of the detected viruses. The k-mer size in kallisto can be altered depending on user’s need. For example, if the sample is expected to contain viral sequences that are divergent from those in the reference database the k-mer size can be reduced to improve detection sensitivity. After the first alignment step, FastViromeExplorer takes the output of kallisto that includes information of the aligned reads together with estimated abundances or estimated read counts of all the identified viruses for the processing of the second step. The second step filters the output of the first step using three criteria, introduced to ensure the quality of virus detection and especially to reduce the number of false positive viruses from the result. In detail, the first criterion, hereafter referred to as “R”, is based on the ratio of the observed extent of genome coverage with the expected extent of genome coverage. FastViromeExplorer was run on both simulated and real data to examine its running time and accuracy. FastViromeExplorer used kallisto (version 0.43.1) with default settings and generated pseudoalignment results in sam format and filtered abundance results in a tab-delimited file. The abundance results contain identified virus names, NCBI accession numbers, NCBI taxonomic path, and estimated read counts. FastViromeExplorer was run on two different reference databases, the default database distributed together with FastViromeExplorer, that is, the NCBI RefSeq database containing 8,957 genomes of eukaryotic viruses and phages, and the set of sequences collected from the JGI “earth virome” study (Paez-Espino et al., 2016) containing 125,842 metagenomic viral contigs (mVCs). The taxonomic annotation and host information for these mVCs were collected from the IMG/VR database";AbundanceEstimation
Kallisto;"We present kallisto, an RNA-seq quantification program that is two orders of magnitude faster than previous approaches and achieves similar accuracy. Kallisto pseudoaligns reads to a reference, producing a list of transcripts that are compatible with each read while avoiding alignment of individual bases. We use kallisto to analyze 30 million unaligned paired-end RNA-seq reads in <10 min on a standard laptop computer. This removes a major computational bottleneck in RNA-seq analysis. The construction of the index starts with the formation of a colored de Bruijn graph15 from the transcriptome, where the colors correspond to transcripts. In the colored transcriptome de Bruijn graph, each node corresponds to a k-mer and every k-mer receives a color for each transcript it occurs in. Contigs are defined to be linear stretches of the de Bruijn graph that have identical colorings. This ensures that all k-mers in a contig are associated with the same equivalence class (the converse is not true: two different contigs can be associated with the same equivalence class). Once the graph and contigs have been constructed, kallisto stores a hash table mapping each k-mer to the contig it is contained in, along with the position within the contig. This structure is called the kallisto index.For error-free reads, there can be a difference between the equivalence class of a read and the intersection of its k-compatibility classes. But for a read of length l this can only happen if there are two transcripts that have the same l – k + 1 k-mers occurring in different order. This is unlikely to happen for large k because it would imply that the T-DBG has a directed cycle shorter than l – k + 1. This fact also provides a criterion that can be tested. Pseudoalignment. Reads are pseudoaligned by looking up the k-compatibility class for each k-mer in the read in the kallisto index and then intersecting the identified k-compatibility classes. In the case of paired-end reads, the k-compatibility class lookup is done for both ends of the fragment and all the resulting classes are intersected. Since the T-DBG identifies each k-mer with its reverse complement, the k-mer hashing in kallisto is strand-agnostic; however, the implementation could also be adapted to require specific strandedness of reads from strand-specific protocols. To further speed up the processing, kallisto uses the structural information stored in the index: because all k-mers in a contig of the T-DBG have the same k-compatibility class, it would be redundant to include more than one k-mer from a contig in the intersection of k-compatibility classes. This observation is leveraged in kallisto by finding the distances to the junctions at the end of its contig each time a k-mer is looked up using the hash. If the read does arise from a transcript in the T-DBG, the k-mers up to those distances can be skipped without affecting the result of the intersection, resulting in fewer hash lookups. To help ensure that the read is consistent with the T-DBG, kallisto checks the last k-mer that is skipped to ensure its k-compatibility class is equal as expected. In rare case when there is a mismatch, kallisto defaults to examining each k-mer of the read. For the majority of reads, kallisto ends up performing a hash lookup for only two k-mers (Supplementary Fig. 11). While pseudoalignment does not require or make use of the locations of k-mers in transcripts, it is possible to extract such data from the T-DBG, and a “pseudobam output” option of kallisto takes advantage of this to produce an alignment file containing positions of reads within transcripts. With pseudobam it is possible to examine the location of reads within transcripts and genes of interest for quality control and analysis purposes. Quantification. In order to rapidly quantify transcript abundances from pseudoalignments, kallisto makes use of the following form of the likelihood function for RNA-seq: In equation (1), F is the set of fragments, T is the set of transcripts, lt is the (effective) length3 of transcript t and yf,t is a compatibility matrix defined as 1 if fragment f is compatible with t and 0 otherwise. The parameters are the αt, the probabilities of selecting fragments from transcripts. The likelihood can be rewritten as a product over equivalence classes, in which similar summation terms have been factored together. In the factorization the numbers ce are the number of counts observed from equivalence class e. When equation (1) is written in terms of the equivalence classes, the equivalence class counts are sufficient statistics and thus, in the computations, are based on a much smaller set of data (usually hundreds of thousands of equivalence classes instead of tens of millions of reads). The likelihood function is iteratively optimized with the EM algorithm, with iterations terminating when, for every transcript t, αtN > 0.01 (N is the total number of fragments) changes less than 1% from iteration to iteration.

The transcript abundances are output by kallisto in transcripts per million9 (TPM) units.";AbundanceEstimation
GASiC;"One goal of sequencing-based metagenomic community analysis is the quantitative taxonomic assessment of microbial community compositions. In particular, relative quantification of taxons is of high relevance for metagenomic diagnostics or microbial community comparison. However, the majority of existing approaches quantify at low resolution (e.g. at phylum level), rely on the existence of special genes (e.g. 16S), or have severe problems discerning species with highly similar genome sequences. Yet, problems as metagenomic diagnostics require accurate quantification on species level. We developed Genome Abundance Similarity Correction (GASiC), a method to estimate true genome abundances via read alignment by considering reference genome similarities in a non-negative LASSO approach. We demonstrate GASiC’s superior performance over existing methods on simulated benchmark data as well as on real data. In addition, we present applications to datasets of both bacterial DNA and viral RNA source. We further discuss our approach as an alternative to PCR-based DNA quantification. As in most reference-based methods, the reads are first aligned to every genome in a set of references and the number of reads matching to each genome is counted. We call these counts the ‘observed abundances’, as opposed to the ‘abundance estimates’ which we want to obtain in the end. In the next step, GASiC constructs a similarity matrix encoding the alignment similarities between the reference sequences. The similarity matrix and the observed abundances are then used together in a linear system of equations, where GASiC solves for the corrected abundances using a constrained optimization routine to obtain the estimates. The whole procedure can be iterated using bootstrap (9) samples from the original dataset. This yields more stable abundance estimates and provides an intuitive non-parametric statistical test for the presence of a species.  Alignment The reads in D are aligned to all species S with an alignment method suitable for the characteristics of D. Then, we count the number of reads forumla from D that were successfully aligned to forumla irrespective of the number of matching positions in forumla or matches to other species. In particular, we neither restrict ourselves to unique matches only, nor assume any phylogenetic structure within the forumla, as is done for example in MEGAN. If the dataset only contains very dissimilar species, the read counts forumla may already be suitable estimates for the true abundances. Otherwise, the forumla are in general highly disturbed and dominated by shared matches, such that the forumla cannot directly be used as abundance estimates. Similarity estimation A proper similarity estimation of the reference sequences is required to achieve accurate similarity correction of the forumla. The similarities between sequences are encoded in a similarity matrix forumla, where forumla denotes the probability that a read drawn from forumla can be aligned to forumla. In practice, we simulate a set of reads from every reference forumla with a read simulator which is able to imitate the sequencing technology and error characteristics of D. For example, Mason (10) and Grinder (11) simulate Illumina, 454 and Sanger reads; and dwgsim (sourceforge.net/projects/dnaa/) simulates Illumina, ABI SOLiD and IonTorrent reads. Then, we align the simulated reads of forumla to forumla using the very same settings as for aligning the reads in dataset D and count the number of matching reads forumla The matrix entries are then estimated as forumla The key element of similarity estimation is a proper read simulation since we use the simulated reads to estimate the reference genome similarities, the source of ambiguous alignments. Thus, the simulated reads should have the read characteristics and the error characteristics of the instrument (read length, paired/single end, etc.) and should cover the reference genome at least once. For very complex metagenomic communities with a high number of species M, the calculation of the complete similarity matrix may become infeasible because of its computational complexity forumla We recommend to first estimate similarities using, for example, fast k-mer-based methods (12) and refine the estimates via the simulation approach only for genomes with sufficiently high (e.g. forumla) similarity. Similarity correction We introduce a linear model to correct the forumla for the genome similarity using the similarity matrix A. Let forumla denote the true, but unknown, abundance of species forumla. We then assume that the observed abundance forumla is a mixture of the true abundances forumla of all species forumla weighted with the estimated probability forumla that a read from j can be aligned to i: formula To simplify notation, we use a matrix representation of the true and the observed abundances, i.e. forumla and forumla In matrix notation, this can be written as formula Since direct inversion of the matrix A may result in instable abundance estimates, we formulate the solution for c as a non-negative LASSO (13,14) problem: formula formula The constraints enforce the result to be meaningful, i.e. each estimated relative abundance forumla must be equal to or greater than zero and the sum of all relative abundances must be less than or equal to one. The first conditions also ensure that the correction produces abundances lower than or equal to the measured abundances. The last condition allows the presence of reads from a totally unrelated species, since the abundances are allowed to sum up to less than or equal to one. It also enforces the sparsity of results such that only meaningful contributions have abundances larger than 0. We solve the constraint optimization problem with the COBYLA method implemented in SciPy";AbundanceEstimation
Salmon;"Transcript quantification is a central task in the analysis of RNA-seq data. Accurate computational methods for the quantification of transcript abundances are essential for downstream analysis. However, most existing approaches are much slower than is necessary for their degree of accuracy. We introduce Salmon, a novel method and software tool for transcript quantification that exhibits state-of-the-art accuracy while being significantly faster than most other tools. Salmon achieves this through the combined application of a two-phase inference procedure, a reduced data representation, and a novel lightweight read alignment algorithm. Salmon consists of three components: a lightweight-alignment model, an online phase that estimates initial expression levels and model parameters and constructs equivalence classes over the input fragments, and an offline phase that refines the expression estimates. The online and offline phases together optimize the estimates of the latent parameters α, and each method can compute η directly from these parameters. The online phase uses a variant of stochastic, collapsed variational Bayesian inference [7]. The offline phase applies the variational Bayesian EM algorithm [12] over a reduced representation of the data represented by the equivalence classes until a data-dependent convergence criterion is satisfied.";AbundanceEstimation
Sailfish;"We introduce Sailfish, a computational method for quantifying the abundance of previously annotated RNA isoforms from RNA-seq data. Because Sailfish entirely avoids mapping reads, a time-consuming step in all current methods, it provides quantification estimates much faster than do existing approaches (typically 20 times faster) without loss of accuracy. By facilitating frequent reanalysis of data and reducing the need to optimize parameters, Sailfish exemplifies the potential of lightweight algorithms for efficiently processing sequencing reads. Indexing. The first step in the Sailfish pipeline is building an index from the set of reference transcripts T. Given a k-mer length k, we compute an index Ik(T) containing four components. The first component is a minimum perfect hash function h on the set of k-mers kmers(T) contained in T. A minimum perfect hash function is a bijection between kmers(T) and the set of integers {0,1,..., |kmers(T)| – 1}. Sailfish uses the BDZ minimum perfect hash function9. The second component of the index is an array C containing a count C(si) for every si∈ kmers(T). Finally, the index contains a lookup table mapping each transcript to the multiset of k-mers that it contains, and a reverse lookup table mapping each k-mer to the set of transcripts in which it appears. The index is a product only of the reference transcripts and the choice of k, and thus needs only to be recomputed when either of these change. Quantification. The second step in the Sailfish pipeline is the quantification of relative transcript abundance; this requires the Sailfish index Ik(T) for the reference transcripts T as well as a set of RNA-seq reads . First, we count the number of occurrences of each si ∈ kmers(T) ∩ kmers(). Because we know exactly the set of k-mers that need to be counted and already have a perfect hash function h for this set, we can perform this counting in a particularly efficient manner, even faster than efficient hash-based approaches. For example, performing concurrent 20-mer lookups using 8 threads, Jellyfish10 requires an average of 0.35 μs/key while the minimal perfect hash requires an average of 0.1 μs/key. We maintain an array  of the appropriate size |kmers(T)|, where  contains the number of times we have thus far observed si in . In an unstranded protocol, sequencing reads, and hence the k-mers they contain, may originate from transcripts in either the forward or reverse direction. To account for both possibilities, we check both the forward and reverse-complement k-mers from each read and use a majority-rule heuristic to determine which of the k-mers to increment in the final array of counts . If the number of k-mers appearing in h from the forward direction of the read is greater than the number of reverse-complement k-mers, then we only increment the counts for k-mers appearing in this read in the forward direction. Otherwise, only counts for k-mers appearing in the reverse-complement of this read are incremented in the array of counts. Ties are broken in favor of the forward-directed reads. In a stranded RNA-seq protocol, the reads have a known orientation. Reads provided to Sailfish can be specified as originating from unstranded, forward-strand or reverse-strand reads, and the appropriate k-mers are counted in each case. By taking advantage of atomic integers and the compare-and-swap (CAS) operation provided by modern processors, which allows many hardware threads to efficiently update the value of a memory location without the need for explicit locking, we can stream through and update the counts in  in parallel while sustaining very little resource contention. We then apply an expectation-maximization (EM) algorithm to obtain estimates of the relative abundance of each transcript. We define a k-mer equivalence class as the set of all k-mers that appear in the same set of transcripts with the same frequency. In other words, let ×(s) be a vector such that entry t of ×(s) gives how many times k-mer s appears in transcript t∈ T. Then the equivalence class of a k-mer si is given by [si] = {sj∈ kmers(T) : ×(sj) = ×(si)}. When performing the EM procedure, we will allocate counts to transcripts according to the set of equivalence classes rather than the full set of k-mers. We will let denote the total count of k-mers in  that originate from equivalence class [si]. We say that transcript t contains equivalence class [s] if [s] is a subset of the multiset of k-mers of t and denote this by [s] ⊆ t. Estimating abundances via an EM algorithm. The EM algorithm (Algo. 1) alternates between estimating the fraction of counts of each observed k-mer that originates from each transcript (E-step) and estimating the relative abundances of all transcripts given this allocation (M-step). We initially allocate k-mers to transcripts proportional to their occurrences in the transcript (i.e., if a transcript is the only potential origin for a particular k-mer, then all observations of that k-mer are attributed to this transcript, whereas for a k-mer that appears once in each of n different transcripts and occurs m times in the set of reads, m/n observations are attributed to each potential transcript of origin). The E-step of the EM algorithm computes the fraction of each k-mer equivalence class's total count that is allocated to each transcript. For equivalence class [sj] and transcript ti, this value is computed by where μ′i is the currently estimated relative abundance of transcript i. These allocations are then used in the M-step of the algorithm to compute the relative abundance of each transcript. The relative abundance of transcript i is estimated by where μi is The variable li′ denotes the adjusted length of transcript i and is simply li′ = li − k + 1, where li is the length of transcript i in nucleotides. However, rather than perform the standard EM update steps, we perform updates according to the SQUAREM procedure11 described in Algo. 2, where μ′ = μ′0, ...,μ′|T| is a vector of relative abundance maximum-likelihood estimates, and EM(•) is a standard iteration of the expectation-maximization procedure as outlined in Algo. 1. For a detailed explanation of the SQUAREM procedure and its proof of convergence, see ref. 11. Intuitively, the SQUAREM procedure builds an approximation of the Jacobian of μ′ from three successive steps along the EM solution path, and uses the magnitude of the differences between these solutions to determine a step size by which to update the estimates according to the update rule (line 7). The procedure is then capable of making relatively large updates to the μ′ parameters, which substantially improves the speed of convergence. In Sailfish, the iterative SQUAREM procedure is repeated until a specified convergence criterion is met (by default, the procedure terminates when no transcript with a relative abundance greater than 10−7 has a relative change greater than half of a percent between consecutive iterations). Additionally, a user may specify either a fixed number of iterations to perform or a required minimum relative change, ɛ, in transcript abundance estimates between SQUAREM iterations; if no relative change in transcript abundance exceeds ɛ, then the procedure is considered to have converged and the estimation procedure terminates.";AbundanceEstimation
ROCker;"Functional annotation of metagenomic and metatranscriptomic data sets relies on similarity searches based on e-value thresholds resulting in an unknown number of false positive and negative matches. To overcome these limitations, we introduce ROCker, aimed at identifying position-specific, most-discriminant thresholds in sliding windows along the sequence of a target protein, accounting for non-discriminative domains shared by unrelated proteins. ROCker employs the receiver operating characteristic (ROC) curve to minimize false discovery rate (FDR) and calculate the best thresholds based on how simulated shotgun metagenomic reads of known composition map onto well-curated reference protein sequences and thus, differs from HMM profiles and related methods. We showcase ROCker using ammonia monooxygenase (amoA) and nitrous oxide reductase (nosZ) genes, mediating oxidation of ammonia and the reduction of the potent greenhouse gas, N2O, to inert N2, respectively. ROCker typically showed 60-fold lower FDR when compared to the common practice of using fixed e-values. Previously uncounted ‘atypical’ nosZ genes were found to be two times more abundant, on average, than their typical counterparts in most soil metagenomes and the abundance of bacterial amoA was quantified against the highly-related particulate methane monooxygenase (pmoA). Therefore, ROCker can reliably detect and quantify target genes in short-read metagenomes. ROCker is implemented in the Ruby programming language and its workflow consists of five tasks. (i) Build: Reads a user-provided list of UniProt (Universal Protein Resource) protein identifiers and downloads the corresponding whole genome sequences encoding these proteins for generating data sets that simulate shotgun, short-read, Illumina metagenomes using GRINDER (5). A second list of known negative references, i.e. closely related proteins that should not be considered as true matches can also be given at this step in order to increase the performance of ROCker (see amoA example below). The training reference sequences are downloaded and annotated using the European Bioinformatics Institute REST API (6) and aligned using ClustalΩ (7). Subsequently, ROCker queries the reference protein sequences provided against the simulated shotgun data sets using BLASTx (8) or DIAMOND (9). (ii)Compile: Translates search results to alignment columns, and identifies the most discriminant bitscore per alignment in a 20 amino acid window (or another, user-defined length) in a set of sequences using pROC (10). The latter algorithm calculates sensitivity and specificity using the number of true and false positive matches in each window. The bitscore thresholds are calculated as the value in the ROC curve that maximizes the distance to the identity line (i.e. the non-discriminatory diagonal line in the ROC curve) according to the Youden method. Windows are iteratively refined to reduce low-accuracy regions (<95% estimated accuracy), for all windows with sufficient data (≥5 amino acid positions and ≥3 true positives available). Thresholds in regions with insufficient data are inferred by linear interpolation of surrounding windows. (iii)Filter: Uses the calculated set of bitscore thresholds (as estimated by the compile task) to filter the result of a preexisting search. (iv)Search: Executes a search of metagenomic sequences against target protein sequences (i.e. single protein function) using BLASTx or DIAMOND, and filters the output according to the most-discriminating bitscores calculated in the Compile step. (v)Plot: Generates a graphical representation of the alignment, the thresholds and the matches obtained, together with summary statistics";AbundanceEstimation
GAAS;"Metagenomic studies characterize both the composition and diversity of uncultured viral and microbial communities. BLAST-based comparisons have typically been used for such analyses; however, sampling biases, high percentages of unknown sequences, and the use of arbitrary thresholds to find significant similarities can decrease the accuracy and validity of estimates. Here, we present Genome relative Abundance and Average Size (GAAS), a complete software package that provides improved estimates of community composition and average genome length for metagenomes in both textual and graphical formats. GAAS implements a novel methodology to control for sampling bias via length normalization, to adjust for multiple BLAST similarities by similarity weighting, and to select significant similarities using relative alignment lengths. In benchmark tests, the GAAS method was robust to both high percentages of unknown sequences and to variations in metagenomic sequence read lengths. Re-analysis of the Sargasso Sea virome using GAAS indicated that standard methodologies for metagenomic analysis may dramatically underestimate the abundance and importance of organisms with small genomes in environmental systems. Using GAAS, we conducted a meta-analysis of microbial and viral average genome lengths in over 150 metagenomes from four biomes to determine whether genome lengths vary consistently between and within biomes, and between microbial and viral communities from the same environment. Significant differences between biomes and within aquatic sub-biomes (oceans, hypersaline systems, freshwater, and microbialites) suggested that average genome length is a fundamental property of environments driven by factors at the sub-biome level. The behavior of paired viral and microbial metagenomes from the same environment indicated that microbial and viral average genome sizes are independent of each other, but indicative of community responses to stressors and environmental conditions. Similarity filtering. BLAST analyses (NCBI BLAST 2.2.1) were conducted through GAAS in order to determine significant similarities between metagenomic sequences and completely sequenced genomes. Similarities were filtered based on a combination of maximum E-value, minimum similarity percentage and minimum relative alignment length. E-value filtering removed non-significant similarities, and the alignment similarity percentage and relative length were used to select for strong similarities likely to reflect the taxonomy of the metagenomic sequences. E-values depend on the size of the database and the absolute length of alignments between query and target sequences, and thus may not be comparable between analyses [32],[33]. Relative alignment length, also called alignment coverage [34], is the ratio of the length of the alignment to the length of the query sequence (Figure S7). It is independent of the database size and sequence length, and provides an intuitive and consistent threshold to select significant similarities. Since the ends of sequenced reads can be of lower quality, similarities were kept only if the length of the alignment represented the majority of the length of the query sequence. Sequences with no similarity satisfying the filtering criteria were ignored in the rest of the analysis. Similarity weighting. In order to avoid the loss of relevant similarities by reliance upon smallest E-values alone [5], all significant similarities for each query sequence (as defined by our criteria above) were kept and assigned weights as follows. Based on the Karlin-Altschul equation, the expect value Eij between a metagenomic query sequence i and a target genome sequence j is given by:  where m'i is the effective query sequence length, n' is the effective database size (in number of residues) and S'ij is the high-scoring pair (HSP) bitscore [32]. Using the effective length corrects for the “edge effect” of local alignment and is significant for sequences smaller than 200 bp such as sequences produced by the high throughput Roche-454 GS20 platform. Assuming that a query sequence is more likely to have local similarities to longer target genomes, each of the E-values can be reformulated into an expect value Fij of a similarity in a given target genome by:  where t'j is the effective length [35] of the target genome j. Using the length of the target genome in the F-value produces an expect value relative to the target genome, not to the totality of the genome database (as is the case of the E-value). From Fij, a weight wij can be calculated as  with zi being a constant such that for a given metagenomic query sequence i, . This weight carries the statistical meaning of the expect value of the similarity relative to the given genome in such a way that the larger the expect value, the lower the weight. Therefore, for a given query sequence i, the weight was calculated as . Genome relative abundance using genome length normalization. The relative abundance of sequences in a random shotgun library is proportional not only to the relative abundance of the genomes in the library but also to their length. Similarly to the normalization used in proteomics [36]–[38], normalization by genome length is needed to obtain correct relative abundance of the species in a metagenome. For each target genome j, the weights wij to that genome were added to obtain Wj. The weighted similarities Wj to each genome were then normalized by the actual length tj of the genome (including chromosomes, organelles, plasmids and other replicons) to obtain accurate relative abundance estimates:";AbundanceEstimation
GRAMMy;"Accurate estimation of microbial community composition based on metagenomic sequencing data is fundamental for subsequent metagenomics analysis. Prevalent estimation methods are mainly based on directly summarizing alignment results or its variants; often result in biased and/or unstable estimates. We have developed a unified probabilistic framework (named GRAMMy) by explicitly modeling read assignment ambiguities, genome size biases and read distributions along the genomes. Maximum likelihood method is employed to compute Genome Relative Abundance of microbial communities using the Mixture Model theory (GRAMMy). GRAMMy has been demonstrated to give estimates that are accurate and robust across both simulated and real read benchmark datasets. We applied GRAMMy to a collection of 34 metagenomic read sets from four metagenomics projects and identified 99 frequent species (minimally 0.5% abundant in at least 50% of the data- sets) in the human gut samples. Our results show substantial improvements over previous studies, such as adjusting the over-estimated abundance for Bacteroides species for human gut samples, by providing a new reference-based strategy for metagenomic sample comparisons. GRAMMy can be used flexibly with many read assignment tools (mapping, alignment or composition-based) even with low-sensitivity mapping results from huge short-read datasets. It will be increasingly useful as an accurate and robust tool for abundance estimation with the growing size of read sets and the expanding database of reference genomes. We developed a finite mixture model for the GRAMMy framework. Following Angly et al. we used genome relative abundance (GRA) as the relative abundance measure of mostly unicellular microbial organisms [15]. We describe the sampling and sequencing procedure as follows: First, randomly choose a genome  with probability  proportional to , where  is the abundance and  is the genome length. Second, randomly generate a read  from it. Without loss of generality, we further assume that for the given genome  we can reasonably approximate the generation of shotgun reads by some component distribution  such that the probability of generating a read  from  is . With a reasonable assumption of independence between the two sampling steps, the whole procedure is probabilistically equivalent to sampling from a mixture distribution , with the mixing parameters denoted by ,  and the component distributions denoted by , where m is the number of genomes. Subsequently, each read set, denoted by , can be regarded as a realized independent, identically distributed (iid) sample of size  from the mixture . The relative abundance of known genomes is exactly a transformation of the mixing parameters , which can be estimated based on the read set . A schematic view of the finite mixture model is shown in Figure 1. With the component distributions properly set up, we can find the maximum likelihood estimate (MLE) of the mixing parameters. In many studies, our knowledge of the genomes present in the community is limited. Under these circumstances, we can define the mixture with the first  components for known genomes and the last -th component for the collective of unknown genomes. Note that for the  known components, we suppose that their genome sequences  and genome sizes  are known. Therefore, the GRA for known genomes  is the normalized abundance, where the relative abundance for the -th known genome is , where . In the biological setting, we want to estimate vector , which is a measure of organism relative abundance. In the transformed mixture problem,  is related to the mixing parameters  by: (1) or the inverse: (2) for . The number of sampled reads is both proportional to the genome relative abundance and the length. Because the two factors are confounded, the missing knowledge of the genome length  prohibits the estimation of  from the data. Since the effective genome length  for the unknown genomes is not available, we cannot estimate the relative abundance of the unknown component. However, the relative abundance of known genomes can still be estimated using our procedures. Estimation of GRA using Expectation Maximization (EM) algorithm To estimate the mixing parameters, we adopted the EM algorithm to calculate the maximum likelihood estimate (MLE). In the EM framework, we assume a ‘missing’ data matrix , in which each entry  is a random variable indicating whether  is from  or not. Then we can solve for the parameters by iteratively estimating  and  using Algorithm 1 (see supporting Methods). Note that a variable with superscript  stands for its value at the -th iteration, e.g.,  is the estimate of  at the -th step. The EM at the -th iteration is: E-step Assuming  known,  can be updated by the corresponding posterior probabilities: (3) M-step Assuming  known, the new mixing parameters  are updated by: (4) When the MLE of  is found, using Equation (1), the MLE of  can be calculated, thereby solving the original problem. The space complexity of the EM algorithm is  and the time complexity of the EM algorithm is , where  the average number of associated genomes for one read and  is the time cost related to the convergence criteria for EM. Since  and  are both constants not related to n, the algorithm is linear in space and time complexity with the read number . Further, the concavity of the log-likelihood function can be shown and the EM algorithm is guaranteed to converge to global maximum. Read probability approximations The probability  is assessed based on . Ideally, it is the probability that  is generated when read being uniformly sampled from genome . Let  be the number of copies of read  in . Then the probability is approximated by: (5) However, due to sequencing errors and natural genetic variations, the 's are not readily observable. When the mapping or alignment results from BLAST, BLAT, or other mapping tools are available, the number of high quality hits of  on  can effectively be used as 's. To keep only these reliable and statistically significant hits, raw hits are filtered by E-value, alignment length and identity rate. We refer to the finite mixture model with the read probability from mapping and alignment results as ‘map’ in the remainder of the paper. An alternative way to assess the read probabilities is by using k-mer composition. For the j-th genome, we calculate the fraction of a k-word w by , the normalized frequency of the word  in genome . For a read , we define pseudo-likelihood for  by: (6) where  is the set of words formed by sliding windows of size k along . This probabilistic assignment captures the overall similarity between reads and genomes, an idea adopted in other composition-based studies such as in Sandberg et al. [35]. It is especially useful when a large number of reads do not have reliable hits with reference genomes. We will refer to the finite mixture model with the read probability from the multinomial k-mer composition as ‘k-mer’ in the remainder of the paper.";AbundanceEstimation
imGLAD;"Accurate detection of target microbial species in metagenomic datasets from environmental samples remains limited because the limit of detection of current methods is typically inaccessible and the frequency of false-positives, resulting from inadequate identification of regions of the genome that are either too highly conserved to be diagnostic (e.g., rRNA genes) or prone to frequent horizontal genetic exchange (e.g., mobile elements) remains unknown. To overcome these limitations, we introduce imGLAD, which aims to detect (target) genomic sequences in metagenomic datasets. imGLAD achieves high accuracy because it uses the sequence-discrete population concept for discriminating between metagenomic reads originating from the target organism compared to reads from co-occurring close relatives, masks regions of the genome that are not informative using the MyTaxa engine, and models both the sequencing breadth and depth to determine relative abundance and limit of detection. We validated imGLAD by analyzing metagenomic datasets derived from spinach leaves inoculated with the enteric pathogen Escherichia coli O157:H7 and showed that its limit of detection can be comparable to that of PCR-based approaches for these samples. imGLAD assumes that reads of a metagenomic dataset originate at random from all regions of the genome. Thus, the fraction of the genome that is recovered in the dataset (sequencing breadth) as well as the number of times each region is sequenced (sequencing depth), both depend on the abundance of the organism in the community. Highly conserved regions (e.g., rRNA and tRNA genes), as well as regions resulting from recent horizontal gene transfer (e.g., transposase and integrase genes), can recruit reads from other non-target genomes and misleadingly increase the value of sequencing depth (and hence, estimated relative abundance) in some datasets depending on the gene composition of the organisms present. To address this problem, we developed a framework to identify which fraction of a target genome corresponds to reads that belong to the target and what fraction is the result of spurious matches. This framework has two steps: initial training and subsequent prediction (Fig. 1). Training set selection can be automatic or user defined. The automatic training generates reads from a randomly selected number of genomes (default is 200 genomes) from RefSeq (Pruitt, Tatusova & Maglott, 2007), and builds in-silico-generated datasets of about 1 million reads each. Simulated reads from the target genome(s) are then generated in a similar way and added to the former datasets in order to create the positive datasets with decreasing target abundances. Reads from the target genome(s) are omitted for the construction of negative datasets. All other genomes used to create the datasets are sampled in equal proportions (i.e., same relative abundances). The user can also choose the genomes to use to generate the training set (e.g., genomes previously known to co-occur in the same environment). In this case, the construction of the training set will be performed based on these genomes rather than the default genome collection from RefSeq. Simulated Illumina-like reads are generated using ART-MountRainier (Huang et al., 2012) with default settings. Simulation of reads from additional sequencing platforms is provided as an option, using also ART-MountRainier. Reads from both positive and negative samples are then recruited against the target genome sequence (reference) using BLAT (Kent, 2002). Alternatively, BLAST can be used to improve sensitivity at the expense of computational time (Altschul et al., 1997). By default, reads with identity higher than 95% and at least 90% of the read length aligned are selected to calculate sequencing breadth and sequencing depth, after normalizing for the size of the dataset. This level of identity has been shown to capture well the genome-aggregate Average Nucleotide Identity (ANI) typically seen between most currently named bacterial species, i.e., >95% ANI within vs. <95% ANI between species (Konstantinidis & Tiedje, 2005; Rodriguez et al., 2018) and the sequence-discrete populations recovered frequently in metagenomes of natural habitats (Caro-Quintero & Konstantinidis, 2012), although different user-defined cut-offs can be used as well. Members of such sequence-discrete populations show high gene-content and nucleotide sequence similarity among themselves, often -but not always- >95% ANI, and/or lower relatedness (e.g., <90% ANI) to close relatives (reviewed in Caro-Quintero & Konstantinidis, 2012). Sequencing depth (SD) is calculated as the number of reads mapping to the genome (N) multiplied by the read length (L) divided by the total length of the genome (G), and sequencing breadth (SB) is calculated as the number of bases covered (B) divided by the total length of the genome, using Eqs. (1) and (2) below, respectively. If the genome consists of more than one contig (e.g., draft genomes), the length is assumed to be the sum of the lengths of all contigs. (1)SD=L∗N∕G (2)SB=B∕G. A logistic function is fitted to the resulting recruitment data (i.e., SB and SD values or SB values alone; see also below) that attempts to separate the positive from the negative training datasets in terms of sequencing depth and sequencing breadth (the latter two are the variables of the function). In particular, this approach calculates the parameters of the logistic function by computing the error in the training set, i.e., what SB and SD values are observed for the 100 positive vs. the 100 negative training datasets, and modifying the parameters accordingly to reduce the error until convergence is reached. Error is assessed by a log-likelihood maximization via gradient approach, which modifies the parameter values until the error is minimized. Regression coefficients of the logistic equation are calculated for the SD and SB variables as well as for an intercept term and thus, the model estimates three parameters, i.e., SD, SB, and intercept. Final parameters of the model are estimated by default only based on SB (sequencing breadth), as this variable was found to be the most discriminating parameter for positive vs. negative samples (see also below). However, an estimation including SD is also provided as an option in order to produce, in addition to the probability of presence/absence, an accurate estimation of the abundance of the target genome. Estimation of the probability of detection and limit of detection Once the parameters of the logistic function have been determined (above), SB and SD can be used to reliably predict the probability of presence of the target genome in any number of query metagenomes after the reads of the query have been recruited against the target genome and (observed) SB is estimated as described above for training datasets. The probability of presence is estimated according to: (3)p=1−11∓e−z where z is a linear function of the form βTt, β represents the regression parameters and t is either a vector composed of the SD (Eq. (1)) and SB (Eq. (2)) or, by default, a one-dimensional variable corresponding to SB. Based on the model parameters (Eq. (3)), it is possible to establish a detection limit for the target genome in each metagenomic dataset analyzed. This limit is defined as the minimum fraction (SB) that needs to be sampled in order to estimate a probability of presence at 0.95. The result is displayed as a black solid line in a 2D plot of SB and SD (e.g., Fig. 2). The SD value observed based on the read recruitment, when corresponding to a probability value equal or higher to 0.95, is then used to estimate the relative abundance of the organism in the sample. The SD corresponding to 0.95 probability then provides the limit of detection in terms of relative abundance.";AbundanceEstimation
Trimmomatic;"Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested. Trimmomatic includes a variety of processing steps for read trimming and filtering, but the main algorithmic innovations are related to identification of adapter sequences and quality filtering, and are described in detail below. Trimmomatic uses two approaches to detect technical sequences within the reads. The first, referred to as ‘simple mode’, works by finding an approximate match between the read and the user-supplied technical sequence. This mode has the advantage of working for all technical sequences, including adapters and polymerase chain reaction (PCR) primers, or fragments thereof. Such sequences can be detected in any location or orientation within the reads but requires a substantial minimum overlap between the read and technical sequence to prevent false-positive findings. However, short partial adapter sequences, which often occur at the ends of reads, are inherently unable to meet this minimum overlap requirement and therefore are not detectable. The second mode, referred to as ‘palindrome mode’, is specifically aimed at detecting this common ‘adapter read-through’ scenario, whereby the sequenced DNA fragment is shorter than the read length, and results in adapter contamination on the end of the reads. This is especially the case for longer read length as supported by the Miseq. Although such short fragments should normally be removed during library preparation, in practice this process is not perfectly efficient, and thus many libraries suffer from this problem to some extent. ‘Palindrome mode’ can only be used with paired-end data, but has considerable advantages in sensitivity and specificity over ‘simple’ mode. Note that the current technical sequence identification approaches in Trimmomatic are not designed to filter or categorize data on the basis of ‘barcodes’. The algorithmic approach used for technical sequence alignments is somewhat unusual, avoiding the precalculated indexes often used in NGS alignments ( Li and Homer, 2010 ). Initial sequence comparisons are done using a 16-base fragment from each sequence. The 16 bases are converted to the 64-bit integer, known as the seed, using a 4-bit code for each base: A = 0001, T = 0010, C = 0100 and T = 1000. These seeds are then compared using a bitwise-XOR, which determines which bits differ between the two seeds. This will result in a 0000 code for each matching base, and a code with two 1 s for each mismatch, e.g. 0011 for an A-T mismatch, as XOR(0001,0010) = 0011. The ‘1’s within this result are then counted using the ‘popcount’ operation, and this count will be exactly twice the number of differing bases for the 16-base fragments. If the seeds are within the user-specified distance, the full alignment scoring algorithm is used. Matching bases are scored as forumla , which is ∼0.602, while mismatches are penalized depending on their quality score, by forumla , which can thus vary from 0 to 4. This results in a higher penalty for bases that are believed to be highly accurate. ‘Simple’ mode aligns each read against each technical sequence, using local alignment. This is implemented by finding the highest scoring region within the alignment, and thus may omit divergent regions on the ends. ‘Palindrome’ mode aligns the forward and reverse reads, combined with their adapter sequences. It uses global alignment, which is the total alignment score of the overlapping region. Trimmomatic offers two main quality filtering alternatives. Both approaches exploit the Illumina quality score of each base position to determine where the read should be cut, resulting in the retention of the 5′ portion, while the sequence on the 3′ of the cut point is discarded. This fits well with typical Illumina data, which generally have poorer quality toward the 3′ end. These two approaches are described in the following sections. The Sliding Window uses a relatively standard approach. This works by scanning from the 5′ end of the read, and removes the 3′ end of the read when the average quality of a group of bases drops below a specified threshold. This prevents a single weak base causing the removal of subsequent high-quality data, while still ensuring that a consecutive series of poor-quality bases will trigger trimming. A novel alternative approach was motivated by the realization that, for many applications, the incremental value of retaining additional bases in a read is related to the read length. Intuitively, it is clear that short reads are almost worthless because they occur multiple times within the target sequence and thus they give only ambiguous information. Even at the risk of introducing errors, it is worthwhile to retain additional low-quality bases early in a read, so that the trimmed read is sufficiently long to be informative. However, beyond a certain read length, retaining additional bases is less beneficial, and may even be detrimental. Reads of moderate length are likely to be already informative and, depending on the task at hand, can be almost as valuable as full-length reads. Therefore, the smaller potential benefit of retaining additional bases must be balanced against the increasing risk of retaining errors, which could cause the existing read value to be lost.";Trimming
SeqPurge;"Trimming of adapter sequences from short read data is a common preprocessing step during NGS data analysis. When performing paired-end sequencing, the overlap between forward and reverse read can be used to identify excess adapter sequences. This is exploited by several previously published adapter trimming tools. However, our evaluation on amplicon-based data shows that most of the current tools are not able to remove all adapter sequences and that adapter contamination may even lead to spurious variant calls. Here we present SeqPurge (https://github.com/imgag/ngs-bits), a highly-sensitive adapter trimmer that uses a probabilistic approach to detect the overlap between forward and reverse reads of Illumina sequencing data. SeqPurge can detect very short adapter sequences, even if only one base long. Compared to other adapter trimmers specifically designed for paired-end data, we found that SeqPurge achieves a higher sensitivity. The number of remaining adapter bases after trimming is reduced by up to 90 %, depending on the compared tool. In simulations with different error rates, we found that SeqPurge is also the most error-tolerant adapter trimmer in the comparison. SeqPurge achieves a very high sensitivity and a high error-tolerance, combined with a specificity and runtime that are comparable to other state-of-the-art adapter trimmers. The very good adapter trimming performance, complemented with additional features such as quality-based trimming and basic quality control, makes SeqPurge an excellent choice for the pre-processing of paired-end NGS data. The primary design goal of SeqPurge was to achieve a very high sensitivity while maintaining a state-of-the-art specificity. In general, the insert match approach is very sensitive and thus is the best approach for paired-end reads. However, certain sequence motives and unbalanced base content can make sequencing difficult in one read direction or even in both read directions. In these difficult sequences, the adapter match approach can perform better than the insert match approach. Thus, we combine the two approaches to increase the sensitivity of SeqPurge. First, we try to find an insert match between forward read and the reverse-complement of the reverse read. To detect an insert match, each possible offset between both reads is tested for a match (see Fig. 1b). If we find a match, the adapters are trimmed and the insert remains. If we find several matches, we select the best offset, i.e. the one with the lowest probability of being random. Several matches occur primarily in reads of regions with simple repeats. To prevent overtrimming because of false-positive matches in simple repeat regions, we also require a match between the previously defined adapter sequence and the sequence flanking the putative insert. This additional adapter match is required in only one out of the two reads, which makes the algorithm more robust towards bad read quality in one read direction. If no insert match was found, we check for adapter matches in the forward and reverse read separately. Again, each possible offset is tested for an adapter sequence match. If an adapter match is detected, the read is trimmed starting at the offset position. If only one of the two reads has an adapter match, the other read is trimmed at the offset as well, because a ‘read through’ is always symmetrical. The rationale is again that one read could have bad quality due to sequencing problems.";Trimming
Skewer;"Adapter trimming is a prerequisite step for analyzing next-generation sequencing (NGS) data when the reads are longer than the target DNA/RNA fragments. Although typically used in small RNA sequencing, adapter trimming is also used widely in other applications, such as genome DNA sequencing and transcriptome RNA/cDNA sequencing, where fragments shorter than a read are sometimes obtained because of the limitations of NGS protocols. For the newly emerged Nextera long mate-pair (LMP) protocol, junction adapters are located in the middle of all properly constructed fragments; hence, adapter trimming is essential to gain the correct paired reads. However, our investigations have shown that few adapter trimming tools meet both efficiency and accuracy requirements simultaneously. The performances of these tools can be even worse for paired-end and/or mate-pair sequencing. To improve the efficiency of adapter trimming, we devised a novel algorithm, the bit-masked k-difference matching algorithm, which has O(k n) expected time with O(m) space, where k is the maximum number of differences allowed, n is the read length, and m is the adapter length. This algorithm makes it possible to fully enumerate all candidates that meet a specified threshold, e.g. error ratio, within a short period of time. To improve the accuracy of this algorithm, we designed a simple and easy-to-explain statistical scoring scheme to evaluate candidates in the pattern matching step. We also devised scoring schemes to fully exploit the paired-end/mate-pair information when it is applicable. All these features have been implemented in an industry-standard tool named Skewer (https://sourceforge.net/projects/skewer). Experiments on simulated data, real data of small RNA sequencing, paired-end RNA sequencing, and Nextera LMP sequencing showed that Skewer outperforms all other similar tools that have the same utility. Further, Skewer is considerably faster than other tools that have comparative accuracies; namely, one times faster for single-end sequencing, more than 12 times faster for paired-end sequencing, and 49% faster for LMP sequencing. Skewer achieved as yet unmatched accuracies for adapter trimming with low time bound. with initialization at the upper boundary by C[0,j]=0, and at the left boundary by C[i,0]=i, for i=1,2,…,m and j=1,2,…,n. Finally, the algorithm tests the last row of the matrix, i.e. C[m,j], and reports those elements that are no greater than k. This algorithm has O(m n) time and O(m n) space complexity. The space bound can be easily reduced to O(m) if matrix C is computed by columns, noted as C j for j=1,2,…n, and report a match each time C j [m]≤k, because computing column C j requires only the knowledge of previous column Cj−1. With careful design, C j and Cj−1 can share one column vector, as proposed by Ukkonen [4]. Ukkonen also observed that for columns that have the last element greater than k, there is a boundary index of C j , noted as l a c(C j ), such that C j [l a c(C j )]=k and C j [l]>k for l=l a c(C j )+1,…m. It is easy to prove that l a c(C j )≤l a c(Cj−1)+1. Using this observation, Ukkonen reduced the time from O(m n) to expected O(k n) [4]. Our algorithm was developed from Ukkonen’s algorithm; however, we use a queue instead of an array to store all elements of current column above the boundary index. When there is a new element that corresponds to the topmost element of the new column, all elements in the queue shift automatically to the next (lower) position, just as elements transfer in the diagonals of matrix C. This process inherently keeps the basic properties of Ukkonen’s algorithm and facilitates subsequent improvements. Lemma 1 In the dynamic programming matrix C for tackling the k-difference problem, the values of elements along each diagonal are monotonically non-decreasing. The proof is provided in Additional file 5 Appendix A. Theorem 1 All the matched elements of the query pattern and sequence are equal to their precursors in the diagonal and do not need to be updated in the dynamic programming process. Proof. This theorem is a direct consequence of Lemma 1 and the dynamic programming recurrence, when δ i j =0. In other words, only mismatched elements need to be updated in the dynamic programming process. If bit-vectors that denote mismatched positions of comparison between the adapter sequence and each of the four nucleotide characters are pre-computed and a bit-vector that marks all positions of the queue elements that exceed the k-difference constraint is maintained, then unnecessary computations in updating the column vector can be inhibited. This is the key point that led to the main improvement of our algorithm over Ukkonen’s algorithm. As listed in Algorithm 1, the bit-masked k-difference matching algorithmhas the following characteristics: Use a queue instead of an array to store all elements of the current column above boundary index. In preprocessing, calculate for each of four nucleotide characters a bit-vector that denotes the mismatched positions compared with the adapter. Mark the internal cells that exceed the k-difference constraint by a bit-vector which shifts as the queue pushes it. When processing the column starting from each input nucleotide, update only the cells that mismatch and have not been marked. This algorithm uses a queue of size m and several bit vectors of size ⌈m/w⌉, where w is the word length of the computer (for example w equals 64 for a 64-bit machine), and hence has a space of O(m). For each of the n characters in a target sequence, the character enters the queue once and exits from the queue at most once. For a random sequence, the expected size of the queue is O(k); hence, generally the algorithm has O(k n) expected time. However, because it is restricted by the bit-mask operations, each element in the queue usually updates at most k+1 times. Because bit operations are negligible compared with element update operations, this algorithm achieves O(k n) worst-case time in practice, which is better than the O(k n) expected time for Ukkonen’s algorithm.";Trimming
Ktrim;"Next-generation sequencing (NGS) data frequently suffer from poor-quality cycles and adapter contaminations therefore need to be preprocessed before downstream analyses. With the ever-growing throughput and read length of modern sequencers, the preprocessing step turns to be a bottleneck in data analysis due to unmet performance of current tools. Extra-fast and accurate adapter- and quality-trimming tools for sequencing data preprocessing are therefore still of urgent demand. Ktrim was developed in this work. Key features of Ktrim include: built-in support to adapters of common library preparation kits; supports user-supplied, customized adapter sequences; supports both paired-end and single-end data; supports parallelization to accelerate the analysis. Ktrim was ∼2–18 times faster than current tools and also showed high accuracy when applied on the testing datasets. Ktrim could thus serve as a valuable and efficient tool for short-read NGS data preprocessing. Ktrim provides both adapter- and quality-trimming of the sequencing data. The schematic workflow of Ktrim was shown in Supplementary Fig. S1. In the current implementation, Ktrim performs quality-trimming first. For each read, Ktrim screens the quality of the sequencing cycles from its tail to head and stops at a position where the quality of the corresponding cycle is higher than the given threshold; then Ktrim will trim all the cycles after this one. If the remaining sequence is shorter than the minimum acceptable size (which parameter could be set by the users), this read will be discarded; otherwise Ktrim will perform adapter-trimming on this read. Ktrim employs a dual-index scheme to search for adapters in the sequencing data. For paired-end reads, Ktrim uses the head 3 bp of the two adapters linked to read 1 and read 2, respectively, as indexes to find the potential ligating position (“seed”s) of the adapters; for single-end data, however, Ktrim uses the head 3 bp and the very next 3 bp of the adapter linked to read 1 as the indexes. This dual-index approach could tolerate sequencing errors in the adapter sequences thus improves sensitivity. After finding the “seed”s, Ktrim searches for adapters from the left-most “seed” cycle by comparing the sequencing data versus the adapters base-by-base until the end of the sequencing reads or the adapters, whichever occurs first. In this manner, Ktrim could handle both scenarios where adapters are completely included or partially covered in the sequencing data. If the number of mismatches identified during the comparison of sequencing reads and adapters is smaller than a dynamic threshold (which parameter could be set by the users), this “seed” position will be called as the adapter ligation site, then Ktrim trims all the cycles after the “seed” position and ignores all the rest “seed”s; otherwise the next “seed” position will be investigated until all the “seed”s get exhausted. After adapter-trimming, Ktrim checks the sizes of the reads again and only reports those longer than the minimum acceptable size specified by the user. For performance evaluations, 10 million paired-end DNA reads were in silico generated. The read length was set to 100 bp while DNA insert size ranged from 10 to 200 bp. Hence, the dataset was consisted of reads that contain complete or partial adapters as well as those not contaminated by adapters. Meanwhile, random mutations were also introduced into the reads to simulate sequencing errors. Notably, the adapter information is known for these in silico reads, therefore allows accuracy evaluation and comparison of adapter-trimming algorithms. Furthermore, the read-1 also served as the testing dataset of single-end reads. Three widely used tools, Trim Galore (a wrapper of the Cutadapt program (Martin, 2011)), Trimmomatic (Bolger, et al., 2014) and SeqPurge (Sturm, et al., 2016), were collected for performance comparisons. In addition, a real dataset collected from the literature (Zhang, et al., 2016) was also analyzed. Detailed information (including codes) on simulation data generation and performance evaluations could be found in Supplementary Method.";Trimming
ConDetri-a;"During the last few years, DNA and RNA sequencing have started to play an increasingly important role in biological and medical applications, especially due to the greater amount of sequencing data yielded from the new sequencing machines and the enormous decrease in sequencing costs. Particularly, Illumina/Solexa sequencing has had an increasing impact on gathering data from model and non-model organisms. However, accurate and easy to use tools for quality filtering have not yet been established. We present ConDeTri, a method for content dependent read trimming for next generation sequencing data using quality scores of each individual base. The main focus of the method is to remove sequencing errors from reads so that sequencing reads can be standardized. Another aspect of the method is to incorporate read trimming in next-generation sequencing data processing and analysis pipelines. It can process single-end and paired-end sequence data of arbitrary length and it is independent from sequencing coverage and user interaction. ConDeTri is able to trim and remove reads with low quality scores to save computational time and memory usage during de novo assemblies. Low coverage or large genome sequencing projects will especially gain from trimming reads. The method can easily be incorporated into preprocessing and analysis pipelines for Illumina data. ConDeTri is implemented in Perl (required version 5.8.9 or above), is platform independent, has no additional hardware or library requirements, and is distributed under Artistic License/GPL. It is designed to run single-threaded on desktop computers or on cluster machines. In default mode, it can be run by giving only one Fastq file for single-end sequencing or two Fastq files for paired-end sequencing. More advanced options allow the user to control such things as the quality values used for trimming, trimming size, the fraction of a read containing high quality bases, and the quality format used (either Illumina/Solexa Fastq format or Sanger Fastq format is chosen by different offset scores). Our trimming approach does not correct the actual quality scores called by the Illumina pipeline. Instead, it removes bases with quality values lower than a threshold from the 3′-end of a read and checks the remaining read for internal low quality bases. ConDeTri applies two filtering steps on each read. First, each read is trimmed, one base at the time, in an iterative process. Starting from the 3′-end of the read, bases are removed if the corresponding quality score is lower than a threshold QH. When reaching a base with a quality score higher than QH, the base is kept temporarily while following bases are evaluated. After parsing a certain number of consecutive high quality bases, nH, the trimming is terminated. However, even bases with low quality scores below QH, recorded before nH is reached, are saved temporarily. Up to nL consecutive low quality bases are accepted when they are surrounded by high quality bases. If nL is overrun, all temporarily saved bases are removed, and the process starts over again. The trimming continues until either nH consecutive high quality bases are found, or the read is trimmed down to length L. For a trimmed read to be approved, it must contain more than a certain fraction f of bases with a quality score higher than QH, and no bases with a quality score less than a lower bound threshold QL. If a base has a quality score lower than QL the read is removed. When all reads have been trimmed, each read or each read pair is examined. If a single read passes the quality check, it is stored in a new Fastq file. For paired end reads, pairs where both the reads fulfill the quality demands are saved in new paired Fastq files. If a pair contains only one read passing the quality requirements, the high quality read is saved in an extra Fastq. These reads can be used as single end reads. Besides Fastq files, ConDeTri reports the number of scanned and removed reads and the number of reads that are present as paired-end and as single-end reads. Figure S1 summarizes the algorithm in a flowchart and Figure S2 gives two examples. Per default, the high quality score (QH) is set to 25, which is similar to a sequencing error probability of 0.0032. This value was chosen after inspecting quality score distributions from several data sets with different insert sizes from the collared flycatcher genome-sequencing project, as a level where the number of bases kept are of highest possible quality without having a considerable loss of reads. For the sets inspected, changing the quality threshold to 30 resulted in a loss of the majority of reads during filtering. On the other hand, lowering it to 20 did not increase the number of reads kept significantly, but the per base error probability of those reads will be up to three times higher (∼0.01). However, the default value is by no means universal, and the threshold should be set according to the data. The low quality score (QL) is set to 10, which equals a probability of a sequencing error of 0.0909, the fraction f of bases with a quality score higher than QH is set to 80% and L, the minimum number of bases after trimming, is set to 50, to prevent saving reads that are too short for de novo assembly. The parameters nH and nL are set to 5 and 1, respectively. This means that for each low quality-base there must be at least five high quality bases, which is more than the QH value of 80%. The connection between these numbers must be considered when tweaking the parameters – keeping nH and nL as 5 and 1 but increasing the QH to 95% results in removing a large proportion of reads in the second step. However, all these settings can be changed as desired. Quality score distribution along reads and read length distribution after trimming for the libraries used for choosing the default values are shown in Figures S3, S4, S5, S6, S7, S8, S9, S10, S11, S12, S13, S14, S15, S16, S17 and Table S1. ConDeTri can read all three different Fastq quality score standards: Illumina and Solexa (early Illumina) quality scores with an offset of 64 and Sanger standard with an offset of 33.";Trimming
InfoTrim;"Biological DNA reads are often trimmed before mapping, genome assembly, and other tasks to improve the quality of the results. Biological sequence complexity relates to alignment quality as low complexity regions can align poorly. There are many read trimmers, but many do not use sequence complexity for trimming. Alignment of reads generated from whole genome bisulfite sequencing is especially challenging since bisulfite treated reads tend to reduce sequence complexity. InfoTrim, a new read trimmer, was created to explore these issues. It is evaluated against five other trimmers using four read mappers on real and simulated bisulfite treated DNA data. InfoTrim has reasonable results.
Trimmer performance was evaluated with bisulfite-treated read mapping accuracy using four read mappers: BisPin [10], Bismark [11], BWAmeth [12], and Walt [13]. Bisulfite-treated reads are used to study epigenetic methyl-cytosine, and these reads are challenging to map correctly since bisulfite-treatement introduces variation and tends to reduce entropy [5]. The default r=0.4 and s=0.1 values were found from tuning on 500k 200bp Illumina simulated bisulfite DWGSIM reads using the F1-score [14]. Trimmo-matic was set to MAXINFO:25:0.5, cutadapt to q=10, and reaper had DUST=90. All other trimmers and mappers used default settings. Reaper sometimes produced empty records. One simulated and two real data sets were used to validate the trimmer and mapper pipelines. Sherman was used to generate 500k 150bp simulated Illumina bisulfite reads with err=2%, CG=20%, and CH=2% conversion rates [15]. The F1-score was calculated where a read was correctly mapped if the starting position was within three bases of the real position. A strict BisPin filter of 85 was used to map 500k real mouse bisulfite reads with SRA number SRR921759. Alignments passing this test are aligned presumptively correctly. Bismark's filter is complex, and Walt and BWAmeth have no alignment filter. A second real data set with hairpin data was prepared. This data allows the recovery of the original untreated strand, which can be aligned with regular read mappers [5], and read alignment locations from the trimmer and mapper pipelines for bisulfite-treated reads were compared with these presumptively correct locations as an accuracy test.";Trimming
CutAdapt;"When small RNA is sequenced on current sequencing machines, the resulting reads are usually longer than the RNA and therefore contain parts of the 3' adapter. That adapter must be found and removed error-tolerantly from each read before read mapping. Previous solutions are either hard to use or do not offer required features, in particular support for color space data. As an easy to use alternative, we developed the command-line tool cutadapt, which supports 454, Illumina and SOLiD (color space) data, offers two adapter trimming algorithms, and has other useful features. Cutadapt computes either ‘regular’ or slightly modified semi-global alignments. Regular semi-global alignments, also called end-space free alignments [7, Chapter 11.6.4], do not penalise initial or trailing gaps. This allows the two sequences to shift freely relative to each other. When the “-a” parameter is used to provide the sequence of an adapter, the adapter is assumed to be ligated to the 3’ end of the molecule, and the behaviour of cutadapt is therefore to remove the adapter and all characters after it. With regular semi-global alignments, a short, usually random, match that overlaps the beginning of a read would lead to the removal of the entire read. We therefore require that an adapter starts at the beginning or within the read. This is achieved by penalising initial gaps in the read sequence, which is the only modification to regular overlap alignment. Regular semi-global alignment is used when the location of the adapter is unknown (assumed when the “-b” parameter is used). Then, if the adapter is found to overlap the beginning of the read, all characters before the first non-adapter character are removed. See Figure 1 for an illustration of all possible cases. After aligning all adapters to the read, the alignment with the greatest number of characters that match between read and adapter is considered to be the best one. Next, the error rate e/l is computed, where e is the number of errors, and l is the length of the matching segment between read and adapter. Finally, if the error rate is below the allowed maximum, the read is trimmed.";Trimming
AlienTrimmer;"Contaminant oligonucleotide sequences such as primers and adapters can occur in both ends of high-throughput sequencing (HTS) reads. AlienTrimmer was developed in order to detect and remove such contaminants. Based on the decomposition of specified alien nucleotide sequences into k-mers, AlienTrimmer is able to determine whether such alien k-mers are occurring in one or in both read ends by using a simple polynomial algorithm. Therefore, AlienTrimmer can process typical HTS single- or paired-end files with millions of reads in several minutes with very low computer resources. Based on the analysis of both simulated and real-case Illumina®, 454™ and Ion Torrent™ read data, we show that AlienTrimmer performs with excellent accuracy and speed in comparison with other trimming tools. Decomposing DNA sequence into overlapping k-mers (i.e. sequences of length k) is a standard task in bioinformatics. Given a non-zero positive integer value k, each of the 4k k-mers can be stored with only nk bits by using an n-bit binary code bn (provided n > 1). AlienTrimmer uses a binary coding b2 defined as b2(A) = 00, b2(C) = 01, b2(G) = 10, and b2(T) = 11. In order to deal with degenerate nucleotides N, it also uses another binary coding b4 defined as b4(A) = 0001, b4(C) = 0010, b4(G) = 0100, and b4(T) = 1000 (see below for details about b4 coding of degenerate residues). Therefore, each k-mer is bijectively associated to a unique binary number of nk bits. Usual computer words being 32 or 64 bit long, such binary codes allow k-mers with k ≤ 16 to be easily computed and stored. Given an alien or read sequence, AlienTrimmer computes the binary representation of its k-mers by using three basic bit operations: bitwise OR (|), bitwise AND (&), and bit left shifting (<<). More formally, if wi is the binary representation of the k-mer starting at position i and ending at position i + k − 1, then the binary representation wi + 1 of the next k-mer is easily computed by the formula wi + 1 = ((wi << n) | bn(ci + k)) & maskn, where ci + k is the character state at position i + k, and maskn a constant binary representation of nk − 1 that allows setting to zero all bits shifted beyond the nkth one. Following this approach, the list of every k-mer from a nucleotide sequence of length L is computed in time O(L). Given a fixed integer value k (= 10 by default), AlienTrimmer first performs k-mer decomposition from each specified alien sequence, and the binary representation w of each extracted alien k-mer is computed following both binary codes b2 and b4 in order to deal with the presence of degenerate nucleotides (see below). Each binary representation of a k-mer encoded with b2 being equivalent to an integer lying between 0 (i.e. poly-A of length k) and 4k − 1 (i.e. poly-T of length k), each distinct alien k-mer is stored inside a bitset Β of size 4k (i.e. if an alien k-mer is b2-coded as w, the xth bit of Β is set to 1 where x is the integer value equivalent to w). Each alien k-mer binary representation encoded with b4 is stored in a sorted list Λ. Note that AlienTrimmer also stores the binary representations of the reverse-complement of each extracted alien k-mer, in order to be able to search for the reverse-complement of each specified alien sequence in read ends. This whole procedure allows computing and storing (in both bitset Β and sorted list Λ) the K = |Λ| distinct k-mers that can be extracted from the different specified alien sequences (see algorithm details in Supplementary materials S1.1). Denoting LA as the sum of the different alien sequence lengths, we have K ∈ O(LA) even if K is much smaller than LA in practice; therefore this first pre-computing step requires O(LA log LA) time complexity. Given a read sequence of length LR, AlienTrimmer proceeds its successive k-mer surveying following the same way as for alien sequences. However, for each read k-mer, AlienTrimmer searches whether it is present among the K alien ones. When a read k-mer only contains non-degenerate nucleotides (i.e. A, C, G and T), this search is directly performed by considering its b2 coding, and looking in the pre-computed alien k-mer bitset Β whether its corresponding bit is set to 1. In contrast, when a read k-mer contains at least one degenerate nucleotide, AlienTrimmer considers its b4 coding, which is able to deal with such character state. By setting b4(N) = 0000, a k-mer containing degenerate nucleotides is compatible with another k-mer with no degenerate nucleotide if their respective binary coding w and w′ verify the simple property w | w′ = w′; indeed, b4(s) | b4(s′) = b4(s′) only when s = s′ or s = N. When a read k-mer w contains at least one degenerate nucleotide, AlienTrimmer searches whether there exists one compatible alien k-mer w′ among the K ones. This leads to a worst case O(K) time complexity for each read k-mer with at least one degenerate nucleotide. However, as the K b4-coded alien k-mers are sorted in Λ (see above), this step is accelerated by searching first the largest alien binary representation w′min such that w′min < w. Recalling that b4(N) = 0000, the binary search algorithm (e.g. [21]) allows finding w′min among the K alien k-mers in time O(log K). Reciprocally, by updating a second binary representation  from w with the coding , AlienTrimmer also searches for the lowest alien binary representation w′max such that  with the binary search algorithm. Thanks to the two coding b4 and  of N, if there exists inside Λ at least one alien binary representation w′ compatible with the read one w containing at least one degenerate nucleotide, then one has w′min ≤ w′ ≤ w′max, and AlienTrimmer searches for w′ only between these two bounds. The two O(log K) binary searches of w′min and w′max do not modify the theoretical O(K) computational complexity, but allow observing faster running times in practice than crudely testing compatibility of w with each of the K alien w′ in Λ. Using the previously described method, AlienTrimmer is able to determine every nucleotide indexes of a read where an exact alien k-mer match occurs. By using these nucleotide indexes, AlienTrimmer easily estimates the alien coverage within the read, i.e. the number of times each read nucleotide is covered by alien k-mers. The alien k-mer coverage can be directly computed together with the alien k-mer matching process without modifying the overall time complexity (see algorithm details in Supplementary materials S1.2). Moreover, this simple approach can be easily extended to quality-based trimming by incrementing by 1 the coverage value of every nucleotide supported by a low Phred score value. Finally, AlienTrimmer performs trimming by removing the prefix or suffix sub-sequences of the read when their alien k-mer coverage is higher than zero. Fig. 4 represents four examples of read contamination, with graphical representations of the alien coverage estimated for each nucleotide with k = 10, and the resulting trimmed alien residues. Though fast and efficient when complete alien oligonucleotide sequences are present in at least one of both ends of a read (see Figs. 4A and B), this algorithm may not accurately perform its task when mismatches occur within the read ends to be trimmed. In practice, this situation corresponds to some single nucleotides with zero alien coverage occurring between nucleotides with non-zero alien coverage (see base ‘A’ with white background on Fig. 4C). Albeit infrequent when low quality nucleotides were trimmed off (see Section 2.2 and Fig. 3), such mismatches are easily accommodated by AlienTrimmer in the following manner. For 5′ end, AlienTrimmer first searches for the index i of the first nucleotide with non-zero alien coverage. Second, AlienTrimmer surveys the following alien covered nucleotides until a nucleotide of index j with zero alien coverage is reached. Given a specified number of mismatches m (= ⌈k/2⌉ by default), AlienTrimmer verifies whether the nucleotide at index j + m is covered by an alien k-mer; if any, AlienTrimmer re-iterates its search of the next index j such that nucleotides at both indexes j and j + m are not covered by any alien k-mer (see algorithm details in Supplementary materials S1.3). This approach allows identifying the sub-read defined between indexes i and j that is mainly covered by alien k-mers. If this sub-read is sufficiently close enough to the 5′ end (i.e. j ≥ 2i), then AlienTrimmer removes the read prefix up to index j. The 3′ end trimming is performed following the same approach by starting from the last nucleotide index i with non-zero alien coverage, and performs backward searching of the index j < i such that nucleotides at indexes j and j − m are not covered by any alien k-mer. Given a collection of alien sequences of total length LA, decomposing and storing them into K distinct alien k-mers require O(LA log LA) time complexity (see above), which is negligible in comparison with the overall alien trimming process. Given a read of length LR, AlienTrimmer is able to detect and remove alien sequences in 5′ and 3′ ends in time O(LR) or O(LR K) depending on the absence of degenerate nucleotide within the read or not, respectively. Knowing that K ∈ O(LA), the overall time complexity required by AlienTrimmer for processing each read of length LR is O(LA log LA + LR) in the best case scenario (i.e. no degenerate nucleotide), or O(LA(LR + log LA)) in the worst case scenario (i.e. presence of degenerate nucleotides within the read). Therefore, when dealing with standard HTS data with N reads with no degenerate nucleotide and N′ reads with degenerate nucleotides, the overall time complexity required by AlienTrimmer is O(LA log LA + N LR + N′ LA LR).";Trimming
cutPrimers;"Cutting of primers from reads is an important step of processing targeted amplicon-based next generation sequencing data. Existing tools are adapted for cutting of one or several primer/adapter sequences from reads and removing all of their occurrences. Also most of the existing tools use kmers and may cut only part of primers or primers with studied sequence of gene. Because of this, use of such programs leads to incorrect trimming, reduction of coverage, and increase in the number of false-positive and/or false-negative results. We have developed a new tool named cutPrimers for accurate cutting of any number of primers from reads. Using sequencing reads that were obtained during study of BRCA1/2 genes, we compared it with cutadapt, AlienTrimmer, and BBDuk. All of them trimmed reads in such a way that coverage of at least two amplicons decreased to unacceptable level (<30 reads) comparing with reads trimmed with cutPrimers. At the same time, Trimmomatic and AlienTrimmer cut all occurrences of primer sequences, so the length of the remaining reads was less than prospective. As an input, the program cutPrimers uses four FASTA-files with primer sequences located on the 50 -end and 30 -end of R1 and R2 reads, respectively. Alternatively, it may trim primers only on the 50 -end (e.g., when amplicon length is 150 bp and read length is 2 · 75 bp). For input and output of FASTQ- and FASTAfiles, it uses biopython package SeqIO (Cock et al., 2009). For searching primer sequences in the reads, cutPrimers applies two Python modules: regex, which searches primer sequences with regular expressions and multiprocessing. The last one is necessary for multithreading. During processing of the reads, the program can save error statistics in the primers. If an error occurs in one of the reads of the read pair, it considers it as a sequencing error, if an error occurs in both reads, it considers it as a primer synthesis error. Such additional information is useful for evaluation of primer synthesis quality as well as for estimation of the amplicon coverage. Moreover, cutPrimers can detect primer dimers that may form during amplification. Our algorithm allows achieving high accuracy of identifying of the primer sequences in NGS reads by simultaneous search of the primer sequences in the pairs of the reads. We compared our tool with other available tools: cutadapt, BBDuk, and AlienTrimmer. Use of Trimmomatic has not yielded acceptable number of reads. Examples of commands used for execution are available in Supplementary Material.";Trimming
AfterQC;"Some applications, especially those clinical applications requiring high accuracy of sequencing data, usually have to face the troubles caused by unavoidable sequencing errors. Several tools have been proposed to profile the sequencing quality, but few of them can quantify or correct the sequencing errors. This unmet requirement motivated us to develop AfterQC, a tool with functions to profile sequencing errors and correct most of them, plus highly automated quality control and data filtering features. Different from most tools, AfterQC analyses the overlapping of paired sequences for pair-end sequencing data. Based on overlapping analysis, AfterQC can detect and cut adapters, and furthermore it gives a novel function to correct wrong bases in the overlapping regions. Another new feature is to detect and visualise sequencing bubbles, which can be commonly found on the flowcell lanes and may raise sequencing errors. Besides normal per cycle quality and base content plotting, AfterQC also provides features like polyX (a long sub-sequence of a same base X) filtering, automatic trimming and K-MER based strand bias profiling. For each single or pair of FastQ files, AfterQC filters out bad reads, detects and eliminates sequencer’s bubble effects, trims reads at front and tail, detects the sequencing errors and corrects part of them, and finally outputs clean data and generates HTML reports with interactive figures. AfterQC can run in batch mode with multiprocess support, it can run with a single FastQ file, a single pair of FastQ files (for pair-end sequencing), or a folder for all included FastQ files to be processed automatically. Based on overlapping analysis, AfterQC can estimate the sequencing error rate and profile the error transform distribution. The results of our error profiling tests show that the error distribution is highly platform dependent. Much more than just another new quality control (QC) tool, AfterQC is able to perform quality control, data filtering, error profiling and base correction automatically. Experimental results show that AfterQC can help to eliminate the sequencing errors for pair-end sequencing data to provide much cleaner outputs, and consequently help to reduce the false-positive variants, especially for the low-frequency somatic mutations. While providing rich configurable options, AfterQC can detect and set all the options automatically and require no argument in most cases. AfterQC is designed to process FastQ files in batches. It goes through a folder with all FastQ files (can be single-end or pair-end output), which are typically data of a sequencing run for different samples, and passes each FastQ file or pair into the QC and filtering pipeline. As described in Fig. 1, firstly, AfterQC will run a bubble detection to find the bubbles raised during the sequencing process. Secondly, a pre-filtering QC will be conducted to profile the data with per-cycle base content and quality curves. Thirdly, AfterQC will do automatic read trimming based on data quality profiling. Fourthly, each read will be filtered by bubble filter, polyX filter, quality filter and overlapping analysis filters, the ones failed to pass these filters will be discarded as bad reads. Fifthly, an error correction based on overlapping analysis will be applied for pair-end sequencing data. Finally, AfterQC will store the good reads, perform post-filtering QC profiling and generate HTML reports.";QualityControl
NGSQC;"Next generation sequencing (NGS) technologies provide a high-throughput means to generate large amount of sequence data. However, quality control (QC) of sequence data generated from these technologies is extremely important for meaningful downstream analysis. Further, highly efficient and fast processing tools are required to handle the large volume of datasets. Here, we have developed an application, NGS QC Toolkit, for quality check and filtering of high-quality data. This toolkit is a standalone and open source application freely available at http://www.nipgr.res.in/ngsqctoolkit.html. All the tools in the application have been implemented in Perl programming language. The toolkit is comprised of user-friendly tools for QC of sequencing data generated using Roche 454 and Illumina platforms, and additional tools to aid QC (sequence format converter and trimming tools) and analysis (statistics tools). A variety of options have been provided to facilitate the QC at user-defined parameters. The toolkit is expected to be very useful for the QC of NGS data to facilitate better downstream analysis. All tools in the toolkit have been developed using Perl programming language by implementing modularized structure using several sub-routines for various tasks, which allows better maintainability. GD module has been used to generate various graphs for statistics and String::Approx module for searching primer/adaptor sequence in input reads. Parallel::ForkManager and Threads modules have been utilized for parallelizing the QC tools. IO::Zlib module has been used to facilitate reading/writing compressed (gzip) files. QC reports are generated using Hypertext Markup Language (HTML) and Cascading Style Sheets (CSS). All the tools have been tested on Windows and Linux (CentOS) operating systems for full functionality.";QualityControl
QC-Chain;"Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7–8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis. The objectives of QC-Chain include (1) retrieving reads with high quality; (2) identifying and quantifying the source of contaminations, and filtering contaminating reads; (3) accomplishing the QC process in a relatively short time. To achieve these objectives, the overall method of QC-Chain includes sequencing quality assessment and trimming, and contamination screening and removal. Additionally, evaluation and comparison of downstream analysis results using reads after QC were also included as an important component for this holistic approach. The sequencing quality assessment and trimming is the first step for NGS data quality control, which requires both accuracy and efficiency. To accomplish this step, we developed a parallel quality control software, Parallel-QC, which could be used to trim, filter and remove low sequencing-quality reads from NGS data. Parallel-QC is developed by Linux C++ and multi-thread technology based on multi-core X86 CPU platform, and is compatible for X86 and X86-64 Linux. Specifically, by Parallel-QC, sequences could be trimmed to a specific length; low-quality bases within reads could be trimmed from both 5′ and 3′ ends; low-quality reads could be filtered by quality value with user defined percentage; duplications could be identified and removed. For tag sequences filtration, multiple tag sequences could be aligned and shifted on both 5' and 3' ends of the reads with mismatches allowed, and the positive aligned reads could be removed. To significantly accelerate the speed of computation, Parallel-QC parallelizes the sequencing quality evaluation and filtration steps by assigning balanced and weighted tasks to independent threads, which could be executed on different CPU cores simultaneously. In addition, all progresses could be completed with only one disk I/O operation, which highly improves the efficiency of analysis. On the other hand, the multiple steps can be accomplished by using a single command line with user-friendly options. Therefore, Parallel-QC significantly shortens the processing time compared to traditional single core CPU based method, and simplifies user’s operation compared to using multiple single function QC tools. Identification and Removal of Contaminating Reads The aim of contamination screening is to identify and quantify the (mostly unknown) source of contaminations, filter the contaminating reads, and obtain the processed reads as clean as possible. We adopted two complementary strategies, both of which could provide (known and unknown) species information of the dataset. In the “rDNA-reads based” method, ribosomal DNA reads were used to qualitatively detect the taxonomical structures of the dataset quickly. Ribosomal RNAs, such as 16S (for prokaryote) and 18S (for eukaryotes) sequences, are good indicators to characterize prokaryotic and eukaryotic species and are commonly used in phylogenetic analysis. They are also widely used in metagenomic analysis to detect the community structure. Here we applied Parallel-META [6], a high-performance 16S/18S rRNA analysis pipeline to report the taxonomic classification, construction and distribution of NGS reads. Parallel-META is a GPU and Multi-Core CPU based software, which firstly extracts the (user selected) 16S or 18S rRNA sequences from the input data and aligns the obtained rRNA reads to several optional databases, including RDP [7], GREENGENES [8] and SILVA [9]. The taxonomy information is produced and then shown in a dynamic graphic view with corresponding species’ proportion. Additionally, in QC-Chain, Parallel-META was updated to be able to accomplish eukaryotic species screening and identification, but in previous version it could only identify prokaryotic species. Through this approach, all the possible species sources of the raw reads, including both prokaryotic and eukaryotic information, could be detected de novo. The other method is “random-reads based”, which could quantitatively provide the species information. Generally, detecting all possible contaminations requires aligning reads to a comprehensive database, which includes species records as many as possible. The most popular and widely-used alignment method is BLAST against NCBI (National Center for Biotechnology Information) database (http://www.ncbi.nlm.nih.gov/). However, it is known that BLAST is a time-consuming process and the speed is a bottleneck, especially when analyzing immense amount of reads. An alternative is to reduce the size of the query data and perform BLAST to get the species information quickly. With such a consideration, we developed an in-house script which could randomly extract reads from the raw reads with a user-defined proportion of all reads. The extracted reads were then aligned to NCBI-NT database using BLASTn, to extract species information in a relatively short time. The above two approaches are complementary and synergetic to each other: rDNA-reads based method could quickly screen and identify the possible contaminating species. The random-reads based method could provide quantitative evaluation of the contaminations, and also help to verify the result of rDNA-reads based method. After confirming the contaminating sources by combining the results of the above two methods, the contaminating reads are filtered by the alignment tool Bowtie 0.12.8 [5] with default parameters: reads aligned to contaminating species’ genomes are filtered out.";QualityControl
SAMStat;"The sequence alignment/map format (SAM) is a commonly used format to store the alignments between millions of short reads and a reference genome. Often certain positions within the reads are inherently more likely to contain errors due to the protocols used to prepare the samples. Such biases can have adverse effects on both mapping rate and accuracy. To understand the relationship between potential protocol biases and poor mapping we wrote SAMstat, a simple C program plotting nucleotide overrepresentation and other statistics in mapped and unmapped reads in a concise html page. Collecting such statistics also makes it easy to highlight problems in the data processing and enables non-experts to track data quality over time. We demonstrate that studying sequence features in mapped data can be used to identify biases particular to one sequencing protocol. Once identified, such biases can be considered in the downstream analysis or even be removed by read trimming or filtering techniques. SAMStat automatically recognizes the input files as either fasta, fastq, SAM or BAM and reports several basic properties of the sequences as listed in Table 1. Multiple input files can be given for batch processing. For each dataset, the output consists of a single html5 page containing several plots allowing non-specialists to visually inspect the results. Naturally, the html5 pages can be viewed both on- and off-line and easily be stored for future reference. All properties are plotted separately for different mapping quality intervals if those are present in the input file. For example, mismatch profiles are given for high-and low-quality alignments allowing users to verify whether poorly mapped reads contain a specific collection of mismatches. The latter may represent untrimmed linkers in a subset of reads. Dinucleotide overrepresentation is calculated as described by Frith et al. (2008). Overrepresented 10mers are calculated by comparing the frequency of 10mer within a mapping quality interval compared with the overall frequency of the 10mer.";QualityControl
ClinQC;"Traditional Sanger sequencing has been used as a gold standard method for genetic testing in clinic to perform single gene test, which has been a cumbersome and expensive method to test several genes in heterogeneous disease such as cancer. With the advent of Next Generation Sequencing technologies, which produce data on unprecedented speed in a cost effective manner have overcome the limitation of Sanger sequencing. Therefore, for the efficient and affordable genetic testing, Next Generation Sequencing has been used as a complementary method with Sanger sequencing for disease causing mutation identification and confirmation in clinical research. However, in order to identify the potential disease causing mutations with great sensitivity and specificity it is essential to ensure high quality sequencing data. Therefore, integrated software tools are lacking which can analyze Sanger and NGS data together and eliminate platform specific sequencing errors, low quality reads and support the analysis of several sample/patients data set in a single run. We have developed ClinQC, a flexible and user-friendly pipeline for format conversion, quality control, trimming and filtering of raw sequencing data generated from Sanger sequencing and three NGS sequencing platforms including Illumina, 454 and Ion Torrent. First, ClinQC convert input read files from their native formats to a common FASTQ format and remove adapters, and PCR primers. Next, it split bar-coded samples, filter duplicates, contamination and low quality sequences and generates a QC report. ClinQC output high quality reads in FASTQ format with Sanger quality encoding, which can be directly used in down-stream analysis. It can analyze hundreds of sample/patients data in a single run and generate unified output files for both Sanger and NGS sequencing data. Our tool is expected to be very useful for quality control and format conversion of Sanger and NGS data to facilitate improved downstream analysis and mutation screening. It uses four other tools including FASTQC [7], PRINSEQ [8], Alientrimmer [18], and TraceTuner [19]. The ClinQC workflow is depicted in Fig. 1 and consist of several sequential steps that lead from the raw sequencing reads to the high quality Sanger encoded FASTQ file for each patient/sample. All parameter settings can be specified in a single configuration file (Additional files 1 and 2). To achieve the optimized performance, ClinQC uses the available hardware (Physical memory and CPU) in a best possible way. A buffer file read write concept was implemented where input and output are partially stored in memory during the analysis, which reduces the computation time and reduces the disk reading and writing workload. The ClinQC pipeline (Fig. 1) consists of nine sequential steps that starts with raw sequencing reads and ends up with three outputs: 1) QC summary table, 2) FASTQ files with high quality reads and 3) QC report. The detailed description of each step is given below: 1. Base calling Due to unclear signal in Sanger pherogram files, the base caller of the sequencer always calls ambiguous nucleotide as N. However, it could output more specific ambiguous nucleotides, i.e., R, if signal is not clear between A or G; Y, if signal is not clear between C and T. Therefore, ClinQC uses the tool TraceTunner [19] to improve the base calling and assign more specific ambiguous nucleotides. 2. Format conversion In this step, ClinQC check the raw sequencing files and their formats and, if needed, converts from native file format to FASTQ with Sanger quality encoding (Fig. 2). Sanger sequencing files are accepted in AB1 and SCF format and NGS files are accepted in SFF, FASTA-QUAL and FASTQ format. This step is only applicable for NGS data, where multiple samples are sequenced in a single sequencing run by using the multiplexing method. Based on the barcode sequences (MID: Multiplexed Identifier) provided in the “ClinQCTargetFile” file (as shown in Additional file 3), one FASTQ file per barcode is created. In case of paired-end sequencing, two FASTQ files (one for forward and one for reverse reads) are generated. This step will be skipped if the input data is already de-multiplexed. 4. Adapter and primer trimming In this step, ClinQC trim the forward and reverse adapter and primer sequences provided in the “PrimerAdapter” file (as shown in Additional file 4) by using the AlienTrimmer [17] tool. AlienTrimmer is a flexible and sensitive sequence trimmer with mismatch tolerance, which allows the customization of the number of mismatches and k-mers based on the data quality and user requirements. 5. Duplicate and contamination filtering PCR duplicates are a critical known problem, which arise when low abundant fragments are over amplified during the library preparation process. These duplicates can substantially inflate the allele frequency leading to wrong mutation detection and unexpected species richness in metagenomic analysis [20]. Therefore, ClinQC identify and remove duplicates using the PRINSEQ [8] tool to eliminate this technical artifact. Contamination is another problem particularly in metagenomic analysis [21] leading to wrong analysis when DNA from unknown sources is sequenced. Hence, ClinQC assesses and eliminates the contamination from the samples using the PRINSEQ [8] software. 6. Quality trimming As NGS short read sequencing errors increase with the position in the read [22], ClinQC trim the low quality stretch and Ns from the 5’ and 3’ end of the reads. 7. Read filtering In this Step ClinQC eliminate the reads, which do not meet the minimum average base quality and the minimum and maximum read length threshold. Thus, only high quality reads, which fulfill all quality trimming and filtering criteria, are kept in the final output file. 8. GC content assessment GC content is crucial parameter when analyzing NGS data as the under or over representation of GC content could effect the downstream analysis and biological conclusions. Therefore, ClinQC reports the average GC content before and after QC in the summary table for each dataset. 9. Output generation In this final step ClinQC write three output files: 1) summary output file in HTML format, 2) QC report, and 3) FASTQ files after filtering the low quality reads.";QualityControl
UrQt;"Quality control is a necessary step of any Next Generation Sequencing analysis. Although customary, this step still requires manual interventions to empirically choose tuning parameters according to various quality statistics. Moreover, current quality control procedures that provide a “good quality” data set, are not optimal and discard many informative nucleotides. To address these drawbacks, we present a new quality control method, implemented in UrQt software, for Unsupervised Quality trimming of Next Generation Sequencing reads. Our trimming procedure relies on a well-defined probabilistic framework to detect the best segmentation between two segments of unreliable nucleotides, framing a segment of informative nucleotides. Our software only requires one user-friendly parameter to define the minimal quality threshold (phred score) to consider a nucleotide to be informative, which is independent of both the experiment and the quality of the data. This procedure is implemented in C++ in an efficient and parallelized software with a low memory footprint. We tested the performances of UrQt compared to the best-known trimming programs, on seven RNA and DNA sequencing experiments and demonstrated its optimality in the resulting tradeoff between the number of trimmed nucleotides and the quality objective. By finding the best segmentation to delimit a segment of good quality nucleotides, UrQt greatly increases the number of reads and of nucleotides that can be retained for a given quality objective. A read is defined as a vector (n 1,…,n m ) of m nucleotides associated with a vector of phred scores (q 1,…,q m ). We want to find the best cut-point k 1∈[1,m] in a read of length m between an informative segment for nucleotide n i ,i∈[1,k 1] and a segment of unreliable quality for nucleotide n i ,i∈[k 1+1,m] (Figure 1). Then, having found k 1, we want to find the best cut-point k 2∈[1,k 1] between a segment of unreliable quality for nucleotide n i ,i∈[1,k 2−1] and an informative segment for nucleotide n i ,i∈[k 2,k 1]. Given the shape of the calling error probability distribution, there is less signal to find k 1 (the probability slowly increases at the extremity of the read) than k 2 (abruptly decreases). Therefore, we want to have the highest number of nucleotides to support the choice of k 1 when k 2 can be found with a subsequence of the read. With q the quality value of a nucleotide, the probability for this nucleotide to be correct is defined by: 𝑝𝑎(𝑞)=1−10−𝑞10 ((A)) which gives, for example, a probability p a (q)=0.99 for a phred q=20 [2]. However, in QC, the word “informative” is typically defined as a phred score above a certain threshold and not the probability of calling the correct nucleotide. From a probabilistic point of view, we need to discriminate informative nucleotides (with p a (q)≥p a (t) and t a given threshold) from other nucleotides, rather than discriminate fairly accurate nucleotides (with p a (q)≥0.5) from the others. Therefore, we propose to define the probability of having an informative nucleotide as 𝑝𝑏(𝑞,𝑡)=1−2−𝑞𝑡 with t the minimal phred score acceptable to be informative. This definition shifts the probability function such that for q=t, we have p b (q,t)=0.5. Therefore, at the threshold t, nucleotides with p b (q,t)≥0.5 are informative and the others are not. With t=3.0103, we go back to the classical phred function (Figure 2) in which p b (q,t)=p a (q). With the function p b (q,t), low phred scores are associated with a low probability to be correct (p b (0,t)=0), but for t≤20 a high phred score does not correspond to a high probability to be correct (for example, p b (40,20)=0.75). Therefore, from a probabilistic point of view, unreliable nucleotides will have more weight than informative ones. To associate a high phred score with a high probability of having an informative nucleotide, we constrain this probability to reach 1 for a phred score of 45 by using the following spline function (Figure 2): 𝑝(𝑞,𝑡)={1−2−𝑞𝑡𝐵(𝑞⋆,𝑝1,𝑝2,1,1)if𝑞≤max(20,𝑡),otherwise ((B)) with B(q ⋆,p 1,p 2,p 3,p 4) the cubic Bezier curve starting at p 1 toward p 2 and arriving at p 4 coming from the direction of p 3 for q ⋆∈[0,1]. We have p 1=1−2− max(20,t)/t, p 2=g(1/3×(45− max(20,t))) with g(q) the tangent to the function 1−2−𝑞𝑡 in max(20,t). We scale the Bezier curve to the interval [t,45] with q ⋆=(q−t)/(45−t). The constraint max(20,t) ensures that 𝑑𝑑𝑞⋆𝐵(𝑞⋆,𝑝1,𝑝2,𝑝3,𝑝4)<0 for q ⋆∈[0,1] (see Figure 2). With the maximum likelihood framework, finding the position of the cut-point between a segment of informative nucleotides (q>t) and a segment of unreliable nucleotides (q<t) consists in estimating k 1 by: 𝑘1ˆ=argmax𝑘∏𝑖=1𝑘1𝑘𝑓0(𝑛𝑖,𝑡)∏𝑖=𝑘+1𝑚1𝑚−𝑘−1𝑓1(𝑛𝑖,𝑡) ((C)) with f 0(n i ,t) the probability that the nucleotide n i comes from the segment of informative nucleotides and f 1(n i ,t) the probability that the nucleotide n i comes from the segment of unreliable nucleotides for a given t. Such that: 𝑓0(𝑛𝑖,𝑡)=𝑝(𝑞𝑖,𝑡)∏𝑁∈ΩPr(𝑁)1(𝑛𝑖=𝑁) ((D)) 𝑓1(𝑛𝑖,𝑡)=(1−𝑝(𝑞𝑖,𝑡))14 ((E)) with 1(n i =N) an indicator variable such that 1(n i =N)=1 if the nucleotide n i is equal to N and 0 otherwise, Pr(𝑁)=∑𝑘𝑖=11(𝑛𝑖=𝑁)/𝑘 the probability to observe the nucleotide N between 1 and k, and Ω the standard IUB/IUPAC dictionary [10]. Pr(N) N∈Ω and k 1 are estimated with the complete data framework of the EM algorithm [11]. After finding 𝑘1ˆ, we apply the same procedure on the interval [1,𝑘1ˆ] to estimate the best cut-point k 2 between a segment of unreliable nucleotides ahead of a segment of informative nucleotides. This double binary segmentation ensures to provide the best two cut-points for a given read [12]. For p(q,t)=p a (q), we can interpret the segment of informative nucleotides as a segment for which on average we are confident that a given nucleotide is the correct one, whereas the segment of unreliable nucleotides is composed of uninformative nucleotides in which on average any of the four nucleotides can be present at a given position. The cut-point k 1 maximizes the probability that the nucleotides n i ,i∈[1,k 1] are informative and that nucleotides n i ,i∈[k 1,m] are not. With our model, trimming nucleotides of unreliable quality is somewhat similar to removing homopolymers from the extremities of the reads. The task of removing homopolymers, such as polyA tails in RNA-Seq experiments, is not trivial, because the quality of a given nucleotide decreases both at the end of the read and with the size of the homopolymer. Therefore, because the number of incorrectly called nucleotides increases, we are less likely to observe As at the end of the polyA tail. UrQt implements a procedure for the unsupervised trimming of polyN with a straightforward modification of equation (E) such that: 𝑓1(𝑛𝑖,𝑡)=𝑝𝑎(𝑞𝑖,𝑡)1(𝑛𝑖=𝐴)((1−𝑝𝑎(𝑞𝑖,𝑡))14)1(𝑛𝑖≠𝐴) ((F)) in which we can replace A by any letter of the standard IUB/IUPAC dictionary. With this definition of f 1, we consider the calling error probability of the nucleotide at position i if n i =A or if n i ≠A, the probability that the nucleotide could have been an A.";QualityControl
EasyQC;"The advent of next-generation sequencing (NGS) technologies has revolutionized the world of genomic research. Millions of sequences are generated in a short period of time and they provide intriguing insights to the researcher. Many NGS platforms have evolved over a period of time and their efficiency has been ever increasing. Still, primarily because of the chemistry, glitch in the sequencing machine and human handling errors, some artifacts tend to exist in the final sequence data set. These sequence errors have a profound impact on the downstream analyses and may provide misleading information. Hence, filtering of these erroneous reads has become inevitable and myriad of tools are available for this purpose. However, many of them are accessible as a command line interface that requires the user to enter each command manually. Here, we report EasyQC, a tool for NGS data quality control (QC) with a graphical user interface providing options to carry out trimming of NGS reads based on quality, length, homopolymer, and ambiguous bases. EasyQC also possesses features such as format converter, paired end merger, adapter trimmer, and a graph generator that generates quality distribution, length distribution, GC content, and base composition graphs. Comparison of raw and processed sequence data sets using EasyQC suggested significant increase in overall quality of the sequences. Testing of EasyQC using NGS data sets on a standalone desktop proved to be relatively faster. EasyQC is developed using PERL modules and can be executed in Windows and Linux platforms. With the various QC features, easy interface for end users, and cross-platform compatibility, EasyQC would be a valuable addition to the already existing tools facilitating better downstream analyses. Format converter. This module of EasyQC mediates the conversion of .fastq files to .fasta files. It verifies the accuracy of the input FASTQ file and converts it to FASTA file upon successful format check. Adapter trimmer. Adapter or primer trimming is a vital step in NGS data QC. It is important to remove the forward and reverse adapter sequences from the data set. The adapter trimmer module of EasyQC takes the forward and reverse adapter sequences as input and trims the sequences accordingly. 3.1.3. Paired end merging. Many sequencing protocols that are in use today sequence the DNA fragments from both ends. Merging of these paired end reads is important in genome assembly and other analyses. The paired end merging module of EasyQC uses FLASH (Magoc and Salzberg, 2011) to merge the input FASTQ files. 3.1.4. Graph generator. This feature of EasyQC enables the user to obtain graphical outputs of length distribution, average quality distribution, ATGC composition, and GC content of the input sequence data set. 3.1.5. Quality-based trimming. Removal of low quality sequences is essential, since they can have a significant impact on the downstream analyses of NGS data. This module of EasyQC gets the quality threshold for each base along with the percentage threshold for each sequence as input. Sequences that contain lesser percentage of high-quality bases than the threshold will be removed from the data set. 3.1.6. Length-based trimming. At times, data sets possess sequences with a very wide range of read lengths and it is important to remove those sequences that fall short of the required length. This module trims those sequences that are shorter than the input threshold length value. 3.1.7. Ambiguous-based trimming. Presence of ambiguous bases has significant impact on the subsequent alignment process. This feature fetches the threshold percentage of the ambiguous bases that shall be permitted in a sequence and those with a greater percentage will be removed. Homopolymer trimming. Homopolymer repeats in a sequence shall also cause misalignment of the sequences leading to false results. This module takes the cutoff value for the homopolymers to be present in a sequence and those that exceed this cutoff value will be taken off from the data set. The trimming modules are designed as a single pipeline in EasyQC. The user shall upload the input sequence data set and specify the threshold values of all the modules at the same time and execute the pipeline.";QualityControl
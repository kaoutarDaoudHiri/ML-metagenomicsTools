{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"/Volumes/TOSHIBA EXT/phd/DSS/ToolsClassifier/Datasets/Tools_Abst_Meth_9classes.csv\"\n",
    "df = pd.read_csv(df_path, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_1'] = df['Content'].str.lower()\n",
    "\n",
    "# removing links\n",
    "regex_link = r\"\\bhttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.replace(regex_link, \"\")\n",
    "\n",
    "# removing numbers\n",
    "regex_nums = r\"\\b[0-9][0-9]*\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_nums, \"\")\n",
    "\n",
    "# removing special characters\n",
    "special_character = list(\"←=()[]/‘’|><\\\\∼+%$&×–−-·\")\n",
    "for spec_char in special_character:\n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(spec_char, '')\n",
    "\n",
    "\n",
    "# removing punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(punct_sign, '') \n",
    "    \n",
    "# removing strings with length 1-2\n",
    "regex_short = r\"\\b\\w{0,2}\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_short, \"\")    \n",
    "\n",
    "# removing strings starting with numbers\n",
    "regex_short = r\"\\b[0-9][0-9]*\\w\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_short, \"\")\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "# nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')\n",
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Iterating through every word to lemmatize\n",
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_2']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "df['Content_Parsed_3'] = lemmatized_text_list\n",
    "\n",
    "# removing possessive pronoun terminations\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "\n",
    "# removing english stop words\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# looping through all stop words\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(regex_stopword, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' alignment  sequence read   protein reference database   major computational bottleneck  metagenomics  dataintensive evolutionary project although recent tool offer improve performance   gold standard blastx  exhibit   modest speedup  low sensitivity  introduce diamond  opensource algorithm base  double index    time faster  blastx  short read    similar degree  sensitivity\\ndiamond   highthroughput alignment program  compare  file  dna sequence read   file  protein reference sequence   implement     design  run  multicore serversdiamond  faster  blastx4  compare short dna read   ncbinr database  maintain  comparable level  sensitivity  alignments  program  explicitly design  make use  modern computer architectures   large memory capacity  many core  follow  seedandextend approach additional algorithmic ingredients   use   reduce alphabet space seed  double index seed  extend  program  base   traditional seedandextend paradigm  sequence comparison   exact occurrences  seed   short word   give fix length contain  query sequence  locate   reference sequence   seed match   extend  possible  full alignments   query  reference  seed length use   alignment program   substantial impact  performance shorter seed increase sensitivity whereas longer seed increase speed reduce alphabet  increase speed without lose sensitivity one approach   use  reduce alphabet  compare seed use  rapsearch2      time faster  blastx  minimal loss  sensitivity  diamond  investigate  use  publish reductions  four eight  ten letter  analyze  large number  blastx alignments  develop  new reduction   alphabet  size eleven  achieve slightly better sensitivity space seed  second improvement   seed step   use space seed   longer seed     subset  position  use  number  exact layout   position  call  weight  shape   space seed respectively theoretical analysis show   single space seed  perform better   contiguous seed    weight   shape  suitably choose moreover sensitivity   increase   use additional seed shape  result    sublinear increase  run time  default diamond use  set  four shape  length     weight  seed index  main bottleneck  align  large number  read   large reference database   cpu performance  rather memory latency  bandwidth  limit factor   amount  time require  load seed locations  main memory  comparison move data  main memory   cache take hundreds  cpu clock cycle thus  fast algorithm  take  cache hierarchy  computers  account    maximize data locality  minimize  number  main memory access      decompose  problem  smaller subproblems  fit   cache   seedandextend program  index structure  build   reference sequence  facilitate  location  seed   reference query  process   order   occur   input file   give query  seed  determine   index   use  look   match locations   reference sequence  reference locations  load  main memory   cache  another read  later encounter  contain     seed  correspond index  sequence data  usually   evict   cache   data     load  main memory  hence use  single index   way   make good use   cache  problem  compound  use  fulltext index    suffix array   compress  index   require multiple individual memory access per index lookup double index diamond use  doubleindexing approach     query   reference  index  diamond index   list  seedlocation pair   lexicographically order   compress representation   seed  traverse  two index list linearly  parallel  match seed   query   reference  identify allow  local alignment computation   correspond seed locations  index memory access pattern   approach  linear    efficiently handle   hardware prefetcher   fill  cache   index information    need  doubleindexed approach also improve data locality  respect  access  sequence  see  let    denote  set  seed contain   set  query  reference respectively   give seed  let      number  occurrences   seed   query   reference use  standard index approach    occurrence   reference seed   query  correspond reference locations   load  memory use  doubleindexing approach  number  memory access operations assume   combine size  query  reference locations  one seed  small enough  fit   cache  number  much smaller   unless  sum  dominate  singleton seed  demonstrate  effect  real data  plot  ratio  memory access   two approach depend   length   query sequence  letter  observe   benchmark data  doubleindexing algorithm use  diamond  base   wellknown database sortmerge join algorithm apply   two seed set   query  reference  main computation  compilation  sort   two list  seed   efficiently address  parallel use  radix cluster  combination   fast sort algorithm  total amount  time require  sort   seed   give set  query  smaller    require  access  seed   hashtable approach  sort  seed   reference sequence   fly take much less time  load  precomputed index  example  complete seed index   current version   ncbinr database      size  take    generate  memory   six time  long  read  disk   typical read rate    per second  alignment program mrsfast use sort list   index structure  author   tool spend  lot  effort  make  algorithm cache oblivious much   challenge discuss  stem   fact  short nonoverlapping seed  use  cause  set   occurrences   seed  exceed  cache capacity owe    elaborate seed strategy use  diamond   program  amount  data associate   give seed  always  small enough  fit   cache memory efficiency  drawback  use multiple space seed    use  lot  memory    main reason   approach    widely use despite  prove advantage  naïve implementation   multiple space seed index build  hashtable index     seed shape  one hash table index   ncbinr database      size four seed shape would consume     shape would consume    memory diamond construct  process  index  one shape   time free   memory use  one shape  move    next thus diamond  perform alignment task   sensitive shape configuration use   much memory  one shape index require    additional advantage   approach moreover use  radix cluster technique  seed space  decompose   disjoint partition  build  process index    subset   partition    time  memory usage   limit   size   subset index seed extension   seed match find diamond determine whether    extend   ungapped alignment  ten   amino acids     case   seed match trigger  extend phase   algorithm  involve compute  smithwaterman alignment diamond use   stream simd extension sseaccelerated smithwaterman implementation  extend previous algorithms  allow  computation  band  anchor alignments  default  program use  blosum matrix  gap score     extension score   however  blosum matrices  score parameters   use  program determine  bite score  expect value   compute alignment   blastx  default alignments   bite score    report  diamond proceed seed  seed rather  read  read  key issue    avoid compute   local alignment   read   reference     different time   search phase  address  diamond allow  seed match  trigger  extension      leftmost seed match   correspond ungapped alignment'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]['Content_Parsed_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diamond</td>\n",
       "      <td>The alignment of sequencing reads against a pr...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>the alignment of sequencing reads against a pr...</td>\n",
       "      <td>the alignment  sequencing reads against  prote...</td>\n",
       "      <td>alignment  sequence read   protein reference ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name                                            Content   Category  \\\n",
       "0   Diamond  The alignment of sequencing reads against a pr...  Alignment   \n",
       "\n",
       "                                    Content_Parsed_1  \\\n",
       "0  the alignment of sequencing reads against a pr...   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  the alignment  sequencing reads against  prote...   \n",
       "\n",
       "                                    Content_Parsed_3  \n",
       "0   alignment  sequence read   protein reference ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"File_Name\", \"Content\",\"Category\", \"Content_Parsed_3\"]\n",
    "df = df[list_columns]\n",
    "df = df.rename(columns={'Content_Parsed_3': 'Content_Parsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'Alignment': 0,\n",
    "    'Classification': 1,\n",
    "    'VirusDetection': 2,\n",
    "    'VirusIdentification': 3,\n",
    "    'Mapping': 4,\n",
    "    'Assembly': 5,\n",
    "    'AbundanceEstimation': 6,\n",
    "    'Trimming': 7,\n",
    "    'QualityControl': 8\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df['Category_Code'] = df['Category']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 60)\n",
      "(13, 60)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectors as features\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 60\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'AbundanceEstimation' category:\n",
      "  . Most correlated unigrams:\n",
      ". assembly\n",
      ". trim\n",
      ". species\n",
      ". estimate\n",
      ". abundance\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'Alignment' category:\n",
      "  . Most correlated unigrams:\n",
      ". map\n",
      ". align\n",
      ". seed\n",
      ". alignment\n",
      ". alignments\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'Assembly' category:\n",
      "  . Most correlated unigrams:\n",
      ". database\n",
      ". kmers\n",
      ". contigs\n",
      ". assembly\n",
      ". graph\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'Classification' category:\n",
      "  . Most correlated unigrams:\n",
      ". quality\n",
      ". database\n",
      ". kmers\n",
      ". species\n",
      ". kmer\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'Mapping' category:\n",
      "  . Most correlated unigrams:\n",
      ". hit\n",
      ". reference\n",
      ". seed\n",
      ". index\n",
      ". mismatch\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'QualityControl' category:\n",
      "  . Most correlated unigrams:\n",
      ". analysis\n",
      ". filter\n",
      ". file\n",
      ". trim\n",
      ". quality\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'Trimming' category:\n",
      "  . Most correlated unigrams:\n",
      ". quality\n",
      ". genomes\n",
      ". reference\n",
      ". position\n",
      ". trim\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'VirusDetection' category:\n",
      "  . Most correlated unigrams:\n",
      ". seed\n",
      ". estimate\n",
      ". hit\n",
      ". genomes\n",
      ". database\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n",
      "# 'VirusIdentification' category:\n",
      "  . Most correlated unigrams:\n",
      ". sample\n",
      ". database\n",
      ". nucleotide\n",
      ". hit\n",
      ". identify\n",
      "  . Most correlated bigrams:\n",
      ". reference genome\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#most correlated unigrams and bigrams\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"/Volumes/TOSHIBA EXT/phd/DSS/ToolsClassifier/Datasets/Tools_Abst_9classes.csv\"\n",
    "df = pd.read_csv(df_path, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_1'] = df['Content'].str.lower()\n",
    "\n",
    "# removing links\n",
    "regex_link = r\"\\bhttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.replace(regex_link, \"\")\n",
    "\n",
    "# removing numbers\n",
    "regex_nums = r\"\\b[0-9][0-9]*\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_nums, \"\")\n",
    "\n",
    "# removing special characters\n",
    "special_character = list(\"←=()[]/‘’|><\\\\∼+%$&×–−-·\")\n",
    "for spec_char in special_character:\n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(spec_char, '')\n",
    "\n",
    "\n",
    "# removing punctuation\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(punct_sign, '') \n",
    "    \n",
    "# removing strings with length 1-2\n",
    "regex_short = r\"\\b\\w{0,2}\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_short, \"\")    \n",
    "\n",
    "# removing strings starting with numbers\n",
    "regex_short = r\"\\b[0-9][0-9]*\\w\\b\"\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_2'].str.replace(regex_short, \"\")\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "# nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')\n",
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# Iterating through every word to lemmatize\n",
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_2']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "df['Content_Parsed_3'] = lemmatized_text_list\n",
    "\n",
    "# removing possessive pronoun terminations\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "\n",
    "# removing english stop words\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# looping through all stop words\n",
    "for stop_word in stop_words:\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(regex_stopword, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20,000 times faster than BLASTX on short reads and has a similar degree of sensitivity.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]['Content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diamond.txt</td>\n",
       "      <td>The alignment of sequencing reads against a pr...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>the alignment of sequencing reads against a pr...</td>\n",
       "      <td>the alignment  sequencing reads against  prote...</td>\n",
       "      <td>alignment  sequence read   protein reference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Minimap2.txt</td>\n",
       "      <td>Recent advances in sequencing technologies pro...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>recent advances in sequencing technologies pro...</td>\n",
       "      <td>recent advances  sequencing technologies promi...</td>\n",
       "      <td>recent advance  sequence technologies promise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bowtie.txt</td>\n",
       "      <td>Bowtie is an ultrafast, memory-efficient align...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>bowtie is an ultrafast, memory-efficient align...</td>\n",
       "      <td>bowtie   ultrafast memoryefficient alignment p...</td>\n",
       "      <td>bowtie   ultrafast memoryefficient alignment p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HISAT.txt</td>\n",
       "      <td>The human reference genome represents only a s...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>the human reference genome represents only a s...</td>\n",
       "      <td>the human reference genome represents only  sm...</td>\n",
       "      <td>human reference genome represent   small numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STAR.txt</td>\n",
       "      <td>Motivation: Accurate alignment of high-through...</td>\n",
       "      <td>Alignment</td>\n",
       "      <td>motivation: accurate alignment of high-through...</td>\n",
       "      <td>motivation accurate alignment  highthroughput ...</td>\n",
       "      <td>motivation accurate alignment  highthroughput ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File_Name                                            Content   Category  \\\n",
       "0   Diamond.txt  The alignment of sequencing reads against a pr...  Alignment   \n",
       "1  Minimap2.txt  Recent advances in sequencing technologies pro...  Alignment   \n",
       "2    Bowtie.txt  Bowtie is an ultrafast, memory-efficient align...  Alignment   \n",
       "3     HISAT.txt  The human reference genome represents only a s...  Alignment   \n",
       "4      STAR.txt  Motivation: Accurate alignment of high-through...  Alignment   \n",
       "\n",
       "                                    Content_Parsed_1  \\\n",
       "0  the alignment of sequencing reads against a pr...   \n",
       "1  recent advances in sequencing technologies pro...   \n",
       "2  bowtie is an ultrafast, memory-efficient align...   \n",
       "3  the human reference genome represents only a s...   \n",
       "4  motivation: accurate alignment of high-through...   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  the alignment  sequencing reads against  prote...   \n",
       "1  recent advances  sequencing technologies promi...   \n",
       "2  bowtie   ultrafast memoryefficient alignment p...   \n",
       "3  the human reference genome represents only  sm...   \n",
       "4  motivation accurate alignment  highthroughput ...   \n",
       "\n",
       "                                    Content_Parsed_3  \n",
       "0   alignment  sequence read   protein reference ...  \n",
       "1  recent advance  sequence technologies promise ...  \n",
       "2  bowtie   ultrafast memoryefficient alignment p...  \n",
       "3   human reference genome represent   small numb...  \n",
       "4  motivation accurate alignment  highthroughput ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"File_Name\", \"Content\",\"Category\", \"Content_Parsed_3\"]\n",
    "df = df[list_columns]\n",
    "df = df.rename(columns={'Content_Parsed_3': 'Content_Parsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'Alignment': 0,\n",
    "    'Classification': 1,\n",
    "    'VirusDetection': 2,\n",
    "    'VirusIdentification': 3,\n",
    "    'Mapping': 4,\n",
    "    'Assembly': 5,\n",
    "    'AbundanceEstimation': 6,\n",
    "    'Trimming': 7,\n",
    "    'QualityControl': 8\n",
    "}\n",
    "\n",
    "# Category mapping\n",
    "df['Category_Code'] = df['Category']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 60)\n",
      "(13, 60)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectors as features\n",
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 60\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)\n",
    "\n",
    "# for the models stratified cross validation\n",
    "features = tfidf.transform(df['Content_Parsed']).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'AbundanceEstimation' category:\n",
      "  . Most correlated unigrams:\n",
      ". species\n",
      ". approach\n",
      ". metagenomic\n",
      ". methods\n",
      ". abundance\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'Alignment' category:\n",
      "  . Most correlated unigrams:\n",
      ". short\n",
      ". map\n",
      ". human\n",
      ". algorithm\n",
      ". alignment\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'Assembly' category:\n",
      "  . Most correlated unigrams:\n",
      ". alignment\n",
      ". produce\n",
      ". genome\n",
      ". assemble\n",
      ". assembly\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'Classification' category:\n",
      "  . Most correlated unigrams:\n",
      ". present\n",
      ". species\n",
      ". metagenomic\n",
      ". metagenomics\n",
      ". classification\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'Mapping' category:\n",
      "  . Most correlated unigrams:\n",
      ". new\n",
      ". short\n",
      ". alignment\n",
      ". reference\n",
      ". map\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'QualityControl' category:\n",
      "  . Most correlated unigrams:\n",
      ". feature\n",
      ". ngs\n",
      ". generate\n",
      ". trim\n",
      ". quality\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'Trimming' category:\n",
      "  . Most correlated unigrams:\n",
      ". reference\n",
      ". genomes\n",
      ". tool\n",
      ". ngs\n",
      ". trim\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'VirusDetection' category:\n",
      "  . Most correlated unigrams:\n",
      ". genomic\n",
      ". detection\n",
      ". ngs\n",
      ". genomes\n",
      ". viruses\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n",
      "# 'VirusIdentification' category:\n",
      "  . Most correlated unigrams:\n",
      ". microbial\n",
      ". assemble\n",
      ". metagenomic\n",
      ". identify\n",
      ". viruses\n",
      "  . Most correlated bigrams:\n",
      ". sequence data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#most correlated unigrams and bigrams\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features\n",
    "with open('Pickles/features.pickle', 'wb') as output:\n",
    "    pickle.dump(features, output)\n",
    "    \n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
